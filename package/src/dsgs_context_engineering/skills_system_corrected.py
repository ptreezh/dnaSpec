"""
DNASPEC Context Engineering Skills - AI native implementation (final clean version)
Leveraging native AI intelligence through instruction engineering, not local models
"""
import json
import random
from typing import Dict, Any
from src.dnaspec_spec_kit_integration.core.skill import DNASpecSkill, SkillResult, SkillStatus


def simulate_ai_completion(instruction: str) -> str:
    """
    Simulate AI model completion function (in real implementation would call AI API)
    """
    import re
    
    # Simulate different instruction responses based on content
    if "åˆ†æ" in instruction or "è¯„ä¼°" in instruction:
        # Extract context from instruction
        context_match = re.search(r'"([^"]+)"', instruction)
        context_text = context_match.group(1) if context_match else "æµ‹è¯•ä¸Šä¸‹æ–‡"
        
        # Calculate metrics based on context
        clarity = min(1.0, max(0.0, 0.5 + len(context_text) * 0.0001))
        relevance = min(1.0, max(0.0, 0.7 + (0.1 if any(kw in context_text for kw in ['ç³»ç»Ÿ', 'åŠŸèƒ½', 'ä»»åŠ¡']) else 0)))
        completeness = min(1.0, max(0.0, 0.3 + (0.3 if any(kw in context_text for kw in ['çº¦æŸ', 'è¦æ±‚', 'ç›®æ ‡']) else 0)))
        consistency = min(1.0, max(0.0, 0.8 - (0.2 if any(kw in context_text for kw in ['ä½†æ˜¯', 'ç„¶è€Œ']) else 0)))
        efficiency = min(1.0, max(0.0, 1.0 - len(context_text) * 0.00005))
        
        result_data = {
            "context_length": len(context_text),
            "token_count_estimate": max(1, len(context_text) // 4),
            "metrics": {
                "clarity": round(clarity, 2),
                "relevance": round(relevance, 2),
                "completeness": round(completeness, 2),
                "consistency": round(consistency, 2),
                "efficiency": round(efficiency, 2)
            },
            "suggestions": [
                "å¢åŠ æ›´æ˜ç¡®çš„ç›®æ ‡æè¿°",
                "è¡¥å……çº¦æŸæ¡ä»¶å’Œå…·ä½“è¦æ±‚",
                "æé«˜è¡¨è¿°æ¸…æ™°åº¦"
            ],
            "issues": [
                i for i in [
                    "ç¼ºå°‘æ˜ç¡®çš„çº¦æŸæ¡ä»¶" if completeness < 0.6 else "",
                    "éƒ¨åˆ†è¡¨è¿°å¯ä»¥æ›´ç²¾ç¡®" if clarity < 0.7 else ""
                ] if i  # Filter out empty strings
            ],
            "confidence": 0.85
        }
        
        return json.dumps(result_data, ensure_ascii=False, indent=2)
    
    elif "ä¼˜åŒ–" in instruction or "æ”¹è¿›" in instruction:
        # Extract original context from instruction
        original_match = re.search(r'åŸå§‹ä¸Šä¸‹æ–‡:\s*["\']([^"\']+)["\']', instruction)
        original_context = original_match.group(1) if original_match else "å¾…ä¼˜åŒ–å†…å®¹"
        
        # Extract goals from instruction
        goals_match = re.search(r'ä¼˜åŒ–ç›®æ ‡:\s*([^\n\]]+)', instruction)
        goals_text = goals_match.group(1) if goals_match else "clarity,completeness"
        
        goals = [g.strip() for g in goals_text.split(',') if g.strip()]
        
        # Simulate optimization
        optimized_context = original_context
        applied_optimizations = []
        
        if any(goal in goals for goal in ['clarity', 'æ¸…æ™°åº¦']):
            optimized_context += "\n\nè¯·æ˜ç¡®å…·ä½“çš„ç›®æ ‡å’Œçº¦æŸæ¡ä»¶ã€‚"
            applied_optimizations.append("æå‡è¡¨è¿°æ¸…æ™°åº¦")
        
        if any(goal in goals for goal in ['completeness', 'å®Œæ•´æ€§']):
            optimized_context += "\n\nçº¦æŸæ¡ä»¶: éœ€åœ¨æŒ‡å®šæ—¶é—´å†…å®Œæˆ\næ˜ç¡®ç›®æ ‡: å®ç°é¢„æœŸåŠŸèƒ½\nå‰æå‡è®¾: æœ‰å¿…è¦çš„èµ„æºæ”¯æŒ"
            applied_optimizations.append("è¡¥å……å®Œæ•´æ€§è¦ç´ ")
        
        result_data = {
            "original_context": original_context,
            "optimized_context": optimized_context,
            "applied_optimizations": applied_optimizations,
            "improvement_metrics": {
                "clarity": 0.2 if any(goal in goals for goal in ['clarity', 'æ¸…æ™°åº¦']) else 0.0,
                "relevance": 0.15 if any(goal in goals for goal in ['relevance', 'ç›¸å…³æ€§']) else 0.0,
                "completeness": 0.3 if any(goal in goals for goal in ['completeness', 'å®Œæ•´æ€§']) else 0.0,
                "conciseness": -0.1 if any(goal in goals for goal in ['conciseness', 'ç®€æ´æ€§']) else 0.0
            },
            "optimization_summary": f"æ ¹æ®ç›®æ ‡ {', '.join(goals)} å®Œæˆä¼˜åŒ–"
        }
        
        return json.dumps(result_data, ensure_ascii=False, indent=2)
    
    else:
        # Default response
        result_data = {
            "enhanced_content": f"AIå¤„ç†äº†æŒ‡ä»¤: {instruction[:50]}...",
            "success": True,
            "confidence": 0.8
        }
        
        return json.dumps(result_data, ensure_ascii=False, indent=2)


class ContextAnalysisSkill(DNASpecSkill):
    """Context Analysis Skill - leveraging native AI intelligence for analysis"""
    
    def __init__(self):
        super().__init__(
            name="dnaspec-context-analysis",
            description="DNASPEC Context Analysis Skill - Professional context quality analysis using native AI intelligence"
        )
    
    def _execute_skill_logic(self, request: str, context: Dict[str, Any]) -> Any:
        """Execute context analysis logic using AI native intelligence"""
        if not request.strip():
            return {
                'success': False,
                'error': 'Context cannot be empty',
                'result': {}
            }
        
        analysis_instruction = f"""
As a professional context quality analyst, please perform a five-dimensional evaluation of the following context:

Context: "{request}"

Dimensions (0.0-1.0 rating):
1. Clarity (æ¸…æ™°åº¦): Expression clarity, terminology accuracy, goal clarity
2. Relevance (ç›¸å…³æ€§): Association with task goals, information relevance
3. Completeness (å®Œæ•´æ€§): Key information completeness, constraint completeness
4. Consistency (ä¸€è‡´æ€§): Internal logical consistency, expression coherence
5. Efficiency (æ•ˆç‡): Information density, conciseness, redundancy control

Please return analysis results in JSON format.
"""
        
        try:
            # In real implementation: send request to AI API
            # Currently using simulation for testing purposes
            simulation_result = simulate_ai_completion(analysis_instruction)
            parsed_result = json.loads(simulation_result)
            
            return {
                'success': True,
                'result': parsed_result
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'AI processing failed: {str(e)}',
                'result': {}
            }
    
    def _calculate_confidence(self, request: str) -> float:
        """Calculate execution confidence"""
        if len(request) < 5:
            return 0.3  # Low confidence for short contexts
        else:
            return 0.85  # High confidence for appropriate length


class ContextOptimizationSkill(DNASpecSkill):
    """Context Optimization Skill - leveraging native AI intelligence for optimization"""
    
    def __init__(self):
        super().__init__(
            name="dnaspec-context-optimization",
            description="DNASPEC Context Optimization Skill - AI-powered context quality optimization using native intelligence"
        )
    
    def _execute_skill_logic(self, request: str, context: Dict[str, Any]) -> Any:
        """Execute context optimization using AI native intelligence"""
        if not request.strip():
            return {
                'success': False,
                'error': 'Context cannot be empty',
                'result': {}
            }
        
        # Extract optimization goals
        params = context or {}
        goals = params.get('optimization_goals', ['clarity', 'completeness'])
        if isinstance(goals, str):
            goals = [g.strip() for g in goals.split(',') if g.strip()]
        
        optimization_instruction = f"""
Optimize the context based on the following goals:

Optimization Goals: {', '.join(goals)}

Original Context: "{request}"

Return optimized content and applied improvements in JSON format.
"""
        
        try:
            # In real implementation: send to AI model API
            # Currently using simulation
            simulation_result = simulate_ai_completion(optimization_instruction)
            parsed_result = json.loads(simulation_result)
            
            return {
                'success': True,
                'result': parsed_result
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'AI optimization failed: {str(e)}',
                'result': {}
            }
    
    def _calculate_confidence(self, request: str) -> float:
        """Calculate confidence for optimization"""
        if len(request) < 10:
            return 0.4  # Low confidence for very short context
        else:
            return 0.8  # Good confidence for normal context


class CognitiveTemplateSkill(DNASpecSkill):
    """Cognitive Template Skill - applying native AI cognitive capabilities"""
    
    def __init__(self):
        super().__init__(
            name="dnaspec-cognitive-template",
            description="DNASPEC Cognitive Template Skill - Applying cognitive templates to structure reasoning using AI native intelligence"
        )
        
        self.templates = {
            'chain_of_thought': 'Chain of Thought Reasoning Template',
            'few_shot': 'Few-Shot Learning Template', 
            'verification': 'Verification Check Template',
            'role_playing': 'Role Playing Template',
            'understanding': 'Deep Understanding Template'
        }
    
    def _execute_skill_logic(self, request: str, context: Dict[str, Any]) -> Any:
        """Execute cognitive template application using AI native intelligence"""
        if not request.strip():
            return {
                'success': False,
                'error': 'Context cannot be empty',
                'result': {'success': False}
            }
        
        params = context or {}
        template_type = params.get('template', 'chain_of_thought')
        
        if template_type not in self.templates:
            return {
                'success': False,
                'error': f'Unknown template: {template_type}',
                'available_templates': list(self.templates.keys()),
                'result': {'success': False}
            }
        
        template_desc = self.templates[template_type]
        
        if template_type == 'chain_of_thought':
            template_instruction = f"""
Use chain-of-thought method to analyze the following task:

Task: {request}

Analyze with these steps:
1. Problem Understanding
2. Step Decomposition  
3. Intermediate Reasoning
4. Verification Check
5. Final Answer

Return structured analysis.
"""
        elif template_type == 'verification':
            template_instruction = f"""
Use verification framework to analyze the following:

Content: {request}

Perform verification:
1. Preliminary Answer
2. Logical Consistency Check
3. Fact Accuracy Check
4. Completeness Check
5. Final Confirmation

Return verification results.
"""
        else:
            # Default template
            template_instruction = f"""
Apply {template_desc} to analyze task: {request}

Return structured result.
"""
        
        try:
            # Simulate AI processing with cognitive template
            enhanced_content = f"""
### {template_type} Cognitive Template Application

**Original Task**: {request}

**Structured Analysis**:
[AI model will apply {template_desc} for professional analysis...]

**Professional Result**:
[Return structured analysis based on {template_desc}]

**Confidence**: 0.85
"""
            
            return {
                'success': True,
                'result': {
                    'success': True,
                    'template_type': template_type,
                    'template_description': template_desc,
                    'original_context': request,
                    'enhanced_context': enhanced_content,
                    'template_structure': ['Apply Cognitive Framework', 'Structured Output', 'Verify Results'],
                    'confidence': 0.85
                }
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'Template application failed: {str(e)}',
                'result': {'success': False}
            }
    
    def _calculate_confidence(self, request: str) -> float:
        """Calculate confidence for template application"""
        if len(request) < 5:
            return 0.35  # Low confidence for very short context
        else:
            return 0.85  # High confidence for normal context


def execute(args: Dict[str, Any]) -> str:
    """
    Execute function - unified interface for AI CLI platform integration
    """
    skill_name = args.get('skill', 'context-analysis')
    context_input = args.get('context', '') or args.get('request', '')
    params = args.get('params', {})
    
    if not context_input:
        return "é”™è¯¯: æœªæä¾›éœ€è¦å¤„ç†çš„ä¸Šä¸‹æ–‡"
    
    try:
        if skill_name == 'context-analysis':
            skill = ContextAnalysisSkill()
            result = skill.process_request(context_input, params)
            
            if result.status.name == 'COMPLETED':
                analysis = result.result
                if 'result' in analysis:  # Handle nested result structure
                    analysis_data = analysis['result']
                else:
                    analysis_data = analysis
                
                output_lines = []
                output_lines.append("ä¸Šä¸‹æ–‡è´¨é‡åˆ†æç»“æœ:")
                output_lines.append(f"é•¿åº¦: {analysis_data.get('context_length', len(context_input))} å­—ç¬¦")
                output_lines.append(f"Tokenä¼°ç®—: {analysis_data.get('token_count_estimate', max(1, len(context_input) // 4))}")
                output_lines.append("")
                
                output_lines.append("äº”ç»´è´¨é‡æŒ‡æ ‡ (0.0-1.0):")
                metric_names = {
                    'clarity': 'æ¸…æ™°åº¦', 'relevance': 'ç›¸å…³æ€§', 'completeness': 'å®Œæ•´æ€§',
                    'consistency': 'ä¸€è‡´æ€§', 'efficiency': 'æ•ˆç‡'
                }
                
                for metric, score in analysis_data['metrics'].items():
                    indicator = "ğŸŸ¢" if score >= 0.7 else "ğŸŸ¡" if score >= 0.4 else "ğŸ”´"
                    output_lines.append(f"  {indicator} {metric_names.get(metric, metric)}: {score:.2f}")
                
                if analysis_data.get('suggestions'):
                    output_lines.append("\nä¼˜åŒ–å»ºè®®:")
                    for suggestion in analysis_data['suggestions'][:3]:  # Show first 3 suggestions
                        output_lines.append(f"  â€¢ {suggestion}")
                
                if analysis_data.get('issues'):
                    output_lines.append("\nè¯†åˆ«é—®é¢˜:")
                    for issue in analysis_data['issues']:
                        output_lines.append(f"  â€¢ {issue}")
                
                return "\n".join(output_lines)
            else:
                return f"é”™è¯¯: {result.error_message}"
        
        elif skill_name == 'context-optimization':
            skill = ContextOptimizationSkill()
            result = skill.process_request(context_input, params)
            
            if result.status.name == 'COMPLETED':
                optimization = result.result
                # Check if result has nested structure with 'result' key
                if isinstance(optimization, dict) and 'result' in optimization and isinstance(optimization['result'], dict):
                    # Double-nested structure: result.result.result
                    optimization_data = optimization['result']
                elif isinstance(optimization, dict):
                    # Single structure: result.result
                    optimization_data = optimization
                else:
                    # Unknown structure, return error
                    return "é”™è¯¯: APIå“åº”ç»“æ„ä¸æ­£ç¡®"
                
                output_lines = []
                output_lines.append("ä¸Šä¸‹æ–‡ä¼˜åŒ–ç»“æœ:")
                output_lines.append(f"åŸå§‹é•¿åº¦: {len(optimization_data['original_context'])} å­—ç¬¦")
                output_lines.append(f"ä¼˜åŒ–åé•¿åº¦: {len(optimization_data['optimized_context'])} å­—ç¬¦")
                output_lines.append("")
                
                output_lines.append("åº”ç”¨çš„ä¼˜åŒ–æªæ–½:")
                for opt in optimization_data['applied_optimizations']:
                    output_lines.append(f"  â€¢ {opt}")
                
                output_lines.append("\næ”¹è¿›æŒ‡æ ‡:")
                for metric, change in optimization_data['improvement_metrics'].items():
                    direction = "â†—ï¸" if change > 0 else "â†˜ï¸" if change < 0 else "â¡ï¸"
                    output_lines.append(f"  {direction} {metric}: {change:+.2f}")
                
                output_lines.append("\nä¼˜åŒ–åä¸Šä¸‹æ–‡:")
                output_lines.append(optimization_data['optimized_context'])
                
                return "\n".join(output_lines)
            else:
                return f"é”™è¯¯: {result.error_message}"
        
        elif skill_name == 'cognitive-template':
            skill = CognitiveTemplateSkill()
            result = skill.process_request(context_input, params)
            
            if result.status.name == 'COMPLETED':
                template_result = result.result
                # Check if result has nested structure with 'result' key
                if isinstance(template_result, dict) and 'result' in template_result and isinstance(template_result['result'], dict):
                    # Double-nested structure: result.result.result
                    template_data = template_result['result']
                elif isinstance(template_result, dict):
                    # Single structure: result.result
                    template_data = template_result
                else:
                    # Unknown structure, return error
                    return "é”™è¯¯: APIå“åº”ç»“æ„ä¸æ­£ç¡®"
                
                if template_data.get('success', True):
                    output_lines = []
                    output_lines.append(f"è®¤çŸ¥æ¨¡æ¿åº”ç”¨: {template_data.get('template_type', 'unknown')} ({template_data.get('template_description', 'Unknown')})")
                    output_lines.append("="*60)
                    output_lines.append("")
                    output_lines.append("ç»“æ„åŒ–è¾“å‡º:")
                    output_lines.append(template_data.get('enhanced_context', 'No enhanced context returned'))
                    
                    return "\n".join(output_lines)
                else:
                    error_msg = template_data.get('error', 'æ¨¡æ¿åº”ç”¨å¤±è´¥')
                    return f"é”™è¯¯: {error_msg}"
            else:
                return f"é”™è¯¯: {result.error_message}"
        
        else:
            available_skills = ['context-analysis', 'context-optimization', 'cognitive-template']
            return f"é”™è¯¯: æœªçŸ¥æŠ€èƒ½ '{skill_name}'. å¯ç”¨æŠ€èƒ½: {', '.join(available_skills)}"
    
    except Exception as e:
        return f"é”™è¯¯: æ‰§è¡Œè¿‡ç¨‹å¼‚å¸¸ - {str(e)}"


def get_available_skills() -> Dict[str, str]:
    """Get available skills list"""
    return {
        'context-analysis': 'ä¸Šä¸‹æ–‡è´¨é‡äº”ç»´ä¸“ä¸šåˆ†æ',
        'context-optimization': 'AIé©±åŠ¨çš„ä¸Šä¸‹æ–‡æ™ºèƒ½ä¼˜åŒ–',
        'cognitive-template': 'è®¤çŸ¥æ¨¡æ¿ç»“æ„åŒ–å¤æ‚ä»»åŠ¡'
    }