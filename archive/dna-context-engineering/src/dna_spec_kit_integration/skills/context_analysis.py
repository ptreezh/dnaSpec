"""
Context Analysis Skill - ç¬¦åˆDNASPECåŸå§‹è§„èŒƒçš„å®ç°
ä¸ºAI CLIå¹³å°æä¾›ä¸Šä¸‹æ–‡åˆ†æèƒ½åŠ›ï¼Œä¸åŸå§‹æŠ€èƒ½ä¿æŒä¸€è‡´çš„æ¥å£
"""
from typing import Dict, Any
import re


def execute(args: Dict[str, Any]) -> str:
    """
    æ‰§è¡Œä¸Šä¸‹æ–‡åˆ†æ - ä¸DNASPECåŸå§‹æŠ€èƒ½æ¥å£ä¿æŒä¸€è‡´
    """
    context = args.get("context", "") or args.get("request", "") or args.get("description", "")
    
    if not context.strip():
        return "é”™è¯¯: æœªæä¾›è¦åˆ†æçš„ä¸Šä¸‹æ–‡"
    
    # æ‰§è¡Œä¸Šä¸‹æ–‡åˆ†æ - æ„é€ AIæŒ‡ä»¤
    analysis_result = _analyze_context_with_ai(context)
    
    # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
    output_lines = []
    output_lines.append("ä¸Šä¸‹æ–‡è´¨é‡åˆ†æç»“æœ:")
    output_lines.append(f"é•¿åº¦: {analysis_result['context_length']} å­—ç¬¦")
    output_lines.append(f"Tokenä¼°ç®—: {analysis_result['token_count_estimate']}")
    output_lines.append("")
    
    output_lines.append("äº”ç»´è´¨é‡æŒ‡æ ‡ (0.0-1.0):")
    metric_names = {
        'clarity': 'æ¸…æ™°åº¦', 
        'relevance': 'ç›¸å…³æ€§',
        'completeness': 'å®Œæ•´æ€§', 
        'consistency': 'ä¸€è‡´æ€§',
        'efficiency': 'æ•ˆç‡'
    }
    
    for metric, score in analysis_result['metrics'].items():
        indicator = "ğŸŸ¢" if score >= 0.7 else "ğŸŸ¡" if score >= 0.4 else "ğŸ”´"
        output_lines.append(f"  {indicator} {metric_names.get(metric, metric)}: {score:.2f}")
    
    if analysis_result['suggestions']:
        output_lines.append("\nä¼˜åŒ–å»ºè®®:")
        for suggestion in analysis_result['suggestions'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå»ºè®®
            output_lines.append(f"  â€¢ {suggestion}")
    
    if analysis_result['issues']:
        output_lines.append("\nè¯†åˆ«é—®é¢˜:")
        for issue in analysis_result['issues']:
            output_lines.append(f"  â€¢ {issue}")
    
    return "\n".join(output_lines)


def _analyze_context_with_ai(context: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨AIæ¨¡å‹åˆ†æä¸Šä¸‹æ–‡è´¨é‡ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰
    """
    # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™ä¼šè°ƒç”¨AI APIå¹¶è§£æå“åº”
    # å½“å‰å®ç°åŸºäºä¸Šä¸‹æ–‡ç‰¹å¾è¿›è¡Œæ¨¡æ‹Ÿåˆ†æ
    clarity = _analyze_clarity(context)
    relevance = _analyze_relevance(context)
    completeness = _analyze_completeness(context)
    consistency = _analyze_consistency(context)
    efficiency = _analyze_efficiency(context)
    
    # ç”Ÿæˆå»ºè®®
    suggestions = []
    if clarity < 0.7:
        suggestions.append("å¢åŠ æ›´æ˜ç¡®çš„æœ¯è¯­å’Œç›®æ ‡è¡¨è¿°")
    if completeness < 0.6:
        suggestions.append("è¡¥å……çº¦æŸæ¡ä»¶å’Œå…·ä½“è¦æ±‚")
    if relevance < 0.7:
        suggestions.append("æ˜ç¡®ç›®æ ‡å’Œä»»åŠ¡å…³è”æ€§")
    
    # è¯†åˆ«é—®é¢˜
    issues = []
    if "ä¹Ÿè®¸" in context or "å¯èƒ½" in context or "å¤§æ¦‚" in context:
        issues.append("åŒ…å«ä¸ç¡®å®šè¯æ±‡ï¼š'ä¹Ÿè®¸'ã€'å¯èƒ½'ã€'å¤§æ¦‚'")
    if len(context) < 20:
        issues.append("ä¸Šä¸‹æ–‡è¿‡çŸ­ï¼Œä¿¡æ¯ä¸è¶³")
    if "ä½†æ˜¯" in context and "å› æ­¤" not in context:
        issues.append("åŒ…å«è½¬æŠ˜ä½†ç¼ºå°‘ç»“è®ºé€»è¾‘")
    
    return {
        'context_length': len(context),
        'token_count_estimate': max(1, len(context) // 4),
        'metrics': {
            'clarity': round(clarity, 2),
            'relevance': round(relevance, 2),
            'completeness': round(completeness, 2),
            'consistency': round(consistency, 2),
            'efficiency': round(efficiency, 2)
        },
        'suggestions': suggestions,
        'issues': issues
    }


def _analyze_clarity(context: str) -> float:
    """åˆ†ææ¸…æ™°åº¦"""
    clear_indicators = ['è¯·', 'éœ€è¦', 'è¦æ±‚', 'ç›®æ ‡', 'ä»»åŠ¡', 'å®ç°', 'è®¾è®¡', 'åˆ†æ', 'å¦‚ä½•', 'æ€æ ·', 'æ˜ç¡®']
    unclear_indicators = ['ä¹Ÿè®¸', 'å¯èƒ½', 'å¤§æ¦‚', 'ä¼¼ä¹', 'æŸäº›', 'ä¸€äº›', 'éƒ¨åˆ†', 'ç­‰ç­‰']
    
    clear_count = sum(1 for indicator in clear_indicators if indicator in context)
    unclear_count = sum(1 for indicator in unclear_indicators if indicator in context)
    
    # åŸºäºå¥å­å’Œæ˜ç¡®æŒ‡ä»¤è¯è®¡ç®—æ¸…æ™°åº¦
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?;ï¼›]', context)
    sentence_count = len([s for s in sentences if s.strip() and len(s.strip()) > 3])
    
    clarity_score = min(1.0, (clear_count * 0.3 + sentence_count * 0.05) if sentence_count > 0 else 0)
    unclear_penalty = min(0.5, unclear_count * 0.2)
    
    return max(0.0, clarity_score - unclear_penalty)


def _analyze_relevance(context: str) -> float:
    """åˆ†æç›¸å…³æ€§"""
    task_indicators = ['ç³»ç»Ÿ', 'åŠŸèƒ½', 'ä»»åŠ¡', 'ç›®æ ‡', 'éœ€æ±‚', 'å®ç°', 'å¼€å‘', 'è®¾è®¡', 'åˆ†æ', 'ç®¡ç†', 'å¤„ç†', 'æ”¯æŒ']
    
    task_count = sum(1 for indicator in task_indicators if indicator in context)
    relevance_score = min(1.0, task_count * 0.15)
    
    return max(0.0, relevance_score)


def _analyze_completeness(context: str) -> float:
    """åˆ†æå®Œæ•´æ€§"""
    completeness_indicators = ['çº¦æŸ', 'æ¡ä»¶', 'è¦æ±‚', 'æ ‡å‡†', 'è§„èŒƒ', 'é™åˆ¶', 'å‡è®¾', 'å‰æ', 'çº¦æŸ', 'ç›®æ ‡', 'éªŒæ”¶']
    
    completeness_count = sum(1 for indicator in completeness_indicators if indicator in context)
    completeness_score = min(1.0, completeness_count * 0.15)
    
    return completeness_score


def _analyze_consistency(context: str) -> float:
    """åˆ†æä¸€è‡´æ€§"""
    # æ£€æŸ¥é€»è¾‘çŸ›ç›¾
    contradiction_pairs = [
        ('å¿…é¡»', 'å¯é€‰'),
        ('åº”è¯¥', 'ä¸å¿…'),
        ('æ€»æ˜¯', 'ä»ä¸'),
        ('å…¨éƒ¨', 'éƒ¨åˆ†'),
        ('å¼ºåˆ¶', 'éšæ„'),
        ('è¦æ±‚', 'å¯é€‰'),
        ('å¿…é¡»', 'å¯ä»¥')
    ]
    
    contradiction_count = 0
    for positive, negative in contradiction_pairs:
        if positive in context and negative in context:
            contradiction_count += 1
    
    # ä¸€è‡´æ€§è¶Šé«˜åˆ†æ•°è¶Šé«˜ï¼ŒçŸ›ç›¾è¶Šå¤šåˆ†æ•°è¶Šä½
    consistency_score = max(0.0, 1.0 - (contradiction_count * 0.2))
    return consistency_score


def _analyze_efficiency(context: str) -> float:
    """åˆ†ææ•ˆç‡ï¼ˆä¿¡æ¯å¯†åº¦ï¼‰"""
    if len(context) == 0:
        return 0.0
    
    # è®¡ç®—ä¿¡æ¯å¯†åº¦ï¼šæœ‰æ•ˆè¯æ±‡æ•° / æ€»å­—ç¬¦æ•°
    words = [w for w in re.findall(r'[\w\u4e00-\u9fff]+', context) if len(w) > 1]
    efficiency = len(words) / len(context) * 100
    
    # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´ï¼ˆå‡è®¾æ¯100å­—ç¬¦ç†æƒ³æœ‰25ä¸ªæœ‰æ•ˆè¯ä¸ºæ»¡åˆ†ï¼‰
    normalized_efficiency = min(1.0, efficiency / 25)
    
    return max(0.0, normalized_efficiency)