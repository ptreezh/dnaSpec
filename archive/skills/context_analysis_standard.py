"""
DNASPEC ä¸Šä¸‹æ–‡åˆ†ææŠ€èƒ½ - ç¬¦åˆClaude Skillsæ ‡å‡†è§„èŒƒ
"""
from typing import Dict, Any, Union
import re
from datetime import datetime

class ClaudeContextAnalysisSkill:
    """
    Claude Skillsæ ‡å‡†ä¸Šä¸‹æ–‡åˆ†ææŠ€èƒ½
    éµå¾ªæ¸è¿›æŠ«éœ²ã€æœ€å°è®¤çŸ¥è´Ÿè·ã€å·¥å…·åŒ–æ€ç»´ã€å®šæ€§å®šé‡ç»“åˆåŸåˆ™
    """
    
    def __init__(self):
        self.name = "dnaspec-context-analysis"
        self.description = "ä½¿ç”¨å®ªæ³•åŸåˆ™åˆ†æä¸Šä¸‹æ–‡è´¨é‡çš„æŠ€èƒ½ï¼Œæä¾›äº”ç»´è´¨é‡è¯„ä¼°"
        self.version = "1.0.0"
        self.created_at = datetime.now().isoformat()
    
    def execute(self, args: Dict[str, Any]) -> str:
        """Claude Skillsæ ‡å‡†æ‰§è¡Œå…¥å£"""
        context = args.get("context", "") or args.get("request", "") or args.get("input", "")
        
        if not context.strip():
            return "âŒ æœªæä¾›è¦åˆ†æçš„ä¸Šä¸‹æ–‡ï¼Œè¯·æä¾›contextå‚æ•°"
        
        # æ‰§è¡Œå®šé‡åˆ†æï¼ˆç¨‹åºåŒ–ï¼‰
        analysis_data = self._perform_quantitative_analysis(context)
        
        # åº”ç”¨AIå®šæ€§è§£é‡Šå’Œå†³ç­–ï¼ˆAIåŸç”Ÿèƒ½åŠ›ï¼‰
        qualitative_insights = self._get_qualitative_insights(analysis_data, context)
        
        # æ ¼å¼åŒ–è¾“å‡ºï¼ˆæ¸è¿›æŠ«éœ²ï¼‰
        return self._format_progressive_output(analysis_data, qualitative_insights, args)
    
    def _perform_quantitative_analysis(self, context: str) -> Dict[str, Any]:
        """æ‰§è¡Œå®šé‡åˆ†æ - ç¨‹åºåŒ–é€»è¾‘"""
        # å®šé‡æŒ‡æ ‡è®¡ç®—
        context_length = len(context)
        token_estimate = max(1, len(context) // 4)
        
        # æ¸…æ™°åº¦è¯„ä¼°
        has_goals = bool(re.search(r'(ç›®æ ‡|ç›®çš„|goal|objective|éœ€è¦|requirement|éœ€æ±‚)', context, re.IGNORECASE))
        has_requirements = bool(re.search(r'(è¦æ±‚|æ¡ä»¶|constraint|requirement|constraint|é™åˆ¶|çº¦æŸ)', context, re.IGNORECASE))
        has_structure = bool(re.search(r'(#|\d+\.|[â€¢\-â€¢â—‹â–ª])', context, re.MULTILINE))
        
        clarity_score = min(1.0, 0.3 + (0.3 if has_goals else 0) + (0.2 if has_requirements else 0) + (0.2 if has_structure else 0))
        
        # ç›¸å…³æ€§è¯„ä¼°
        task_indicators = ['ç³»ç»Ÿ', 'åŠŸèƒ½', 'ä»»åŠ¡', 'ç›®æ ‡', 'å®ç°', 'å¼€å‘', 'è®¾è®¡', 'åˆ†æ', 'ç®¡ç†', 'å¤„ç†', 'æ”¯æŒ']
        relevant_indicators = sum(1 for indicator in task_indicators if indicator in context)
        relevance_score = min(1.0, (relevant_indicators * 0.15) if relevant_indicators > 0 else 0.1)
        
        # å®Œæ•´æ€§è¯„ä¼°
        completeness_indicators = ['çº¦æŸ', 'æ¡ä»¶', 'è¦æ±‚', 'æ ‡å‡†', 'è§„èŒƒ', 'é™åˆ¶', 'å‡è®¾', 'å‰æ', 'ç›®æ ‡', 'éªŒæ”¶']
        completeness_count = sum(1 for indicator in completeness_indicators if indicator in context)
        completeness_score = min(1.0, (completeness_count * 0.15) if completeness_count > 0 else 0.2)
        
        # ä¸€è‡´æ€§è¯„ä¼°ï¼ˆæ£€æŸ¥çŸ›ç›¾è¯æ±‡ï¼‰
        contradiction_pairs = [
            ('å¿…é¡»', 'å¯é€‰'), ('åº”è¯¥', 'ä¸å¿…'), ('æ€»æ˜¯', 'ä»ä¸'), ('å…¨éƒ¨', 'éƒ¨åˆ†'),
            ('å¼ºåˆ¶', 'éšæ„'), ('è¦æ±‚', 'å¯é€‰'), ('å¿…é¡»', 'å¯ä»¥')
        ]
        contradiction_count = sum(1 for pos, neg in contradiction_pairs if pos in context and neg in context)
        consistency_score = max(0.0, min(1.0, 0.9 - (contradiction_count * 0.2)))
        
        # æ•ˆç‡è¯„ä¼°ï¼ˆä¿¡æ¯å¯†åº¦ï¼‰
        words = [w for w in re.findall(r'[\w\u4e00-\u9fff]+', context) if len(w) > 1]
        word_count = len(words)
        efficiency_score = min(1.0, word_count / (len(context) / 5 + 1) if context else 0)
        
        # ç”Ÿæˆå»ºè®®ï¼ˆå®šé‡ï¼‰
        suggestions = []
        if not has_goals:
            suggestions.append("å¢åŠ æ›´æ˜ç¡®çš„ç›®æ ‡æè¿°")
        if not has_requirements:
            suggestions.append("è¡¥å……å…·ä½“çš„çº¦æŸæ¡ä»¶å’Œè¦æ±‚")
        if contradiction_count > 0:
            suggestions.append(f"è§£å†³æ£€æµ‹åˆ°çš„{contradiction_count}ä¸ªé€»è¾‘çŸ›ç›¾")
        
        # è¯†åˆ«é—®é¢˜ï¼ˆå®šé‡ï¼‰
        issues = []
        if contradiction_count > 0:
            issues.append(f"å‘ç°{contradiction_count}ä¸ªé€»è¾‘çŸ›ç›¾")
        if len(context) < 20:
            issues.append("ä¸Šä¸‹æ–‡è¿‡çŸ­ï¼Œä¿¡æ¯ä¸è¶³")
        if not has_structure:
            issues.append("ç¼ºä¹æ¸…æ™°çš„ç»“æ„")
        
        return {
            'context_length': context_length,
            'token_count_estimate': token_count,
            'metrics': {
                'clarity': round(clarity_score, 2),
                'relevance': round(relevance_score, 2),
                'completeness': round(completeness_score, 2),
                'consistency': round(consistency_score, 2),
                'efficiency': round(efficiency_score, 2)
            },
            'suggestions': suggestions,
            'issues': issues,
            'indicators': {
                'has_goals': has_goals,
                'has_requirements': has_requirements,
                'has_structure': has_structure,
                'contradiction_count': contradiction_count
            }
        }
    
    def _get_qualitative_insights(self, analysis_data: Dict[str, Any], original_context: str) -> Dict[str, Any]:
        """è·å–å®šæ€§è§è§£ - åˆ©ç”¨AIæ¨¡å‹åŸç”Ÿæ™ºèƒ½"""
        # è¿™é‡Œæ˜¯ç¤ºæ„ï¼Œå®é™…åœ¨Claude CLIç¯å¢ƒä¸­ï¼ŒAIæ¨¡å‹ä¼šç›´æ¥å¤„ç†æç¤ºè¯
        # ä½†åœ¨å½“å‰å®ç°ä¸­ï¼Œæˆ‘ä»¬ä¼šåŸºäºå®šé‡åˆ†ææä¾›æ™ºèƒ½è§£é‡Š
        
        insights = {
            'interpretation': self._interpret_metrics(analysis_data['metrics']),
            'recommendations': self._generate_recommendations(analysis_data),
            'confidence_levels': self._assess_confidence(analysis_data),
            'critical_issues': self._identify_critical_issues(analysis_data)
        }
        
        return insights
    
    def _interpret_metrics(self, metrics: Dict[str, float]) -> str:
        """è§£é‡Šè´¨é‡æŒ‡æ ‡"""
        interpretation = []
        
        for metric, score in metrics.items():
            metric_names = {
                'clarity': 'æ¸…æ™°åº¦',
                'relevance': 'ç›¸å…³æ€§', 
                'completeness': 'å®Œæ•´æ€§',
                'consistency': 'ä¸€è‡´æ€§',
                'efficiency': 'æ•ˆç‡'
            }
            name = metric_names.get(metric, metric)
            
            if score >= 0.8:
                interpretation.append(f"{name}ä¼˜ç§€(â‰¥0.8)")
            elif score >= 0.6:
                interpretation.append(f"{name}è‰¯å¥½(0.6-0.79)")
            elif score >= 0.4:
                interpretation.append(f"{name}ä¸€èˆ¬(0.4-0.59)")
            else:
                interpretation.append(f"{name}è¾ƒå·®(<0.4)")
        
        return "; ".join(interpretation)
    
    def _generate_recommendations(self, analysis_data: Dict[str, Any]) -> list:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        metrics = analysis_data['metrics']
        
        if metrics['clarity'] < 0.6:
            recommendations.append("ä½¿ç”¨æ›´æ˜ç¡®çš„ç›®æ ‡å’Œæœ¯è¯­")
        if metrics['relevance'] < 0.7:
            recommendations.append("æ˜ç¡®ä»»åŠ¡ç›¸å…³æ€§")
        if metrics['completeness'] < 0.6:
            recommendations.append("è¡¥å……çº¦æŸæ¡ä»¶å’Œå…·ä½“è¦æ±‚")
        if metrics['consistency'] < 0.8:
            recommendations.append("æ£€æŸ¥å¹¶è§£å†³é€»è¾‘çŸ›ç›¾")
        
        return recommendations
    
    def _assess_confidence(self, analysis_data: Dict[str, Any]) -> Dict[str, str]:
        """è¯„ä¼°åˆ†æç½®ä¿¡åº¦"""
        confidences = {}
        metrics = analysis_data['metrics']
        
        for metric, score in metrics.items():
            if score >= 0.8:
                confidences[metric] = "é«˜ç½®ä¿¡åº¦"
            elif score >= 0.6:
                confidences[metric] = "ä¸­ç­‰ç½®ä¿¡åº¦"
            else:
                confidences[metric] = "ä½ç½®ä¿¡åº¦"
        
        return confidences
    
    def _identify_critical_issues(self, analysis_data: Dict[str, Any]) -> list:
        """è¯†åˆ«å…³é”®é—®é¢˜"""
        critical_issues = []
        
        if analysis_data['metrics']['consistency'] < 0.5:
            critical_issues.append("ä¸¥é‡ä¸€è‡´æ€§é—®é¢˜")
        if analysis_data['context_length'] < 15:
            critical_issues.append("å†…å®¹è¿‡çŸ­")
        if analysis_data['indicators']['contradiction_count'] > 2:
            critical_issues.append("å¤šé‡é€»è¾‘çŸ›ç›¾")
        
        return critical_issues if critical_issues else ["æ— å…³é”®é—®é¢˜"]
    
    def _format_progressive_output(self, analysis_data: Dict[str, Any], insights: Dict[str, Any], args: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ¸è¿›æŠ«éœ²è¾“å‡º - ç¬¦åˆClaude Skillsè§„èŒƒ"""
        detailed = args.get("detailed", False)
        
        output_lines = []
        
        # åŸºæœ¬ä¿¡æ¯ï¼ˆæœ€ä½è®¤çŸ¥è´Ÿè·ï¼‰
        output_lines.append("ğŸ“‹ ä¸Šä¸‹æ–‡è´¨é‡åˆ†æç»“æœ")
        output_lines.append(f"é•¿åº¦: {analysis_data['context_length']} å­—ç¬¦")
        output_lines.append(f"Tokenä¼°ç®—: {analysis_data['token_count_estimate']}")
        output_lines.append("")
        
        # æ ¸å¿ƒæŒ‡æ ‡ï¼ˆå…³é”®ä¿¡æ¯ä¼˜å…ˆï¼‰
        output_lines.append("äº”ç»´è´¨é‡æŒ‡æ ‡ (0.0-1.0):")
        metric_names = {
            'clarity': 'æ¸…æ™°åº¦',
            'relevance': 'ç›¸å…³æ€§',
            'completeness': 'å®Œæ•´æ€§', 
            'consistency': 'ä¸€è‡´æ€§',
            'efficiency': 'æ•ˆç‡'
        }
        
        for metric, score in analysis_data['metrics'].items():
            indicator = "ğŸŸ¢" if score >= 0.7 else "ğŸŸ¡" if score >= 0.4 else "ğŸ”´"
            output_lines.append(f"  {indicator} {metric_names.get(metric, metric)}: {score:.2f}")
        
        output_lines.append("")
        
        # æŒ‰éœ€æä¾›è¯¦ç»†ä¿¡æ¯
        if detailed:
            # è§£é‡Šå’Œå»ºè®®ï¼ˆAIå®šæ€§åˆ†æï¼‰
            output_lines.append("ğŸ” è´¨é‡è§£è¯»:")
            output_lines.append(f"  {insights['interpretation']}")
            
            if analysis_data['suggestions']:
                output_lines.append("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                for suggestion in analysis_data['suggestions']:
                    output_lines.append(f"  â€¢ {suggestion}")
            
            if insights['recommendations']:
                output_lines.append("\nğŸ¯ AIæ¨èæ”¹è¿›:")
                for rec in insights['recommendations']:
                    output_lines.append(f"  â€¢ {rec}")
            
            if analysis_data['issues']:
                output_lines.append("\nâš ï¸  è¯†åˆ«é—®é¢˜:")
                for issue in analysis_data['issues']:
                    output_lines.append(f"  â€¢ {issue}")
            
            critical_issues = [issue for issue in insights['critical_issues'] if issue != "æ— å…³é”®é—®é¢˜"]
            if critical_issues:
                output_lines.append("\nğŸš¨ å…³é”®é—®é¢˜:")
                for issue in critical_issues:
                    output_lines.append(f"  â€¢ {issue}")
        else:
            # ç®€åŒ–ç‰ˆæœ¬ - åªæ˜¾ç¤ºæœ€å…³é”®ä¿¡æ¯
            suggestions = analysis_data['suggestions'][:2]  # åªæ˜¾ç¤ºå‰2ä¸ªå»ºè®®
            issues = analysis_data['issues'][:2]            # åªæ˜¾ç¤ºå‰2ä¸ªé—®é¢˜
            
            if suggestions:
                output_lines.append("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                for suggestion in suggestions:
                    output_lines.append(f"  â€¢ {suggestion}")
            
            if issues:
                output_lines.append("\nâš ï¸  ä¸»è¦é—®é¢˜:")
                for issue in issues:
                    output_lines.append(f"  â€¢ {issue}")
            
            # æä¾›è¯¦ç»†æ¨¡å¼æç¤º
            output_lines.append("\nğŸ’¡ ä½¿ç”¨ detailed=true å‚æ•°è·å–å®Œæ•´åˆ†æ")
        
        return "\n".join(output_lines)


# å®ä¾‹åŒ–æŠ€èƒ½
CONTEXT_ANALYSIS_SKILL = ClaudeContextAnalysisSkill()

def execute(args: Dict[str, Any]) -> str:
    """
    Claude Skillsæ ‡å‡†æ‰§è¡Œæ¥å£
    """
    return CONTEXT_ANALYSIS_SKILL.execute(args)

def get_manifest() -> Dict[str, Any]:
    """
    è·å–æŠ€èƒ½æ¸…å• - Claude Skillsæ ‡å‡†
    """
    return {
        "name": CONTEXT_ANALYSIS_SKILL.name,
        "description": CONTEXT_ANALYSIS_SKILL.description,
        "version": CONTEXT_ANALYSIS_SKILL.version,
        "created_at": CONTEXT_ANALYSIS_SKILL.created_at,
        "parameters": {
            "type": "object",
            "properties": {
                "context": {
                    "type": "string",
                    "description": "è¦åˆ†æçš„ä¸Šä¸‹æ–‡å†…å®¹"
                },
                "request": {
                    "type": "string", 
                    "description": "åˆ†æè¯·æ±‚ï¼ˆcontextçš„åˆ«åï¼‰"
                },
                "input": {
                    "type": "string",
                    "description": "è¾“å…¥å†…å®¹ï¼ˆcontextçš„åˆ«åï¼‰"
                },
                "detailed": {
                    "type": "boolean",
                    "description": "æ˜¯å¦è¿”å›è¯¦ç»†åˆ†æ",
                    "default": False
                }
            },
            "required": ["context"]
        }
    }