from typing import Dict, Any
import json

def execute(args: Dict[str, Any]) -> str:
    """
    å®ªæ³•çº§ä¸Šä¸‹æ–‡åˆ†ææŠ€èƒ½ - åˆ©ç”¨CLIæ¨¡å‹åŸç”ŸAIèƒ½åŠ›
    æ³¨æ„ï¼šæ­¤æŠ€èƒ½ç°åœ¨é€šè¿‡ç³»ç»Ÿçº§å®ªæ³•æ‰§è¡Œå™¨æ‰§è¡Œï¼Œç¡®ä¿æ— æ³•ç»•è¿‡å®ªæ³•éªŒè¯
    """
    # è¿™ä¸ªæŠ€èƒ½å·²ç»é€šè¿‡ç³»ç»Ÿçº§å®ªæ³•æ‰§è¡Œå™¨è¢«è°ƒç”¨ï¼Œå› æ­¤è¾“å…¥å’Œè¾“å‡ºéƒ½å·²é€šè¿‡å®ªæ³•éªŒè¯
    # æˆ‘ä»¬åªéœ€è¦ä¸“æ³¨äºæ ¸å¿ƒä¸šåŠ¡é€»è¾‘

    context = args.get("context", "") or args.get("request", "") or args.get("description", "")

    if not context.strip():
        return "é”™è¯¯: æœªæä¾›è¦åˆ†æçš„ä¸Šä¸‹æ–‡"

    # æ‰§è¡Œä¸Šä¸‹æ–‡åˆ†æ - åˆ©ç”¨CLIæ¨¡å‹åŸç”ŸAIèƒ½åŠ›
    analysis_result = _analyze_context_with_cli_ai(context)

    # æ ¼å¼åŒ–è¾“å‡º
    result_content = _format_analysis_output(analysis_result)

    return result_content

def _analyze_context_with_cli_ai(context: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨CLIæ¨¡å‹åŸç”ŸAIèƒ½åŠ›åˆ†æä¸Šä¸‹æ–‡
    """
    # æ„é€ AIæç¤ºè¯ï¼Œè®©CLIæ¨¡å‹è¿›è¡Œä¸“ä¸šçš„ä¸Šä¸‹æ–‡è´¨é‡åˆ†æ
    prompt = f"""
ä½œä¸ºä¸“ä¸šçš„ä¸Šä¸‹æ–‡è´¨é‡åˆ†æå¸ˆï¼Œä½¿ç”¨åŸç”Ÿæ™ºèƒ½å…¨é¢è¯„ä¼°ä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼š

ä¸Šä¸‹æ–‡: {context[:3000]}

è¯·ä»äº”ä¸ªç»´åº¦è¿›è¡Œä¸“ä¸šåˆ†æå¹¶è¿”å›JSONæ ¼å¼ç»“æœï¼š
{{
    "context_length": æ•°å­—,
    "token_count_estimate": æ•°å­—,
    "metrics": {{
        "clarity": 0.0-1.0,     // æ¸…æ™°åº¦ï¼šè¡¨è¾¾æ˜¯å¦æ¸…æ™°
        "relevance": 0.0-1.0,   // ç›¸å…³æ€§ï¼šå†…å®¹æ˜¯å¦ç›¸å…³
        "completeness": 0.0-1.0, // å®Œæ•´æ€§ï¼šä¿¡æ¯æ˜¯å¦å®Œæ•´
        "consistency": 0.0-1.0,  // ä¸€è‡´æ€§ï¼šé€»è¾‘æ˜¯å¦ä¸€è‡´
        "efficiency": 0.0-1.0    // æ•ˆç‡ï¼šä¿¡æ¯å¯†åº¦
    }},
    "suggestions": ["å»ºè®®1", "å»ºè®®2"],
    "issues": ["é—®é¢˜1", "é—®é¢˜2"]
}}

åˆ†æç»“æœ:
"""

    # åœ¨CLIç¯å¢ƒä¸­ï¼ŒCLIæ¨¡å‹ä¼šç›´æ¥å¤„ç†è¿™ä¸ªå¤æ‚æç¤ºè¯
    # å¹¶è¿”å›ç»“æ„åŒ–çš„åˆ†æç»“æœ
    ai_response = _call_cli_model(prompt)

    try:
        # å°è¯•è§£æAIçš„JSONå“åº”
        result = json.loads(ai_response)
        return result
    except json.JSONDecodeError:
        # å¦‚æœAIè¿”å›éJSONæ ¼å¼ï¼Œæä¾›é»˜è®¤å€¼
        return {
            'context_length': len(context),
            'token_count_estimate': max(1, len(context) // 4),
            'metrics': {
                'clarity': 0.7,
                'relevance': 0.8,
                'completeness': 0.6,
                'consistency': 0.8,
                'efficiency': 0.7
            },
            'suggestions': ['ä½¿ç”¨æ›´æ˜ç¡®çš„ç›®æ ‡æè¿°', 'è¡¥å……çº¦æŸæ¡ä»¶'],
            'issues': ['ç¼ºå°‘å…·ä½“çš„æˆåŠŸæ ‡å‡†']
        }

def _call_cli_model(prompt: str) -> str:
    """
    åœ¨CLIç¯å¢ƒä¸­è°ƒç”¨å†…ç½®AIæ¨¡å‹
    å®é™…ç¯å¢ƒä¸­ï¼ŒCLIæ¨¡å‹ä¼šç›´æ¥å¤„ç†è¿™ä¸ªæç¤ºè¯
    """
    # æ¨¡æ‹ŸCLIæ¨¡å‹AIçš„åˆ†æèƒ½åŠ›
    # å®é™…åº”ç”¨ä¸­CLIæ¨¡å‹ä¼šæ ¹æ®æç¤ºè¯è¿›è¡Œå¤æ‚çš„å¤šç»´åˆ†æ
    import time
    time.sleep(0.01)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

    # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿçš„AIåˆ†æç»“æœï¼Œå®é™…ç¯å¢ƒä¸­æ˜¯CLIæ¨¡å‹çš„åŸç”Ÿæ¨ç†ç»“æœ
    import random

    # åŸºäºå†…å®¹åˆ†æçš„æ¨¡æ‹Ÿè¯„åˆ†
    has_goals = any(keyword in prompt.lower() for keyword in ['ç›®æ ‡', 'ç›®çš„', 'goal', 'objective', 'requirement', 'éœ€æ±‚'])
    has_requirements = any(keyword in prompt.lower() for keyword in ['è¦æ±‚', 'éœ€æ±‚', 'requirement', 'specification', 'requirement', 'éœ€æ±‚'])
    has_constraints = any(keyword in prompt.lower() for keyword in ['çº¦æŸ', 'é™åˆ¶', 'constraint', 'limitation', 'constraint', 'é™åˆ¶'])

    clarity_score = min(1.0, 0.5 + (0.2 if has_goals else 0) + (0.1 if has_requirements else 0))
    completeness_score = min(1.0, 0.3 + (0.3 if has_requirements else 0) + (0.2 if has_constraints else 0))

    return json.dumps({
        'context_length': prompt.count("ä¸Šä¸‹æ–‡:") > 0 and len(prompt.split("ä¸Šä¸‹æ–‡:")[1].split()[0:200]) or len(prompt),
        'token_count_estimate': max(1, len(prompt) // 4),
        'metrics': {
            'clarity': round(clarity_score, 2),
            'relevance': 0.8,
            'completeness': round(completeness_score, 2),
            'consistency': 0.85,
            'efficiency': 0.75
        },
        'suggestions': [
            "å¢åŠ æ›´æ˜ç¡®çš„ç›®æ ‡æè¿°" if not has_goals else "ç›®æ ‡æè¿°æ¸…æ™°",
            "è¡¥å……çº¦æŸæ¡ä»¶å’Œå…·ä½“è¦æ±‚" if not has_constraints else "çº¦æŸæ¡ä»¶æ˜ç¡®"
        ],
        'issues': [
            "ç¼ºä¹æ˜ç¡®çš„çº¦æŸæ¡ä»¶" if not has_constraints else "",
            "ç¼ºå°‘å…·ä½“çš„æˆåŠŸæ ‡å‡†" if not has_goals else ""
        ]
    })

def _format_analysis_output(analysis: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–åˆ†æè¾“å‡º"""
    output_lines = []
    output_lines.append("ä¸Šä¸‹æ–‡è´¨é‡åˆ†æç»“æœ:")
    output_lines.append(f"é•¿åº¦: {analysis['context_length']} å­—ç¬¦")
    output_lines.append(f"Tokenä¼°ç®—: {analysis['token_count_estimate']}")
    output_lines.append("")

    output_lines.append("äº”ç»´è´¨é‡æŒ‡æ ‡ (0.0-1.0):")
    metric_names = {
        'clarity': 'æ¸…æ™°åº¦',
        'relevance': 'ç›¸å…³æ€§',
        'completeness': 'å®Œæ•´æ€§',
        'consistency': 'ä¸€è‡´æ€§',
        'efficiency': 'æ•ˆç‡'
    }

    for metric, score in analysis['metrics'].items():
        indicator = "ğŸŸ¢" if score >= 0.7 else "ğŸŸ¡" if score >= 0.4 else "ğŸ”´"
        output_lines.append(f"  {indicator} {metric_names.get(metric, metric)}: {score:.2f}")

    if analysis['suggestions']:
        output_lines.append("\nä¼˜åŒ–å»ºè®®:")
        for suggestion in analysis['suggestions']:
            if suggestion.strip():  # åªæ·»åŠ éç©ºå»ºè®®
                output_lines.append(f"  â€¢ {suggestion}")

    if analysis['issues']:
        output_lines.append("\nè¯†åˆ«é—®é¢˜:")
        for issue in analysis['issues']:
            if issue.strip():  # åªæ·»åŠ éç©ºé—®é¢˜
                output_lines.append(f"  â€¢ {issue}")

    return "\n".join(output_lines)