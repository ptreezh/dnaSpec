"""
Context Analysis Skill
åŸºäºAIæ¨¡å‹çš„ä¸Šä¸‹æ–‡åˆ†ææŠ€èƒ½
"""
from typing import Dict, Any
from ..core_skill import ContextEngineeringSkill, SkillResult
from ..ai_client import AIModelClient
from ..instruction_template import TemplateRegistry


class ContextAnalysisSkill(ContextEngineeringSkill):
    """ä¸Šä¸‹æ–‡åˆ†ææŠ€èƒ½ - åˆ†æä¸Šä¸‹æ–‡è´¨é‡çš„äº”ç»´æŒ‡æ ‡"""
    
    def __init__(self, ai_client: AIModelClient, template_registry: TemplateRegistry):
        super().__init__(
            name="dnaspec-context-analysis",
            description="ä¸Šä¸‹æ–‡åˆ†ææŠ€èƒ½ - ä¸“ä¸šåˆ†æä¸Šä¸‹æ–‡è´¨é‡çš„äº”ç»´æŒ‡æ ‡",
            ai_client=ai_client,
            template_registry=template_registry
        )
    
    def execute(self, context: str, params: Dict[str, Any] = None) -> SkillResult:
        """æ‰§è¡Œä¸Šä¸‹æ–‡åˆ†æ"""
        # éªŒè¯è¾“å…¥
        validation_error = self.validate_input(context, params)
        if validation_error:
            return SkillResult(
                success=False,
                data={},
                error=validation_error
            )
        
        # è®¾ç½®é»˜è®¤å‚æ•°
        if params is None:
            params = {}
        
        # æ„é€ åˆ†ææŒ‡ä»¤
        language = params.get('language', 'Chinese')
        instruction = self.template_registry.create_prompt(
            'context-analysis',
            context,
            {'language': language}
        )
        
        # å‘é€åˆ°AIæ¨¡å‹å¹¶è·å–ç»“æœ
        response = self.ai_client.send_instruction(instruction)
        
        # è§£æAIå“åº”
        try:
            parsed_result = self.template_registry.parse_response('context-analysis', response)
            
            # è®¡ç®—æ€»ä½“ç½®ä¿¡åº¦ï¼ˆåŸºäºå„æŒ‡æ ‡çš„ä¸€è‡´æ€§ï¼‰
            metrics = parsed_result.get('metrics', {})
            if metrics:
                avg_score = sum(metrics.values()) / len(metrics) if metrics else 0.5
                confidence = min(1.0, avg_score + 0.2)  # åŸºç¡€ç½®ä¿¡åº¦
            else:
                confidence = 0.5  # é»˜è®¤ç½®ä¿¡åº¦
            
            return SkillResult(
                success=True,
                data={
                    'context_length': len(context),
                    'token_count': self._estimate_tokens(context),
                    'metrics': parsed_result.get('metrics', {}),
                    'suggestions': parsed_result.get('suggestions', []),
                    'issues': parsed_result.get('issues', []),
                    'raw_response': response
                },
                confidence=confidence
            )
            
        except Exception as e:
            return SkillResult(
                success=False,
                data={'raw_response': response},
                error=f"Failed to parse AI response: {str(e)}"
            )
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼‰"""
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡æŒ‰æ¯4å­—ç¬¦çº¦1tokenï¼Œè‹±æ–‡æŒ‰æ¯4å­—ç¬¦çº¦1token
        if len(text) == 0:
            return 0
        
        # æ›´å‡†ç¡®çš„ä¼°ç®—æ–¹æ³•ï¼Œè€ƒè™‘ä¸­è‹±æ–‡æ··åˆ
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        english_words = len([w for w in text.split() if w.isascii()])
        other_chars = len(text) - chinese_chars - len(' '.join(text.split()))
        
        # ä¼°ç®—ï¼šä¸­æ–‡4å­—â‰ˆ1tokenï¼Œè‹±æ–‡å•è¯â‰ˆ1tokenï¼Œå…¶ä»–10å­—ç¬¦â‰ˆ1token
        estimated_tokens = (chinese_chars // 4) + english_words + (other_chars // 10)
        return max(1, estimated_tokens)
    
    def validate_input(self, context: str, params: Dict[str, Any] = None) -> str:
        """éªŒè¯è¾“å…¥å‚æ•°"""
        if not context or not context.strip():
            return "Context cannot be empty"
        
        if len(context) > 50000:  # é™åˆ¶æœ€å¤§é•¿åº¦
            return "Context too long (max 50000 characters)"
        
        params = params or {}
        language = params.get('language', 'Chinese')
        if language not in ['Chinese', 'English']:
            return f"Unsupported language: {language}. Supported: Chinese, English"
        
        return None  # æ— é”™è¯¯


# ä¾¿æ·çš„æ‰§è¡Œå‡½æ•°ï¼Œå…¼å®¹ç°æœ‰æ¥å£
def execute(args: Dict[str, Any]) -> str:
    """
    æ‰§è¡Œä¸Šä¸‹æ–‡åˆ†ææŠ€èƒ½ï¼ˆå…¼å®¹å‡½æ•°ï¼‰
    """
    context = args.get('context', '') or args.get('request', '')
    if not context:
        return "Error: No context provided for analysis"
    
    # æ³¨æ„ï¼šåœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™é‡Œéœ€è¦çœŸå®çš„AIå®¢æˆ·ç«¯å’Œæ¨¡æ¿æ³¨å†Œè¡¨
    # ç°åœ¨æˆ‘ä»¬æä¾›ä¸€ä¸ªæ¨¡æ‹Ÿç‰ˆæœ¬ç”¨äºæ¼”ç¤º
    try:
        # æ¨¡æ‹Ÿåˆ†æç»“æœ
        mock_result = {
            'context_length': len(context),
            'token_count': max(1, len(context) // 4),
            'metrics': {
                'clarity': 0.7,
                'relevance': 0.85, 
                'completeness': 0.6,
                'consistency': 0.9,
                'efficiency': 0.8
            },
            'suggestions': [
                "å¢åŠ æ›´æ˜ç¡®çš„ç›®æ ‡æè¿°",
                "æ·»åŠ çº¦æŸæ¡ä»¶è¯´æ˜", 
                "æé«˜è¡¨è¾¾æ¸…æ™°åº¦"
            ],
            'issues': [
                "ç¼ºå°‘å…³é”®çº¦æŸæ¡ä»¶"
            ]
        }
        
        # æ ¼å¼åŒ–è¾“å‡º
        output_lines = []
        output_lines.append("ä¸Šä¸‹æ–‡åˆ†æç»“æœ:")
        output_lines.append(f"é•¿åº¦: {mock_result['context_length']} å­—ç¬¦")
        output_lines.append(f"Tokenä¼°ç®—: {mock_result['token_count']}")
        output_lines.append("")
        output_lines.append("äº”ç»´æŒ‡æ ‡åˆ†æ:")
        
        for metric, score in mock_result['metrics'].items():
            metric_names = {
                'clarity': 'æ¸…æ™°åº¦',
                'relevance': 'ç›¸å…³æ€§', 
                'completeness': 'å®Œæ•´æ€§',
                'consistency': 'ä¸€è‡´æ€§', 
                'efficiency': 'æ•ˆç‡'
            }
            indicator = "ğŸŸ¢" if score >= 0.7 else "ğŸŸ¡" if score >= 0.4 else "ğŸ”´"
            output_lines.append(f"  {indicator} {metric_names.get(metric, metric)}: {score:.2f}")
        
        if mock_result['suggestions']:
            output_lines.append("\nä¼˜åŒ–å»ºè®®:")
            for suggestion in mock_result['suggestions']:
                output_lines.append(f"  â€¢ {suggestion}")
        
        if mock_result['issues']:
            output_lines.append("\nè¯†åˆ«é—®é¢˜:")
            for issue in mock_result['issues']:
                output_lines.append(f"  â€¢ {issue}")
        
        return "\n".join(output_lines)
        
    except Exception as e:
        return f"Analysis failed: {str(e)}"