"""
Context Fundamentals Skill - End-to-End Test

å®Œæ•´æµ‹è¯•dnaspec-context-fundamentalsæŠ€èƒ½çš„æ‰€æœ‰åŠŸèƒ½
"""

import sys
from pathlib import Path
import json

# æ·»åŠ skillsç›®å½•åˆ°path
skills_dir = Path(__file__).parent / "skills" / "dnaspec-context-fundamentals"
sys.path.insert(0, str(skills_dir))

from scripts.executor import ContextFundamentalsExecutor


class TestResult:
    """æµ‹è¯•ç»“æœ"""
    def __init__(self, test_name: str):
        self.test_name = test_name
        self.passed = False
        self.message = ""
        self.details = {}

    def __str__(self):
        status = "âœ… PASS" if self.passed else "âŒ FAIL"
        return f"{status} - {self.test_name}: {self.message}"


def test_1_prompt_files_loading():
    """æµ‹è¯•1ï¼šæç¤ºè¯æ–‡ä»¶åŠ è½½"""
    result = TestResult("æç¤ºè¯æ–‡ä»¶åŠ è½½")

    try:
        executor = ContextFundamentalsExecutor(skills_dir)

        # æµ‹è¯•åŠ è½½æ‰€æœ‰4å±‚æç¤ºè¯
        levels = ["00", "01", "02", "03"]
        for level in levels:
            prompt_content = executor._load_prompt(level)
            if not prompt_content or len(prompt_content) == 0:
                result.message = f"Level {level} æç¤ºè¯ä¸ºç©º"
                return result

            # æ£€æŸ¥é•¿åº¦èŒƒå›´
            expected_max = {
                "00": 500,
                "01": 1000,
                "02": 2000,
                "03": 3000
            }.get(level, 5000)

            if len(prompt_content) > expected_max * 1.5:  # å…è®¸50%å®¹å·®
                result.message = f"Level {level} æç¤ºè¯è¿‡é•¿: {len(prompt_content)} > {expected_max}"
                return result

        result.passed = True
        result.message = f"æˆåŠŸåŠ è½½{len(levels)}å±‚æç¤ºè¯"
        result.details = {
            "levels": levels,
            "total_characters": sum(len(executor._load_prompt(level)) for level in levels)
        }

    except Exception as e:
        result.message = f"åŠ è½½å¤±è´¥: {str(e)}"

    return result


def test_2_input_validation():
    """æµ‹è¯•2ï¼šè¾“å…¥éªŒè¯"""
    result = TestResult("è¾“å…¥éªŒè¯")

    try:
        executor = ContextFundamentalsExecutor(skills_dir)

        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "name": "æ­£å¸¸è¯·æ±‚",
                "request": "ä»€ä¹ˆæ˜¯ä¸Šä¸‹æ–‡ï¼Ÿä¸ºä»€ä¹ˆå®ƒé‡è¦ï¼Ÿ",
                "should_pass": True
            },
            {
                "name": "ç©ºè¯·æ±‚",
                "request": "",
                "should_pass": False
            },
            {
                "name": "è¿‡çŸ­è¯·æ±‚",
                "request": "å•Šï¼Ÿ",
                "should_pass": False
            },
            {
                "name": "å¤šé—®é¢˜è¯·æ±‚",
                "request": "ä»€ä¹ˆæ˜¯ä¸Šä¸‹æ–‡ï¼Ÿå¦‚ä½•ä¼˜åŒ–ï¼Ÿtokené™åˆ¶æ˜¯å¤šå°‘ï¼Ÿæœ‰å“ªäº›æœ€ä½³å®è·µï¼Ÿ",
                "should_pass": True  # æœ‰æ•ˆä½†ä¼šæœ‰è­¦å‘Š
            }
        ]

        passed_cases = 0
        validation_results = []

        for test_case in test_cases:
            validation_result = executor.validator.validate(
                test_case["request"],
                None
            )

            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆé¢„æœŸ
            is_valid = validation_result.is_valid
            expected_valid = test_case["should_pass"]

            if is_valid == expected_valid or (is_valid and not expected_valid and validation_result.has_warnings()):
                passed_cases += 1
                validation_results.append({
                    "case": test_case["name"],
                    "status": "PASS",
                    "is_valid": is_valid
                })
            else:
                validation_results.append({
                    "case": test_case["name"],
                    "status": "FAIL",
                    "is_valid": is_valid,
                    "expected": test_case["should_pass"]
                })

        if passed_cases == len(test_cases):
            result.passed = True
            result.message = f"æ‰€æœ‰{len(test_cases)}ä¸ªæµ‹è¯•ç”¨ä¾‹é€šè¿‡"
        else:
            result.message = f"{passed_cases}/{len(test_cases)} æµ‹è¯•ç”¨ä¾‹é€šè¿‡"

        result.details = {
            "test_cases": validation_results
        }

    except Exception as e:
        result.message = f"éªŒè¯å¤±è´¥: {str(e)}"

    return result


def test_3_metrics_calculation():
    """æµ‹è¯•3ï¼šæŒ‡æ ‡è®¡ç®—"""
    result = TestResult("æŒ‡æ ‡è®¡ç®—")

    try:
        executor = ContextFundamentalsExecutor(skills_dir)

        # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„è¯·æ±‚
        test_cases = [
            {
                "name": "ç®€å•è¯·æ±‚",
                "request": "ä»€ä¹ˆæ˜¯ä¸Šä¸‹æ–‡ï¼Ÿ",
                "expected_complexity_range": (0.0, 0.3)
            },
            {
                "name": "ä¸­ç­‰å¤æ‚åº¦",
                "request": "å¦‚ä½•åœ¨AIç³»ç»Ÿä¸­ä¼˜åŒ–ä¸Šä¸‹æ–‡ç®¡ç†ï¼Ÿè¯·è¯´æ˜æœ€ä½³å®è·µå’Œå¸¸è§é™·é˜±",
                "expected_complexity_range": (0.3, 0.5)
            },
            {
                "name": "é«˜åº¦å¤æ‚",
                "request": "è®¾è®¡ä¸€ä¸ªåŒ…å«50ä¸ªå¾®æœåŠ¡çš„å¤§å‹ç”µå•†ç³»ç»Ÿçš„ä¸Šä¸‹æ–‡ç®¡ç†æ¶æ„ï¼Œéœ€è¦è€ƒè™‘åˆ†å¸ƒå¼åä½œã€ç‰ˆæœ¬æ§åˆ¶ã€æ€§èƒ½ä¼˜åŒ–ã€å®‰å…¨æ€§ç­‰å¤šä¸ªæ–¹é¢ï¼ŒåŒæ—¶è¦å¤„ç†Lost-in-the-Middleç°è±¡ã€ä¸Šä¸‹æ–‡æ¯’åŒ–ã€åˆ†å¿ƒã€å†²çªç­‰å¤šç§å¤±æ•ˆæ¨¡å¼ï¼Œè¿˜è¦å®ç°æ™ºèƒ½ç¼“å­˜ã€åŠ¨æ€åŠ è½½ã€ç‰ˆæœ¬æ§åˆ¶ç­‰é«˜çº§ç‰¹æ€§",
                "context": {"scale": "large", "services": 50},
                "expected_complexity_range": (0.5, 1.0)
            }
        ]

        passed_cases = 0
        metrics_results = []

        for test_case in test_cases:
            metrics = executor.calculator.calculate(
                test_case["request"],
                test_case.get("context")
            )

            # æ£€æŸ¥å¤æ‚åº¦æ˜¯å¦åœ¨é¢„æœŸèŒƒå›´å†…
            min_complexity, max_complexity = test_case["expected_complexity_range"]
            if min_complexity <= metrics.complexity_score <= max_complexity:
                passed_cases += 1
                status = "PASS"
            else:
                status = "FAIL"

            metrics_results.append({
                "case": test_case["name"],
                "status": status,
                "complexity": metrics.complexity_score,
                "expected_range": test_case["expected_complexity_range"],
                "tokens": metrics.token_count,
                "recommended_level": metrics.recommended_prompt_level
            })

        if passed_cases == len(test_cases):
            result.passed = True
            result.message = f"æ‰€æœ‰{len(test_cases)}ä¸ªæµ‹è¯•ç”¨ä¾‹çš„å¤æ‚åº¦è®¡ç®—å‡†ç¡®"
        else:
            result.message = f"{passed_cases}/{len(test_cases)} æµ‹è¯•ç”¨ä¾‹é€šè¿‡"

        result.details = {
            "metrics_results": metrics_results
        }

    except Exception as e:
        result.message = f"è®¡ç®—å¤±è´¥: {str(e)}"

    return result


def test_4_failure_detection():
    """æµ‹è¯•4ï¼šå¤±æ•ˆæ¨¡å¼æ£€æµ‹"""
    result = TestResult("å¤±æ•ˆæ¨¡å¼æ£€æµ‹")

    try:
        executor = ContextFundamentalsExecutor(skills_dir)

        # æµ‹è¯•ä¸åŒå¤±æ•ˆæ¨¡å¼
        test_cases = [
            {
                "name": "ä¸Šä¸‹æ–‡æº¢å‡º",
                "request": "åˆ†æè¿™ä¸ªå¤§é¡¹ç›®",
                "context": {"content": "data" * 100000},  # å¤§é‡æ•°æ®
                "expected_failure": "overflow"
            },
            {
                "name": "ä¸Šä¸‹æ–‡æ¯’åŒ–",
                "request": "ç‰ˆæœ¬å†²çª",
                "context": {"v1": "use method A", "v2": "use method B", "old": "enabled", "new": "disabled"},
                "expected_failure": "poisoning"
            },
            {
                "name": "æ­£å¸¸ä¸Šä¸‹æ–‡",
                "request": "æ­£å¸¸è¯·æ±‚",
                "context": {"domain": "AI", "task": "learning"},
                "expected_failure": None
            }
        ]

        passed_cases = 0
        detection_results = []

        for test_case in test_cases:
            analysis = executor.analyzer.analyze(
                test_case["request"],
                test_case.get("context")
            )

            expected = test_case["expected_failure"]
            detected = any(f.mode.value == expected for f in analysis.detected_failures) if expected else False

            if expected:
                if detected:
                    passed_cases += 1
                    status = "PASS"
                else:
                    status = "FAIL"
            else:
                if not analysis.detected_failures:
                    passed_cases += 1
                    status = "PASS"
                else:
                    status = "FAIL"

            detection_results.append({
                "case": test_case["name"],
                "status": status,
                "expected": expected,
                "detected_failures": [f.mode.value for f in analysis.detected_failures]
            })

        if passed_cases == len(test_cases):
            result.passed = True
            result.message = f"æ‰€æœ‰{len(test_cases)}ä¸ªå¤±æ•ˆæ¨¡å¼æ£€æµ‹å‡†ç¡®"
        else:
            result.message = f"{passed_cases}/{len(test_cases)} æµ‹è¯•ç”¨ä¾‹é€šè¿‡"

        result.details = {
            "detection_results": detection_results
        }

    except Exception as e:
        result.message = f"æ£€æµ‹å¤±è´¥: {str(e)}"

    return result


def test_5_prompt_level_selection():
    """æµ‹è¯•5ï¼šæç¤ºè¯å±‚æ¬¡æ™ºèƒ½é€‰æ‹©"""
    result = TestResult("æç¤ºè¯å±‚æ¬¡æ™ºèƒ½é€‰æ‹©")

    try:
        executor = ContextFundamentalsExecutor(skills_dir)

        # æµ‹è¯•ä¸åŒåœºæ™¯çš„å±‚æ¬¡é€‰æ‹©
        test_cases = [
            {
                "name": "ç®€å•é—®é¢˜",
                "request": "ä»€ä¹ˆæ˜¯ä¸Šä¸‹æ–‡ï¼Ÿ",
                "expected_level": "00"
            },
            {
                "name": "ä¸­ç­‰å¤æ‚åº¦",
                "request": "å¦‚ä½•ä¼˜åŒ–AIç³»ç»Ÿçš„ä¸Šä¸‹æ–‡ç®¡ç†ï¼Ÿè¯·è¯´æ˜å¸¸è§åœºæ™¯å’Œæœ€ä½³å®è·µ",
                "expected_level": "01"
            },
            {
                "name": "å¤æ‚ä»»åŠ¡",
                "request": "åœ¨å¤§å‹é¡¹ç›®ä¸­å®æ–½ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥ï¼Œéœ€è¦è€ƒè™‘å¤šè½®å¯¹è¯ã€å¤šæ–‡ä»¶åˆ†æã€åŠ¨æ€ç»„è£…ç­‰å¤æ‚åœºæ™¯",
                "context": {"project_size": "large"},
                "expected_level": "02"
            }
        ]

        passed_cases = 0
        selection_results = []

        for test_case in test_cases:
            # æ‰§è¡Œå®Œæ•´æµç¨‹
            execution_result = executor.execute(
                test_case["request"],
                test_case.get("context")
            )

            selected_level = execution_result["prompt_level"]
            expected_level = test_case["expected_level"]

            # å…è®¸ç›¸é‚»å±‚æ¬¡ï¼ˆç®—æ³•å¯èƒ½æœ‰åˆç†åå·®ï¼‰
            level_nums = {"00": 0, "01": 1, "02": 2, "03": 3}
            if abs(level_nums[selected_level] - level_nums[expected_level]) <= 1:
                passed_cases += 1
                status = "PASS"
            else:
                status = "FAIL"

            selection_results.append({
                "case": test_case["name"],
                "status": status,
                "expected": expected_level,
                "selected": selected_level
            })

        if passed_cases == len(test_cases):
            result.passed = True
            result.message = f"æ‰€æœ‰{len(test_cases)}ä¸ªå±‚æ¬¡é€‰æ‹©åˆç†"
        else:
            result.message = f"{passed_cases}/{len(test_cases)} æµ‹è¯•ç”¨ä¾‹é€šè¿‡"

        result.details = {
            "selection_results": selection_results
        }

    except Exception as e:
        result.message = f"é€‰æ‹©å¤±è´¥: {str(e)}"

    return result


def test_6_full_execution_pipeline():
    """æµ‹è¯•6ï¼šå®Œæ•´æ‰§è¡Œæµç¨‹"""
    result = TestResult("å®Œæ•´æ‰§è¡Œæµç¨‹")

    try:
        executor = ContextFundamentalsExecutor(skills_dir)

        # æ‰§è¡Œå®Œæ•´æµç¨‹
        request = "å¦‚ä½•åœ¨åŒ…å«100ä¸ªæ–‡ä»¶çš„å¤§å‹é¡¹ç›®ä¸­ç®¡ç†ä¸Šä¸‹æ–‡ï¼Ÿéœ€è¦è€ƒè™‘å“ªäº›å¤±æ•ˆæ¨¡å¼å’Œä¼˜åŒ–ç­–ç•¥ï¼Ÿ"
        context = {
            "project": "large_scale_refactoring",
            "files": 100,
            "architecture": "microservices"
        }

        execution_result = executor.execute(request, context)

        # éªŒè¯æ‰€æœ‰å¿…éœ€å­—æ®µ
        required_fields = [
            "validation",
            "metrics",
            "analysis",
            "prompt_level",
            "prompt_content",
            "summary",
            "recommendations"
        ]

        missing_fields = [f for f in required_fields if f not in execution_result]

        if missing_fields:
            result.message = f"ç¼ºå°‘å­—æ®µ: {missing_fields}"
            return result

        # éªŒè¯æç¤ºè¯å†…å®¹
        if not execution_result["prompt_content"] or len(execution_result["prompt_content"]) == 0:
            result.message = "æç¤ºè¯å†…å®¹ä¸ºç©º"
            return result

        # éªŒè¯æ¨èä¸ä¸ºç©º
        if not execution_result["recommendations"]:
            result.message = "æ²¡æœ‰ç”Ÿæˆæ¨è"
            return result

        result.passed = True
        result.message = "å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸ"
        result.details = {
            "prompt_level": execution_result["prompt_level"],
            "prompt_length": len(execution_result["prompt_content"]),
            "num_recommendations": len(execution_result["recommendations"]),
            "complexity_score": execution_result["metrics"]["complexity_score"],
            "detected_failures": len(execution_result["analysis"]["detected_failures"]),
            "summary": execution_result["summary"]
        }

    except Exception as e:
        result.message = f"æ‰§è¡Œå¤±è´¥: {str(e)}"
        import traceback
        result.details = {
            "error": str(e),
            "traceback": traceback.format_exc()
        }

    return result


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("="*80)
    print("Context Fundamentals Skill - End-to-End Test Suite")
    print("="*80)
    print()

    tests = [
        test_1_prompt_files_loading,
        test_2_input_validation,
        test_3_metrics_calculation,
        test_4_failure_detection,
        test_5_prompt_level_selection,
        test_6_full_execution_pipeline
    ]

    results = []
    for test_func in tests:
        print(f"Running: {test_func.__name__}...")
        result = test_func()
        results.append(result)
        print(result)
        if result.details:
            print(json.dumps(result.details, indent=2, ensure_ascii=False))
        print()

    # æ±‡æ€»
    print("="*80)
    print("Test Summary")
    print("="*80)

    passed = sum(1 for r in results if r.passed)
    total = len(results)

    print(f"\nTotal: {passed}/{total} tests passed ({passed/total*100:.1f}%)")

    if passed == total:
        print("\nğŸ‰ All tests passed! Context Fundamentals skill is ready.")
    else:
        print("\nâš ï¸ Some tests failed. Please review the details above.")

    return passed == total


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
