"""
Claude Skills ä¸»å¤„ç†å™¨
æ ‡å‡†åŒ–çš„Claude Skillsæ¥å£å®ç°
"""
import json
import os
import sys
import uuid
from typing import Dict, Any, List
from datetime import datetime


def lambda_handler(event: Dict[str, Any], context: Any = None) -> Dict[str, Any]:
    """
    Claude Skillsæ ‡å‡†lambdaå¤„ç†å™¨
    """
    try:
        # Claude Skillsæ ‡å‡†ï¼ševentåŒ…å«inputsæ•°ç»„
        inputs_array = event.get('inputs', [{}])
        if not inputs_array:
            return {
                'statusCode': 400,
                'body': json.dumps({
                    'error': 'No inputs provided',
                    'timestamp': datetime.utcnow().isoformat()
                })
            }

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªè¾“å…¥
        input_data = inputs_array[0]
        
        # Claude Skillsä¸­ï¼Œå·¥å…·åç§°é€šå¸¸é€šè¿‡æŸç§æ–¹å¼æ ‡è¯†
        # åœ¨Claude Tools APIä¸­ï¼Œå·¥å…·åç§°åœ¨function_callå‚æ•°ä¸­æˆ–é€šè¿‡å…¶ä»–æ–¹å¼ä¼ é€’
        tool_name = event.get('tool_name', '').lower()
        
        if not tool_name:
            # ä»å·¥å…·è°ƒç”¨å‚æ•°ä¸­è·å–å·¥å…·åç§°
            function_call = event.get('function_call', {})
            tool_name = function_call.get('name', '').lower()
        
        # æ ¹æ®å·¥å…·åç§°æ‰§è¡Œç›¸åº”çš„æŠ€èƒ½
        if tool_name == 'architect' or 'architect' in tool_name:
            result = _execute_architect_skill(input_data)
        elif tool_name == 'context-analyzer' or 'analyzer' in tool_name:
            result = _execute_context_analysis_skill(input_data)
        elif tool_name == 'context-optimizer' or 'optimizer' in tool_name:
            result = _execute_context_optimization_skill(input_data)
        elif tool_name == 'cognitive-templater' or 'templater' in tool_name:
            result = _execute_cognitive_template_skill(input_data)
        elif tool_name == 'agent-creator' or 'agent' in tool_name:
            result = _execute_agent_creator_skill(input_data)
        elif tool_name == 'task-decomposer' or 'decomposer' in tool_name:
            result = _execute_task_decomposer_skill(input_data)
        elif tool_name == 'constraint-generator' or 'constraint' in tool_name:
            result = _execute_constraint_generator_skill(input_data)
        elif tool_name == 'dnaspec-init' or 'dnaspec' in tool_name:
            result = _execute_dnaspec_init_skill(input_data)
        else:
            return {
                'statusCode': 400,
                'body': json.dumps({
                    'error': f'Unknown tool: {tool_name}',
                    'available_tools': [
                        'architect', 'context-analyzer', 'context-optimizer', 
                        'cognitive-templater', 'agent-creator', 'task-decomposer', 'constraint-generator', 'dnaspec-init'
                    ],
                    'timestamp': datetime.utcnow().isoformat()
                })
            }

        return {
            'statusCode': 200,
            'body': json.dumps(result, ensure_ascii=False)
        }

    except Exception as e:
        error_body = {
            'error': str(e),
            'input_event': event,
            'timestamp': datetime.utcnow().isoformat()
        }
        return {
            'statusCode': 500,
            'body': json.dumps(error_body, ensure_ascii=False)
        }


def _execute_architect_skill(input_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡Œæ¶æ„è®¾è®¡æŠ€èƒ½ - éµå¾ªClaude Skillsè§„èŒƒ
    """
    input_text = input_data.get('input', input_data.get('requirements', ''))
    
    if not input_text.strip():
        return {
            'success': False,
            'error': 'No input provided for architect skill',
            'input_preview': str(input_data)[:100]
        }
    
    # ç®€åŒ–çš„æ¶æ„è®¾è®¡ç®—æ³•
    input_lower = input_text.lower()
    
    # 5ç»´è´¨é‡åˆ†æ
    clarity = min(1.0, max(0.0, 0.5 + len(input_text) * 0.00001))
    relevance = min(1.0, max(0.0, 0.7 + (0.1 if any(kw in input_lower for kw in ['system', 'function', 'task', 'requirement', 'éœ€æ±‚', 'system', 'function', 'task']) else 0)))
    completeness = min(1.0, max(0.0, 0.3 + (0.3 if any(kw in input_lower for kw in ['constraint', 'requirement', 'goal', 'requirement', 'constraint', 'requirement', 'goal']) else 0)))
    consistency = min(1.0, max(0.0, 0.8 - (0.2 if any(kw in input_lower for kw in ['but', 'however', 'ä½†æ˜¯', 'ç„¶è€Œ']) else 0)))
    efficiency = min(1.0, max(0.0, 1.0 - len(input_text) * 0.00005))

    # ç³»ç»Ÿæ¶æ„è®¾è®¡
    architecture_map = {
        "ç”µå•†": "[WebApp] -> [API Server] -> [Database]",
        "åšå®¢": "[WebApp] -> [Database]",
        "ç”¨æˆ·ç®¡ç†": "[Frontend] -> [API Gateway] -> [Auth Service] -> [User DB]",
        "è®¤è¯": "[Auth Service] -> [User DB] -> [Session Store]",
        "api": "[API Gateway] -> [Microservices] -> [Data Layer]"
    }

    # æŸ¥æ‰¾åŒ¹é…çš„æ¶æ„
    architecture_type = "custom"
    architecture_design = f"åŸºäºéœ€æ±‚è®¾è®¡: {input_text[:50]}..." if len(input_text) > 50 else input_text
    for keyword, arch in architecture_map.items():
        if keyword in input_lower:
            architecture_design = arch
            architecture_type = keyword
            break

    result = {
        'success': True,
        'result': {
            'architecture_type': architecture_type,
            'design': architecture_design,
            'context_quality': {
                'clarity': round(clarity, 2),
                'relevance': round(relevance, 2),
                'completeness': round(completeness, 2),
                'consistency': round(consistency, 2),
                'efficiency': round(efficiency, 2)
            },
            'suggestions': [
                'Add more specific goal descriptions',
                'Supplement constraint conditions and specific requirements',
                'Improve expression clarity'
            ],
            'issues': [i for i in [
                'Lack of explicit constraint conditions' if completeness < 0.6 else '',
                'Some expressions can be more precise' if clarity < 0.7 else ''
            ] if i],  # Filter out empty issues
            'confidence': 0.85
        },
        'input_preview': input_text[:100] + '...' if len(input_text) > 100 else input_text,
        'timestamp': datetime.utcnow().isoformat()
    }
    
    return result


def _execute_context_analysis_skill(input_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡Œä¸Šä¸‹æ–‡åˆ†ææŠ€èƒ½ - éµå¾ªClaude Skillsè§„èŒƒ
    """
    input_text = input_data.get('input', input_data.get('context', ''))
    
    if not input_text.strip():
        return {
            'success': False,
            'error': 'No context provided for analysis',
            'input_preview': str(input_data)[:100]
        }
    
    # 5ç»´ä¸Šä¸‹æ–‡è´¨é‡åˆ†æ
    context_length = len(input_text)
    token_count_estimate = max(1, len(input_text) // 4)
    
    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    clarity = min(1.0, max(0.0, 0.5 + len(input_text) * 0.00001))
    relevance = min(1.0, max(0.0, 0.7 + (0.1 if any(kw in input_text.lower() for kw in ['system', 'function', 'task', 'requirement', 'éœ€æ±‚', 'system', 'function', 'task']) else 0)))
    completeness = min(1.0, max(0.0, 0.3 + (0.3 if any(kw in input_text.lower() for kw in ['constraint', 'requirement', 'goal', 'requirement', 'constraint', 'requirement', 'goal']) else 0)))
    consistency = min(1.0, max(0.0, 0.8 - (0.2 if any(kw in input_text.lower() for kw in ['but', 'however', 'ä½†æ˜¯', 'ç„¶è€Œ']) else 0)))
    efficiency = min(1.0, max(0.0, 1.0 - len(input_text) * 0.00005))

    result = {
        'success': True,
        'result': {
            'context_length': context_length,
            'token_count_estimate': token_count_estimate,
            'quality_metrics': {
                'clarity': round(clarity, 2),
                'relevance': round(relevance, 2),
                'completeness': round(completeness, 2),
                'consistency': round(consistency, 2),
                'efficiency': round(efficiency, 2)
            },
            'suggestions': [
                'Add more specific goal descriptions',
                'Supplement constraint conditions and specific requirements',
                'Improve expression clarity'
            ],
            'issues': [i for i in [
                'Lack of explicit constraint conditions' if completeness < 0.6 else '',
                'Some expressions can be more precise' if clarity < 0.7 else ''
            ] if i],  # Filter out empty issues
            'confidence': 0.85
        },
        'input_preview': input_text[:100] + '...' if len(input_text) > 100 else input_text,
        'timestamp': datetime.utcnow().isoformat()
    }
    
    return result


def _execute_context_optimization_skill(input_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡Œä¸Šä¸‹æ–‡ä¼˜åŒ–æŠ€èƒ½ - éµå¾ªClaude Skillsè§„èŒƒ
    """
    input_text = input_data.get('input', input_data.get('context', ''))
    goals = input_data.get('optimization_goals', [])
    
    if not input_text.strip():
        return {
            'success': False,
            'error': 'No context provided for optimization',
            'input_preview': str(input_data)[:100]
        }

    # ç®€åŒ–çš„ä¼˜åŒ–ç®—æ³•
    original_length = len(input_text)
    
    optimized_text = input_text
    applied_improvements = []
    
    # æ ¹æ®ä¼˜åŒ–ç›®æ ‡è¿›è¡Œä¼˜åŒ–
    if any(goal.lower() in ['clarity', 'clear', 'clarity', 'æ¸…æ™°åº¦'] for goal in goals):
        optimized_text += "\n\nä¸ºäº†æé«˜æ¸…æ™°åº¦ï¼Œæ˜ç¡®æŒ‡å‡ºç›®æ ‡å’Œçº¦æŸæ¡ä»¶: [åœ¨æ­¤å¤„è¡¥å……å…·ä½“ç›®æ ‡å’Œçº¦æŸ]"
        applied_improvements.append('Added clarity directives')
    
    if any(goal.lower() in ['completeness', 'complete', 'completeness', 'å®Œæ•´æ€§'] for goal in goals):
        optimized_text += "\n\nçº¦æŸ: éœ€è¦åœ¨è§„å®šæ—¶é—´å†…å®Œæˆ\næ˜ç¡®ç›®æ ‡: å®ç°é¢„æœŸåŠŸèƒ½\nå‰ææ¡ä»¶: å…·å¤‡å¿…è¦çš„èµ„æºæ”¯æŒ"
        applied_improvements.append('Added completeness elements')
    
    # è®¡ç®—æ”¹è¿›æŒ‡æ ‡
    clarity_impact = 0.2 if any(g.lower().find('clarity') != -1 or g.lower().find('clear') != -1 for g in goals) else 0.0
    completeness_impact = 0.3 if any('completeness' in g.lower() or 'complete' in g.lower() for g in goals) else 0.0
    relevance_impact = 0.15 if any('relevance' in g.lower() or 'relevant' in g.lower() for g in goals) else 0.0
    efficiency_impact = -0.1 if any('concise' in g.lower() or 'efficiency' in g.lower() for g in goals) else 0.0

    result = {
        'success': True,
        'result': {
            'original_length': original_length,
            'optimized_length': len(optimized_text),
            'applied_optimizations': applied_improvements,
            'optimization_goals': goals,
            'improvement_metrics': {
                'clarity_change': round(clarity_impact, 2),
                'completeness_change': round(completeness_impact, 2),
                'relevance_change': round(relevance_impact, 2),
                'efficiency_change': round(efficiency_impact, 2)
            },
            'optimized_context': optimized_text[:500] + '...' if len(optimized_text) > 500 else optimized_text,
            'optimization_summary': f"Optimized for goals: {', '.join(goals) if goals else 'general improvement'}"
        },
        'input_preview': input_text[:100] + '...' if len(input_text) > 100 else input_text,
        'timestamp': datetime.utcnow().isoformat()
    }
    
    return result


def _execute_cognitive_template_skill(input_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡Œè®¤çŸ¥æ¨¡æ¿æŠ€èƒ½ - éµå¾ªClaude Skillsè§„èŒƒ
    """
    input_text = input_data.get('input', input_data.get('context', ''))
    template_type = input_data.get('template_type', 'chain_of_thought')
    
    if not input_text.strip():
        return {
            'success': False,
            'error': 'No input provided for cognitive template application',
            'input_preview': str(input_data)[:100]
        }

    # æ ¹æ®æ¨¡æ¿ç±»å‹ç”Ÿæˆç›¸åº”è¾“å‡º
    if template_type == 'chain_of_thought':
        enhanced_output = f"é“¾å¼æ€ç»´åˆ†æ: {input_text}\n\n"
        enhanced_output += "1. é—®é¢˜ç†è§£\n"
        enhanced_output += "2. æ­¥éª¤åˆ†è§£\n"
        enhanced_output += "3. ä¸­é—´æ¨ç†\n"
        enhanced_output += "4. éªŒè¯æ£€æŸ¥\n"
        enhanced_output += "5. æœ€ç»ˆç­”æ¡ˆ\n\n"
        template_desc = "Chain-of-Thought reasoning pattern for systematic problem solving"
        
    elif template_type == 'verification':
        enhanced_output = f"éªŒè¯æ£€æŸ¥: {input_text}\n\n"
        enhanced_output += "1. åˆæ­¥ç­”æ¡ˆ\n"
        enhanced_output += "2. é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥\n"
        enhanced_output += "3. äº‹å®å‡†ç¡®æ€§æ£€éªŒ\n"
        enhanced_output += "4. å®Œæ•´æ€§æ£€éªŒ\n"
        enhanced_output += "5. æœ€ç»ˆç¡®è®¤\n\n"
        template_desc = "Verification pattern for validating reasoning and outputs"
        
    elif template_type == 'few_shot':
        enhanced_output = f"å°‘æ ·æœ¬å­¦ä¹ : {input_text}\n\n"
        enhanced_output += "ç¤ºä¾‹1: [è¦æ¨¡ä»¿çš„æ¨¡å¼]\n"
        enhanced_output += "ç¤ºä¾‹2: [è¦æ¨¡ä»¿çš„æ¨¡å¼]\n"
        enhanced_output += "å½“å‰ä»»åŠ¡: [åº”ç”¨ä¹ å¾—æ¨¡å¼]\n\n"
        template_desc = "Few-shot learning pattern for learning from examples"
        
    elif template_type == 'role_playing':
        enhanced_output = f"è§’è‰²æ‰®æ¼”åˆ†æ: {input_text}\n\n"
        enhanced_output += "è®¾å®šé€‚å½“è§’è‰²...\n"
        enhanced_output += "[åº”ç”¨è§’è‰²ç‰¹å®šæ¨ç†]\n\n"
        template_desc = "Role-playing pattern for perspective-based analysis"
        
    else:  # Default
        enhanced_output = f"è®¤çŸ¥æ¨¡æ¿åº”ç”¨: {input_text}\n\n"
        enhanced_output += "1. ç†è§£ä»»åŠ¡\n"
        enhanced_output += "2. åˆ†è§£ä¸ºå­ç»„ä»¶\n"
        enhanced_output += "3. åº”ç”¨ç›¸å…³æ¡†æ¶\n"
        enhanced_output += "4. ç»¼åˆè§£å†³æ–¹æ¡ˆ\n"
        enhanced_output += "5. æä¾›æœ€ç»ˆè¾“å‡º\n\n"
        template_desc = "Generic cognitive template pattern"

    result = {
        'success': True,
        'result': {
            'template_type': template_type,
            'template_description': template_desc,
            'enhanced_output': enhanced_output[:500] + '...' if len(enhanced_output) > 500 else enhanced_output,
            'cognitive_framework_applied': True,
            'input_processed': input_text[:100] + '...' if len(input_text) > 100 else input_text
        },
        'input_preview': input_text[:100] + '...' if len(input_text) > 100 else input_text,
        'timestamp': datetime.utcnow().isoformat()
    }
    
    return result


def _execute_agent_creator_skill(input_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡Œæ™ºèƒ½ä½“åˆ›å»ºæŠ€èƒ½ - éµå¾ªClaude Skillsè§„èŒƒ
    """
    agent_description = input_data.get('agent_description', input_data.get('input', ''))
    capabilities = input_data.get('capabilities', [])
    
    if not agent_description.strip():
        return {
            'success': False,
            'error': 'No agent description provided',
            'input_preview': str(input_data)[:100]
        }

    # ç”Ÿæˆæ™ºèƒ½ä½“é…ç½®
    agent_config = {
        'id': f"agent_{uuid.uuid4().hex[:8]}",
        'role': agent_description,
        'domain': 'general',
        'capabilities': capabilities or [
            'Task execution',
            'Information retrieval', 
            'Decision making',
            'Context awareness'
        ],
        'instructions': f"You are acting as a {agent_description} in the appropriate domain.",
        'personality': 'Professional, helpful, focused on assigned tasks',
        'created_at': datetime.utcnow().isoformat()
    }

    result = {
        'success': True,
        'result': {
            'agent_config': agent_config,
            'agent_created': True,
            'capabilities_assigned': len(agent_config['capabilities']),
            'domain': agent_config['domain']
        },
        'input_preview': agent_description[:100] + '...' if len(agent_description) > 100 else agent_description,
        'timestamp': datetime.utcnow().isoformat()
    }
    
    return result


def _execute_task_decomposer_skill(input_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡Œä»»åŠ¡åˆ†è§£æŠ€èƒ½ - éµå¾ªClaude Skillsè§„èŒƒ
    """
    input_text = input_data.get('input', input_data.get('description', ''))
    max_depth = input_data.get('max_depth', 3)
    
    if not input_text.strip():
        return {
            'success': False,
            'error': 'No task provided for decomposition',
            'input_preview': str(input_data)[:100]
        }

    # ç®€å•çš„ä»»åŠ¡åˆ†è§£é€»è¾‘
    sentences = input_text.split('.')
    subtasks = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) > 5:  # å¿½ç•¥å¤ªçŸ­çš„å¥å­
            # è¯†åˆ«ä»»åŠ¡æ€§å…³é”®è¯
            task_indicators = ['éœ€è¦', 'å®ç°', 'åˆ›å»º', 'å¼€å‘', 'è®¾è®¡', 'æ„å»º', 'æ·»åŠ ', 'ä¿®æ”¹', 'ä¼˜åŒ–', 'åˆ†æ', 'build', 'develop', 'implement', 'create']
            if any(indicator in sentence.lower() for indicator in task_indicators):
                subtasks.append(sentence)
    
    # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ä»»åŠ¡æ€§æè¿°ï¼ŒæŒ‰åŠŸèƒ½é¢†åŸŸåˆ†è§£
    if not subtasks:
        functional_areas = [
            'è®¤è¯', 'æˆæƒ', 'æ•°æ®ç®¡ç†', 'ç”¨æˆ·ç•Œé¢', 'APIæ¥å£', 'æ•°æ®åº“',
            'å®‰å…¨æ€§', 'æ€§èƒ½', 'æµ‹è¯•', 'éƒ¨ç½²', 'ç›‘æ§', 'æ—¥å¿—', 
            'authentication', 'authorization', 'data management', 'UI', 'API', 'database'
        ]
        
        for area in functional_areas:
            if area in input_text.lower():
                subtasks.append(f"å®ç°{area}åŠŸèƒ½")
    
    # é™åˆ¶å­ä»»åŠ¡æ•°é‡ä»¥é˜²æ­¢çˆ†ç‚¸
    subtasks = subtasks[:10]

    result = {
        'success': True,
        'result': {
            'task_structure': {
                'id': f"TASK-{uuid.uuid4().hex[:8]}",
                'description': input_text,
                'is_atomic': len(subtasks) == 0,
                'depth': 1,
                'subtasks': [{'id': f"SUB-{uuid.uuid4().hex[:8]}", 'description': task, 'completed': False} for task in subtasks],
                'created_at': datetime.utcnow().isoformat()
            },
            'validation': {
                'is_valid': True,
                'issues': [],
                'metrics': {
                    'total_tasks': len(subtasks) + 1,
                    'max_depth': 1,
                    'average_branching_factor': len(subtasks)
                }
            },
            'execution_info': {
                'skill': 'task-decomposer',
                'timestamp': datetime.utcnow().isoformat(),
                'principles_applied': ['KISS', 'YAGNI', 'SOLID']
            }
        },
        'input_preview': input_text[:100] + '...' if len(input_text) > 100 else input_text,
        'timestamp': datetime.utcnow().isoformat()
    }
    
    return result


def _execute_constraint_generator_skill(input_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡Œçº¦æŸç”ŸæˆæŠ€èƒ½ - éµå¾ªClaude Skillsè§„èŒƒ
    """
    requirements = input_data.get('requirements', input_data.get('input', ''))
    change_request = input_data.get('change_request', '')
    
    if not requirements.strip():
        return {
            'success': False,
            'error': 'No requirements provided for constraint generation',
            'input_preview': str(input_data)[:100]
        }

    # ç”Ÿæˆçº¦æŸ
    req_lower = requirements.lower()
    constraints = []
    
    # å®‰å…¨çº¦æŸ
    if any(term in req_lower for term in ['security', 'secure', 'auth', 'encrypt', 'privacy', 'protect', 'password', 'login']):
        constraints.append({
            'id': f"constraint_{uuid.uuid4().hex[:8]}",
            'type': 'security',
            'description': 'ç³»ç»Ÿå¿…é¡»å®ç°æ ‡å‡†å®‰å…¨æªæ–½',
            'severity': 'high',
            'created_at': datetime.utcnow().isoformat()
        })

    # æ€§èƒ½çº¦æŸ
    if any(term in req_lower for term in ['performance', 'fast', 'response', 'throughput', 'latency', 'speed']):
        constraints.append({
            'id': f"constraint_{uuid.uuid4().hex[:8]}",
            'type': 'performance', 
            'description': 'ç³»ç»Ÿå¿…é¡»æ»¡è¶³å®šä¹‰çš„æ€§èƒ½è¦æ±‚',
            'severity': 'medium',
            'created_at': datetime.utcnow().isoformat()
        })

    # æ•°æ®çº¦æŸ
    if any(term in req_lower for term in ['data', 'database', 'storage', 'retrieve', 'persist', 'record']):
        constraints.append({
            'id': f"constraint_{uuid.uuid4().hex[:8]}",
            'type': 'data_integrity',
            'description': 'ç³»ç»Ÿå¿…é¡»ä¿æŒæ•°æ®å®Œæ•´æ€§å’Œå¤‡ä»½èƒ½åŠ›',
            'severity': 'high',
            'created_at': datetime.utcnow().isoformat()
        })

    # å¯¹é½æ£€æŸ¥
    alignment_check = {
        'is_aligned': not (change_request and 
                         any(contradiction in change_request.lower() 
                             for contradiction in ['no security', 'negligible performance', 'unreliable'])),
        'conflicts': [],
        'suggestions': ['No change request provided, requirements are baseline'] if not change_request else ['Change request appears aligned with base requirements']
    }
    
    result = {
        'success': True,
        'result': {
            'constraints': constraints,
            'alignment_check': alignment_check,
            'version_info': {
                'current_version': f"version_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}",
                'tracked': True
            },
            'timestamp': datetime.utcnow().timestamp()
        },
        'input': {
            'requirements': requirements[:100] + '...' if len(requirements) > 100 else requirements,
            'change_request': change_request[:50] + '...' if len(change_request) > 50 else change_request
        },
        'timestamp': datetime.utcnow().isoformat()
    }
    
    return result


# Test function for local development
def test_locally():
    """
    æœ¬åœ°æµ‹è¯•å‡½æ•°
    """
    import uuid
    import json
    from datetime import datetime
    
    print("Testing Claude Skills implementation...")
    
    # Test architect skill
    test_event = {
        'inputs': [{'input': 'Design an e-commerce system'}],
        'tool_name': 'architect'
    }
    
    result = lambda_handler(test_event, None)
    print("Architect skill result:", json.dumps(result, indent=2, ensure_ascii=False)[:300] + "...")
    
    # Test context analyzer skill
    test_event = {
        'inputs': [{'input': 'User needs to login to the system with security requirements'}],
        'tool_name': 'context-analyzer'
    }
    
    result = lambda_handler(test_event, None)
    print("Context analyzer skill result keys:", result.get('body', {}).keys() if isinstance(result.get('body'), dict) else 'Check body type')



def _execute_dnaspec_init_skill(input_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡ŒDNASPECåˆå§‹åŒ–æŠ€èƒ½ - éµå¾ªagentskills.ioè§„èŒƒ
    """
    import os
    import shutil
    from datetime import datetime
    
    try:
        # è§£ææ“ä½œç±»å‹
        operation = input_data.get('operation', 'detect').lower()
        
        # é¡¹ç›®æ ¹ç›®å½•
        project_root = os.getcwd()
        dnaspec_dir = os.path.join(project_root, '.dnaspec')
        constitution_file = os.path.join(project_root, 'PROJECT_CONSTITUTION.md')
        config_file = os.path.join(dnaspec_dir, 'config.json')
        
        # æ£€æµ‹é¡¹ç›®çŠ¶æ€
        def detect_project_status():
            existing_files = []
            missing_files = []
            
            # æ£€æŸ¥æ ¸å¿ƒæ–‡ä»¶
            core_files = [
                constitution_file,
                config_file,
                os.path.join(dnaspec_dir, 'cache'),
                os.path.join(dnaspec_dir, 'meta'),
            ]
            
            for file_path in core_files:
                if os.path.exists(file_path):
                    existing_files.append(file_path)
                else:
                    missing_files.append(file_path)
            
            # æ£€æµ‹é¡¹ç›®ç±»å‹
            detected_types = _detect_project_types()
            detected_tools = _detect_development_tools()
            
            # ç¡®å®šçŠ¶æ€
            if len(existing_files) == len(core_files):
                status = "complete"
            elif len(existing_files) > 0:
                status = "partial"
            else:
                status = "not_initialized"
            
            return {
                "status": status,
                "existing_files": existing_files,
                "missing_files": missing_files,
                "detected_types": detected_types,
                "detected_tools": detected_tools,
                "project_root": project_root,
                "dnaspec_dir": dnaspec_dir
            }
        
        # åˆå§‹åŒ–é¡¹ç›®
        def initialize_project(init_type="auto", project_type="generic", features=None, force=False):
            features = features or []
            
            # æ£€æµ‹å½“å‰çŠ¶æ€
            current_status = detect_project_status()
            
            if current_status["status"] == "complete" and not force:
                return {
                    "message": "é¡¹ç›®å·²ç»åˆå§‹åŒ–",
                    "status": current_status["status"],
                    "existing_files": current_status["existing_files"]
                }
            
            # æ‰§è¡Œåˆå§‹åŒ–
            if init_type == "auto":
                init_type = _detect_init_type()
            
            result = _perform_initialization(init_type, project_type, features, project_root, dnaspec_dir, constitution_file, config_file)
            
            return {
                "success": True,
                "message": f"{init_type} åˆå§‹åŒ–å®Œæˆ",
                "init_type": init_type,
                "project_type": project_type,
                "features_enabled": features,
                "created_files": result.get("created_files", []),
                "configuration": result.get("configuration", {}),
                "next_steps": _generate_next_steps(features)
            }
        
        # é‡ç½®åè°ƒæœºåˆ¶
        def reset_coordination(confirm=False):
            if not confirm:
                return {
                    "success": False,
                    "message": "éœ€è¦ç¡®è®¤é‡ç½®æ“ä½œ",
                    "suggestion": "è®¾ç½® confirm=true æ¥ç¡®è®¤é‡ç½®"
                }
            
            # å¤‡ä»½ç°æœ‰é…ç½®
            backup_dir = f"{dnaspec_dir}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            if os.path.exists(dnaspec_dir):
                shutil.move(dnaspec_dir, backup_dir)
            
            # åˆ é™¤å®ªæ³•æ–‡ä»¶
            if os.path.exists(constitution_file):
                os.remove(constitution_file)
            
            return {
                "success": True,
                "message": "åè°ƒæœºåˆ¶å·²é‡ç½®",
                "backup_location": backup_dir,
                "next_steps": [
                    "è¿è¡Œåˆå§‹åŒ–å‘½ä»¤é‡æ–°é…ç½®",
                    "æ£€æŸ¥å¤‡ä»½æ–‡ä»¶æ¢å¤ç‰¹å®šé…ç½®"
                ]
            }
        
        # è·å–é…ç½®ä¿¡æ¯
        def get_configuration_info():
            try:
                if os.path.exists(config_file):
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                else:
                    config = {}
                
                return {
                    "success": True,
                    "configuration": config,
                    "config_file": config_file,
                    "last_updated": _get_file_modification_time(config_file)
                }
            except Exception as e:
                return {
                    "success": False,
                    "error": str(e),
                    "message": "æ— æ³•è¯»å–é…ç½®ä¿¡æ¯"
                }
        
        # æ‰§è¡Œç›¸åº”æ“ä½œ
        if operation == "detect":
            result = detect_project_status()
            message = "é¡¹ç›®çŠ¶æ€æ£€æµ‹å®Œæˆ"
        elif operation == "initialize":
            init_type = input_data.get('init_type', 'auto')
            project_type = input_data.get('project_type', 'generic')
            features = input_data.get('features', [])
            force = input_data.get('force', False)
            result = initialize_project(init_type=init_type, project_type=project_type, features=features, force=force)
            message = "é¡¹ç›®åˆå§‹åŒ–å®Œæˆ"
        elif operation == "reset":
            confirm = input_data.get('confirm', False)
            result = reset_coordination(confirm=confirm)
            message = "åè°ƒæœºåˆ¶é‡ç½®å®Œæˆ" if result.get('success') else "é‡ç½®æ“ä½œå¤±è´¥"
        elif operation == "get-config":
            result = get_configuration_info()
            message = "é…ç½®ä¿¡æ¯è·å–å®Œæˆ"
        elif operation == "status":
            result = detect_project_status()
            message = "é¡¹ç›®çŠ¶æ€æŸ¥è¯¢å®Œæˆ"
        else:
            return {
                'statusCode': 400,
                'body': json.dumps({
                    "success": False,
                    "error": f"Unknown operation: {operation}",
                    "available_operations": ["detect", "initialize", "reset", "get-config", "status"]
                }, ensure_ascii=False)
            }
        
        return {
            'success': True,
            'operation': operation,
            'message': message,
            'result': result,
            'timestamp': datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'operation': input_data.get('operation', 'unknown'),
            'timestamp': datetime.utcnow().isoformat()
        }


def _detect_project_types():
    """æ£€æµ‹é¡¹ç›®ç±»å‹"""
    types = []
    
    # æ£€æŸ¥å¸¸è§é¡¹ç›®æ–‡ä»¶
    project_indicators = {
        "web_application": ["package.json", "index.html", "vite.config.js", "webpack.config.js"],
        "mobile_app": ["App.js", "app.json", "pubspec.yaml", "build.gradle"],
        "api_service": ["main.py", "app.py", "requirements.txt", "Dockerfile"],
        "ml_project": ["requirements.txt", "jupyter", "notebook.ipynb", "model.pkl"],
        "data_science": ["requirements.txt", "notebook.ipynb", "data/", "pandas"],
        "microservice": ["Dockerfile", "docker-compose.yml", "main.py", "app.py"]
    }
    
    for project_type, indicators in project_indicators.items():
        if any(os.path.exists(indicator) for indicator in indicators):
            types.append(project_type)
    
    return types if types else ["generic"]


def _detect_development_tools():
    """æ£€æµ‹å¼€å‘å·¥å…·"""
    tools = {
        "version_control": [],
        "build_tools": [],
        "team_tools": [],
        "enterprise_tools": [],
        "cicd_tools": []
    }
    
    # ç‰ˆæœ¬æ§åˆ¶
    if os.path.exists('.git'):
        tools["version_control"].append("git")
    
    # æ„å»ºå·¥å…·
    if os.path.exists('package.json'):
        tools["build_tools"].append("npm")
    if os.path.exists('requirements.txt'):
        tools["build_tools"].append("pip")
    if os.path.exists('Dockerfile'):
        tools["build_tools"].append("docker")
    
    # å›¢é˜Ÿå·¥å…·
    if os.path.exists('.github') or os.path.exists('workflows'):
        tools["team_tools"].append("github_actions")
    if os.path.exists('.gitlab-ci.yml') or os.path.exists('.gitlab'):
        tools["team_tools"].append("gitlab_ci")
    
    # ä¼ä¸šå·¥å…·
    if os.path.exists('k8s') or os.path.exists('kubernetes'):
        tools["enterprise_tools"].append("kubernetes")
    if os.path.exists('terraform'):
        tools["enterprise_tools"].append("terraform")
    
    # CI/CDå·¥å…·
    if os.path.exists('.github/workflows'):
        tools["cicd_tools"].append("github_actions")
    if os.path.exists('.circleci'):
        tools["cicd_tools"].append("circleci")
    
    return tools


def _detect_init_type():
    """è‡ªåŠ¨æ£€æµ‹åˆå§‹åŒ–ç±»å‹"""
    detected_tools = _detect_development_tools()
    
    # åŸºäºæ£€æµ‹ç»“æœç¡®å®šåˆå§‹åŒ–ç±»å‹
    if len(detected_tools.get("team_tools", [])) >= 3:
        return "team"
    elif len(detected_tools.get("enterprise_tools", [])) >= 2:
        return "enterprise"
    else:
        return "project"


def _perform_initialization(init_type, project_type, features, project_root, dnaspec_dir, constitution_file, config_file):
    """æ‰§è¡Œå…·ä½“åˆå§‹åŒ–æ“ä½œ"""
    created_files = []
    
    # åˆ›å»ºDNASPECç›®å½•ç»“æ„
    _create_dnaspec_structure(dnaspec_dir)
    created_files.append(dnaspec_dir)
    
    # ç”Ÿæˆé¡¹ç›®å®ªæ³•
    constitution_content = _generate_constitution(init_type, project_type, features)
    with open(constitution_file, 'w', encoding='utf-8') as f:
        f.write(constitution_content)
    created_files.append(constitution_file)
    
    # ç”Ÿæˆé…ç½®æ–‡ä»¶
    config = _generate_configuration(init_type, project_type, features)
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    created_files.append(config_file)
    
    # å¯ç”¨æŒ‡å®šåŠŸèƒ½
    if "caching" in features:
        _setup_caching_system(dnaspec_dir)
        created_files.append(os.path.join(dnaspec_dir, 'cache'))
    
    if "git_hooks" in features:
        _setup_git_hooks(project_root, dnaspec_dir)
        created_files.append(os.path.join(dnaspec_dir, 'hooks'))
    
    if "ci_cd" in features:
        _setup_ci_cd_templates(dnaspec_dir)
        created_files.append(os.path.join(dnaspec_dir, 'cicd'))
    
    return {
        "created_files": created_files,
        "configuration": config
    }


def _create_dnaspec_structure(dnaspec_dir):
    """åˆ›å»ºDNASPECç›®å½•ç»“æ„"""
    directories = [
        dnaspec_dir,
        os.path.join(dnaspec_dir, 'cache'),
        os.path.join(dnaspec_dir, 'cache', 'temp'),
        os.path.join(dnaspec_dir, 'cache', 'staging'),
        os.path.join(dnaspec_dir, 'cache', 'meta'),
        os.path.join(dnaspec_dir, 'meta'),
        os.path.join(dnaspec_dir, 'hooks'),
        os.path.join(dnaspec_dir, 'logs')
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)


def _generate_constitution(init_type, project_type, features):
    """ç”Ÿæˆé¡¹ç›®å®ªæ³•"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    return f"""# DNASPEC é¡¹ç›®åè°ƒå®ªæ³•

## é¡¹ç›®ä¿¡æ¯
- **é¡¹ç›®ç±»å‹**: {project_type}
- **åˆå§‹åŒ–ç±»å‹**: {init_type}
- **åˆå§‹åŒ–æ—¶é—´**: {timestamp}
- **DNASPECç‰ˆæœ¬**: 1.0.0

## åè°ƒæœºåˆ¶
æœ¬é¡¹ç›®å·²å¯ç”¨DNASPECåè°ƒæœºåˆ¶ï¼Œæ”¯æŒæŠ€èƒ½é—´çš„æ™ºèƒ½åä½œã€‚

### å·²å¯ç”¨çš„åŠŸèƒ½
{chr(10).join(f"- {feature}" for feature in features)}

## æŠ€èƒ½åè°ƒè§„åˆ™

### æ ¸å¿ƒæŠ€èƒ½ç»„åˆ
1. **æ¶æ„è®¾è®¡**: `/architect` - ç³»ç»Ÿæ¶æ„è®¾è®¡
2. **ä»»åŠ¡åˆ†è§£**: `/task-decomposer` - ä»»åŠ¡åˆ†è§£å’Œè§„åˆ’
3. **çº¦æŸç”Ÿæˆ**: `/constraint-generator` - çº¦æŸæ¡ä»¶ç”Ÿæˆ
4. **ä¸Šä¸‹æ–‡åˆ†æ**: `/context-analyzer` - ä¸Šä¸‹æ–‡åˆ†æ
5. **ä¸Šä¸‹æ–‡ä¼˜åŒ–**: `/context-optimizer` - ä¸Šä¸‹æ–‡ä¼˜åŒ–
6. **è®¤çŸ¥æ¨¡æ¿**: `/cognitive-templater` - è®¤çŸ¥æ¨¡æ¿åº”ç”¨
7. **æŠ€èƒ½åˆ›å»º**: `/agent-creator` - æ™ºèƒ½ä½“åˆ›å»º
8. **DNASPECåˆå§‹åŒ–**: `/dnaspec-init` - åè°ƒæœºåˆ¶ç®¡ç†

### åè°ƒæ‰§è¡Œæ¨¡å¼
- **è‡ªåŠ¨æ£€æµ‹**: ç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹é¡¹ç›®å®ªæ³•çŠ¶æ€
- **æ™ºèƒ½åè°ƒ**: å½“æ£€æµ‹åˆ°åè°ƒæœºåˆ¶æ—¶å¯ç”¨å¤šæŠ€èƒ½åä½œ
- **ä¼˜é›…é™çº§**: å½“åè°ƒä¸å¯ç”¨æ—¶è‡ªåŠ¨é™çº§åˆ°ç‹¬ç«‹æ¨¡å¼
- **æ€§èƒ½ä¼˜åŒ–**: åŸºäºç½®ä¿¡åº¦åŠ¨æ€é€‰æ‹©æœ€ä¼˜æ‰§è¡Œç­–ç•¥

## ä½¿ç”¨æŒ‡å—

### çŠ¶æ€æ£€æŸ¥
```bash
# æ£€æŸ¥é¡¹ç›®çŠ¶æ€
/dnaspec-init "operation=detect"

# æŸ¥çœ‹é…ç½®ä¿¡æ¯
/dnaspec-init "operation=get-config"

# é‡ç½®åè°ƒæœºåˆ¶ï¼ˆå¦‚éœ€è¦ï¼‰
/dnaspec-init "operation=reset confirm=true"
```

---

**æœ€åæ›´æ–°**: {timestamp}
**ç»´æŠ¤è€…**: DNASPECè‡ªåŠ¨ç”Ÿæˆ
"""


def _generate_configuration(init_type, project_type, features):
    """ç”Ÿæˆé…ç½®æ–‡ä»¶"""
    return {
        "dnaspec": {
            "version": "1.0.0",
            "init_type": init_type,
            "project_type": project_type,
            "created_at": datetime.now().isoformat(),
            "features": {
                "caching": "caching" in features,
                "git_hooks": "git_hooks" in features,
                "ci_cd": "ci_cd" in features,
                "coordination": True,
                "graceful_degradation": True
            },
            "skills": {
                "architect": {"enabled": True, "priority": "high"},
                "task-decomposer": {"enabled": True, "priority": "high"},
                "constraint-generator": {"enabled": True, "priority": "medium"},
                "context-analyzer": {"enabled": True, "priority": "medium"},
                "context-optimizer": {"enabled": True, "priority": "medium"},
                "cognitive-templater": {"enabled": True, "priority": "low"},
                "agent-creator": {"enabled": True, "priority": "low"},
                "dnaspec-init": {"enabled": True, "priority": "high"}
            }
        }
    }


def _setup_caching_system(dnaspec_dir):
    """è®¾ç½®ç¼“å­˜ç³»ç»Ÿ"""
    cache_config = {
        "cache_enabled": True,
        "cache_strategies": {
            "file_cache": {"enabled": True, "ttl": 3600},
            "memory_cache": {"enabled": True, "ttl": 1800},
            "distributed_cache": {"enabled": False}
        },
        "directories": {
            "temp": "cache/temp",
            "staging": "cache/staging", 
            "meta": "cache/meta"
        }
    }
    
    cache_config_file = os.path.join(dnaspec_dir, 'cache', 'config.json')
    with open(cache_config_file, 'w', encoding='utf-8') as f:
        json.dump(cache_config, f, indent=2, ensure_ascii=False)


def _setup_git_hooks(project_root, dnaspec_dir):
    """è®¾ç½®Gité’©å­"""
    git_hooks_dir = os.path.join(project_root, '.git', 'hooks')
    
    if os.path.exists(git_hooks_dir):
        # é¢„æäº¤é’©å­
        pre_commit_hook = """#!/bin/bash
# DNASPEC Pre-commit Hook
echo "ğŸ” Running DNASPEC pre-commit checks..."

# æ£€æŸ¥æ˜¯å¦éœ€è¦è¿è¡ŒæŠ€èƒ½
if [ -f "PROJECT_CONSTITUTION.md" ]; then
    echo "âœ… DNASPEC project detected"
    # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„æ£€æŸ¥é€»è¾‘
fi
"""
        
        hook_file = os.path.join(git_hooks_dir, 'pre-commit')
        with open(hook_file, 'w') as f:
            f.write(pre_commit_hook)
        
        # ä½¿é’©å­å¯æ‰§è¡Œ
        os.chmod(hook_file, 0o755)


def _setup_ci_cd_templates(dnaspec_dir):
    """è®¾ç½®CI/CDæ¨¡æ¿"""
    cicd_dir = os.path.join(dnaspec_dir, 'cicd')
    os.makedirs(cicd_dir, exist_ok=True)
    
    # GitHub Actionsæ¨¡æ¿
    github_workflow = """name: DNASPEC CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  dnaspec-validation:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: DNASPEC Skills Validation
      run: |
        echo "ğŸ” Running DNASPEC skill validations..."
        # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„éªŒè¯é€»è¾‘
"""
    
    with open(os.path.join(cicd_dir, 'github-actions.yml'), 'w') as f:
        f.write(github_workflow)


def _generate_next_steps(features):
    """ç”Ÿæˆåç»­æ­¥éª¤å»ºè®®"""
    steps = [
        "âœ… DNASPECåè°ƒæœºåˆ¶åˆå§‹åŒ–å®Œæˆ",
        "ğŸš€ ç°åœ¨å¯ä»¥å¼€å§‹ä½¿ç”¨DNASPECæŠ€èƒ½",
        "ğŸ“– æŸ¥çœ‹ PROJECT_CONSTITUTION.md äº†è§£è¯¦ç»†è§„åˆ™"
    ]
    
    if "caching" in features:
        steps.append("ğŸ’¾ ç¼“å­˜ç³»ç»Ÿå·²å¯ç”¨ï¼Œæ€§èƒ½å°†å¾—åˆ°ä¼˜åŒ–")
    
    if "git_hooks" in features:
        steps.append("ğŸ”— Gité’©å­å·²é…ç½®ï¼Œä»£ç è´¨é‡æ£€æŸ¥å°†è‡ªåŠ¨æ‰§è¡Œ")
    
    if "ci_cd" in features:
        steps.append("âš™ï¸ CI/CDæ¨¡æ¿å·²ç”Ÿæˆï¼Œå¯ç”¨äºè‡ªåŠ¨åŒ–éƒ¨ç½²")
    
    steps.extend([
        "",
        "ğŸ“ å¸¸ç”¨æŠ€èƒ½ä½¿ç”¨ç¤ºä¾‹:",
        "/architect \"system_type=web_application\"",
        "/task-decomposer \"input=implement_user_interface\"",
        "/constraint-generator \"requirements=performance_requirements\"",
        "",
        "ğŸ”§ çŠ¶æ€æ£€æŸ¥å‘½ä»¤:",
        "/dnaspec-init \"operation=detect\"",
        "/dnaspec-init \"operation=get-config\""
    ])
    
    return steps


def _get_file_modification_time(file_path):
    """è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´"""
    try:
        if os.path.exists(file_path):
            return datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
    except Exception:
        pass
    return None


if __name__ == "__main__":
    test_locally()