"""
æŠ€èƒ½è¯„ä¼°å™¨ - è¯„ä¼°å•ä¸ªDNASPECæŠ€èƒ½çš„è´¨é‡
"""
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

from .metrics import SkillEvaluationResult, QualityScore, PerformanceMetrics


class SkillEvaluator:
    """æŠ€èƒ½è¯„ä¼°å™¨"""

    def __init__(self, dnaspec_root: Optional[Path] = None):
        if dnaspec_root is None:
            dnaspec_root = Path(__file__).parent.parent.parent.parent
        self.dnaspec_root = Path(dnaspec_root)
        self.skills_dir = self.dnaspec_root / 'skills'

    def evaluate_skill(self, skill_name: str) -> SkillEvaluationResult:
        """è¯„ä¼°å•ä¸ªæŠ€èƒ½"""
        print(f"è¯„ä¼°æŠ€èƒ½: {skill_name}")

        skill_dir = self.skills_dir / skill_name
        if not skill_dir.exists():
            return SkillEvaluationResult(
                skill_name=skill_name,
                version="unknown",
                issues=[f"æŠ€èƒ½ç›®å½•ä¸å­˜åœ¨: {skill_dir}"]
            )

        # 1. æ£€æŸ¥æŠ€èƒ½ç»“æ„
        structure_score = self._evaluate_structure(skill_dir)

        # 2. è¿è¡Œæµ‹è¯•
        test_results = self._run_tests(skill_name)

        # 3. è¯„ä¼°æç¤ºè´¨é‡
        prompt_quality = self._evaluate_prompts(skill_dir)

        # 4. è¯„ä¼°ä»£ç è´¨é‡
        code_quality = self._evaluate_code(skill_dir)

        # 5. ç»„åˆè´¨é‡è¯„åˆ†
        quality_score = QualityScore(
            clarity=(structure_score + prompt_quality['clarity']) / 2,
            completeness=prompt_quality['completeness'],
            consistency=code_quality['consistency'],
            efficiency=code_quality['efficiency'],
            relevance=0.8  # é»˜è®¤è‰¯å¥½
        )

        # 6. ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(
            skill_name, quality_score, test_results
        )

        return SkillEvaluationResult(
            skill_name=skill_name,
            version=self._get_version(skill_dir),
            evaluated_at=datetime.now(),
            quality_score=quality_score,
            tests_passed=test_results['passed'],
            tests_total=test_results['total'],
            test_success_rate=test_results['success_rate'],
            issues=test_results['issues'],
            recommendations=recommendations
        )

    def _evaluate_structure(self, skill_dir: Path) -> float:
        """è¯„ä¼°æŠ€èƒ½ç›®å½•ç»“æ„"""
        required_files = [
            'SKILL.md',
            'prompts/00_context.md',
            'prompts/01_basic.md',
            'prompts/02_intermediate.md',
            'scripts/validator.py',
            'scripts/calculator.py',
            'scripts/analyzer.py',
            'scripts/executor.py',
        ]

        score = 0.0
        for file in required_files:
            if (skill_dir / file).exists():
                score += 1.0 / len(required_files)

        return score

    def _run_tests(self, skill_name: str) -> Dict:
        """è¿è¡ŒæŠ€èƒ½æµ‹è¯•"""
        test_file = self.dnaspec_root / f'test_{skill_name.replace("dnaspec-", "")}.py'

        if not test_file.exists():
            return {
                'passed': 0,
                'total': 0,
                'success_rate': 0.0,
                'issues': [f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}"]
            }

        try:
            result = subprocess.run(
                [sys.executable, str(test_file)],
                capture_output=True,
                text=True,
                timeout=30,
                cwd=self.dnaspec_root
            )

            if result.returncode == 0:
                # ç®€å•è§£ææµ‹è¯•è¾“å‡º
                output = result.stdout
                if 'âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡' in output:
                    return {
                        'passed': 5,  # ä¼°è®¡å€¼
                        'total': 5,
                        'success_rate': 1.0,
                        'issues': []
                    }

            return {
                'passed': 0,
                'total': 1,
                'success_rate': 0.0,
                'issues': [f"æµ‹è¯•å¤±è´¥: {result.stderr[:200]}"]
            }
        except Exception as e:
            return {
                'passed': 0,
                'total': 0,
                'success_rate': 0.0,
                'issues': [f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}"]
            }

    def _evaluate_prompts(self, skill_dir: Path) -> Dict:
        """è¯„ä¼°æç¤ºæ–‡ä»¶è´¨é‡"""
        prompts_dir = skill_dir / 'prompts'

        clarity_scores = []
        completeness_scores = []

        for prompt_file in ['00_context.md', '01_basic.md', '02_intermediate.md']:
            file_path = prompts_dir / prompt_file
            if not file_path.exists():
                continue

            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # æ¸…æ™°åº¦è¯„ä¼°ï¼šæ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†markdownæ ¼å¼
            clarity = 0.5
            if '##' in content:  # æœ‰æ ‡é¢˜
                clarity += 0.2
            if '```' in content:  # æœ‰ä»£ç å—
                clarity += 0.15
            if '-' in content:  # æœ‰åˆ—è¡¨
                clarity += 0.15

            # å®Œæ•´æ€§è¯„ä¼°ï¼šæ£€æŸ¥å†…å®¹é•¿åº¦
            min_length = 300
            completeness = min(len(content) / min_length, 1.0)

            clarity_scores.append(clarity)
            completeness_scores.append(completeness)

        return {
            'clarity': sum(clarity_scores) / len(clarity_scores) if clarity_scores else 0.5,
            'completeness': sum(completeness_scores) / len(completeness_scores) if completeness_scores else 0.5
        }

    def _evaluate_code(self, skill_dir: Path) -> Dict:
        """è¯„ä¼°ä»£ç è´¨é‡"""
        scripts_dir = skill_dir / 'scripts'

        consistency = 0.8  # é»˜è®¤è‰¯å¥½
        efficiency = 0.8

        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„è„šæœ¬
        required_scripts = ['validator.py', 'calculator.py', 'analyzer.py', 'executor.py']
        existing_scripts = sum(1 for s in required_scripts if (scripts_dir / s).exists())
        consistency = existing_scripts / len(required_scripts)

        return {
            'consistency': consistency,
            'efficiency': efficiency
        }

    def _get_version(self, skill_dir: Path) -> str:
        """è·å–æŠ€èƒ½ç‰ˆæœ¬"""
        package_json = self.dnaspec_root / 'package.json'
        if package_json.exists():
            import json
            with open(package_json, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('version', 'unknown')
        return 'unknown'

    def _generate_recommendations(
        self,
        skill_name: str,
        quality_score: QualityScore,
        test_results: Dict
    ) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # åŸºäºè´¨é‡è¯„åˆ†ç”Ÿæˆå»ºè®®
        if quality_score.clarity < 0.7:
            recommendations.append("ğŸ“ æç¤ºæ¸…æ™°åº¦è¾ƒä½ï¼Œå»ºè®®æ”¹è¿›æç¤ºæ–‡ä»¶çš„ç»„ç»‡ç»“æ„å’Œè¡¨è¿°")
        if quality_score.completeness < 0.7:
            recommendations.append("ğŸ“‹ æç¤ºå®Œæ•´æ€§ä¸è¶³ï¼Œå»ºè®®è¡¥å……æ›´å¤šç¤ºä¾‹å’Œè¯´æ˜")
        if quality_score.consistency < 0.7:
            recommendations.append("ğŸ”§ ä»£ç ä¸€è‡´æ€§éœ€è¦æ”¹è¿›ï¼Œç¡®ä¿æ‰€æœ‰è„šæœ¬éƒ½å­˜åœ¨")
        if quality_score.overall < 0.6:
            recommendations.append(f"âš ï¸ æŠ€èƒ½ {skill_name} æ•´ä½“è´¨é‡è¾ƒä½ï¼Œå»ºè®®å…¨é¢ä¼˜åŒ–")

        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        if test_results['success_rate'] < 1.0:
            recommendations.append(f"ğŸ§ª æµ‹è¯•é€šè¿‡ç‡ä»… {test_results['success_rate']*100:.0f}%ï¼Œéœ€è¦ä¿®å¤å¤±è´¥çš„æµ‹è¯•")

        if not recommendations:
            recommendations.append("âœ… æŠ€èƒ½è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ")

        return recommendations

    def evaluate_all_skills(self) -> Dict[str, SkillEvaluationResult]:
        """è¯„ä¼°æ‰€æœ‰æŠ€èƒ½"""
        results = {}

        if not self.skills_dir.exists():
            return results

        for skill_dir in sorted(self.skills_dir.iterdir()):
            if not skill_dir.is_dir():
                continue

            skill_name = skill_dir.name
            if not skill_name.startswith('dnaspec-'):
                continue

            results[skill_name] = self.evaluate_skill(skill_name)

        return results
