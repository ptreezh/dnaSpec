"""
Constraint Generator Executor
æ™ºèƒ½åè°ƒå™¨ - ç»„åˆæ‰€æœ‰ç»„ä»¶å¹¶é€‰æ‹©åˆé€‚çš„æç¤ºè¯å±‚æ¬¡
"""

from typing import Dict, Any, Optional
from pathlib import Path
import json

from .validator import ConstraintValidator, ValidationResult
from .calculator import ConstraintCalculator, ConstraintMetrics
from .analyzer import ConstraintAnalyzer, ConstraintAnalysis


class ConstraintExecutor:
    """çº¦æŸç”Ÿæˆæ‰§è¡Œå™¨"""

    def __init__(self, skill_dir: Optional[Path] = None):
        if skill_dir is None:
            skill_dir = Path(__file__).parent.parent

        self.skill_dir = Path(skill_dir)
        self.prompts_dir = self.skill_dir / "prompts"

        # åˆå§‹åŒ–ç»„ä»¶
        self.validator = ConstraintValidator()
        self.calculator = ConstraintCalculator()
        self.analyzer = ConstraintAnalyzer()

    def execute(
        self,
        request: str,
        context: Optional[Dict] = None,
        force_level: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œçº¦æŸç”Ÿæˆ

        Args:
            request: çº¦æŸç”Ÿæˆè¯·æ±‚
            context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            force_level: å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šå±‚æ¬¡çš„æç¤ºè¯ (00/01/02/03)

        Returns:
            åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
            - success: bool - æ˜¯å¦æˆåŠŸ
            - validation: ValidationResult - éªŒè¯ç»“æœ
            - metrics: ConstraintMetrics - è®¡ç®—çš„æŒ‡æ ‡
            - analysis: ConstraintAnalysis - åˆ†æç»“æœ
            - prompt_level: str - ä½¿ç”¨çš„æç¤ºè¯å±‚æ¬¡
            - prompt_content: str - æç¤ºè¯å†…å®¹
            - recommendations: List[str] - ç»¼åˆå»ºè®®
        """
        # 1. éªŒè¯è¯·æ±‚
        validation = self.validator.validate(request, context)

        if not validation.is_valid:
            return {
                "success": False,
                "validation": validation,
                "error": "è¯·æ±‚éªŒè¯å¤±è´¥"
            }

        # 2. è®¡ç®—æŒ‡æ ‡
        metrics = self.calculator.calculate(request, context)

        # 3. åˆ†æçº¦æŸ
        analysis = self.analyzer.analyze(request, context)

        # 4. é€‰æ‹©æç¤ºè¯å±‚æ¬¡
        if force_level:
            prompt_level = force_level
        else:
            prompt_level = self._select_level(metrics, analysis)

        # 5. åŠ è½½æç¤ºè¯
        prompt_content = self._load_prompt(prompt_level)

        # 6. ç”Ÿæˆç»¼åˆå»ºè®®
        recommendations = self._generate_recommendations(metrics, analysis)

        return {
            "success": True,
            "validation": validation,
            "metrics": metrics,
            "analysis": analysis,
            "prompt_level": prompt_level,
            "prompt_content": prompt_content,
            "recommendations": recommendations
        }

    def _select_level(
        self,
        metrics: ConstraintMetrics,
        analysis: ConstraintAnalysis
    ) -> str:
        """
        æ™ºèƒ½é€‰æ‹©æç¤ºè¯å±‚æ¬¡

        è§„åˆ™:
        - 00: åŸºç¡€æ¦‚å¿µ (å¤æ‚åº¦ < 0.3, tokens < 1000)
        - 01: åŸºæœ¬åº”ç”¨ (å¤æ‚åº¦ < 0.5, tokens < 3000)
        - 02: ä¸­çº§åœºæ™¯ (å¤æ‚åº¦ < 0.7, tokens < 5000)
        - 03: é«˜çº§åº”ç”¨ (å…¶ä»–æƒ…å†µ)
        """
        complexity = metrics.complexity_score
        tokens = metrics.token_count

        # æ£€æµ‹çº¦æŸå†²çª â†’ é«˜çº§å±‚æ¬¡
        if analysis.potential_conflicts:
            return "03"

        # æ£€æµ‹å¤šç§çº¦æŸç±»å‹ â†’ ä¸­çº§æˆ–é«˜çº§
        type_count = sum([
            metrics.has_performance_constraint,
            metrics.has_security_constraint,
            metrics.has_functional_constraint,
            metrics.has_technical_constraint
        ])

        if type_count >= 3:
            return "03"
        elif type_count >= 2:
            return "02"

        # åŸºäºå¤æ‚åº¦å’Œtokenæ•°é‡
        if complexity < 0.3 and tokens < 1000:
            return "00"
        elif complexity < 0.5 and tokens < 3000:
            return "01"
        elif complexity < 0.7 or tokens < 5000:
            return "02"
        else:
            return "03"

    def _load_prompt(self, level: str) -> str:
        """åŠ è½½æç¤ºè¯æ–‡ä»¶"""
        prompt_file = self.prompts_dir / f"{level}_context.md"

        if not prompt_file.exists():
            return f"# çº¦æŸç”Ÿæˆ - å±‚æ¬¡ {level}\n\næç¤ºè¯æ–‡ä»¶æœªæ‰¾åˆ°"

        with open(prompt_file, 'r', encoding='utf-8') as f:
            return f.read()

    def _generate_recommendations(
        self,
        metrics: ConstraintMetrics,
        analysis: ConstraintAnalysis
    ) -> list:
        """ç”Ÿæˆç»¼åˆå»ºè®®"""
        recommendations = []

        # æ¥è‡ªcalculatorçš„å»ºè®®
        recommendations.extend(metrics.recommendations)

        # æ¥è‡ªanalyzerçš„å»ºè®®
        recommendations.extend(analysis.recommendations)

        # é¢å¤–çš„æ‰§è¡Œå»ºè®®
        if metrics.complexity_score > 0.7:
            recommendations.append(
                "è¯·æ±‚å¤æ‚åº¦è¾ƒé«˜ï¼Œå»ºè®®åˆ†é˜¶æ®µå¤„ç†çº¦æŸ"
            )

        if metrics.specificity_score < 0.5:
            recommendations.append(
                "å»ºè®®ä½¿ç”¨SMARTåŸåˆ™ä½¿çº¦æŸæ›´å…·ä½“ã€å¯æµ‹é‡"
            )

        if analysis.potential_conflicts:
            recommendations.append(
                f"æ£€æµ‹åˆ°{len(analysis.potential_conflicts)}ä¸ªæ½œåœ¨å†²çªï¼Œéœ€è¦ä¼˜å…ˆå¤„ç†"
            )

        return recommendations


if __name__ == "__main__":
    # æµ‹è¯•
    executor = ConstraintExecutor()

    test_cases = [
        ("ç”ŸæˆAPIæ€§èƒ½çº¦æŸ", None),
        ("ç³»ç»Ÿéœ€è¦é«˜å®‰å…¨æ€§å’Œå¿«é€Ÿå“åº”ï¼Œå¯èƒ½å­˜åœ¨å†²çª", None),
        ("å“åº”æ—¶é—´å°äº100msï¼Œæ”¯æŒ1000 QPSï¼Œéœ€è¦åŠ å¯†", None),
    ]

    for request, context in test_cases:
        print(f"\n{'='*60}")
        print(f"è¯·æ±‚: {request}")
        print(f"{'='*60}")

        result = executor.execute(request, context)

        if result["success"]:
            print(f"âœ… éªŒè¯é€šè¿‡")
            print(f"ğŸ“Š å¤æ‚åº¦: {result['metrics'].complexity_score:.2f}")
            print(f"ğŸ“ˆ æ¨èå±‚æ¬¡: {result['prompt_level']}")
            print(f"ğŸ” æ£€æµ‹åˆ°çš„çº¦æŸç±»å‹: {[t.value for t in result['analysis'].detected_types]}")
            print(f"âš ï¸  æ½œåœ¨å†²çª: {len(result['analysis'].potential_conflicts)}")
            print(f"ğŸ’¡ å»ºè®®: {len(result['recommendations'])}")

            if result['analysis'].potential_conflicts:
                for conflict in result['analysis'].potential_conflicts:
                    print(f"   - {conflict}")
        else:
            print(f"âŒ éªŒè¯å¤±è´¥")
            for issue in result["validation"].issues:
                print(f"   [{issue.severity.value}] {issue.message}")
