"""
Context Fundamentals Executor

æ™ºèƒ½åè°ƒå™¨ï¼šæ•´åˆå®šæ€§åˆ†æå’Œå®šé‡è®¡ç®—
æ ¹æ®è¯·æ±‚å¤æ‚åº¦å’Œä¸Šä¸‹æ–‡ç‰¹å¾è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æç¤ºè¯å±‚æ¬¡
"""

from typing import Dict, Any, Optional, List
from pathlib import Path
import json

from .validator import ContextFundamentalsValidator, ValidationResult
from .calculator import ContextFundamentalsCalculator, ContextMetrics
from .analyzer import ContextFundamentalsAnalyzer, ContextAnalysis, FailureModeDetection


class ContextFundamentalsExecutor:
    """
    ä¸Šä¸‹æ–‡åŸºç¡€çŸ¥è¯†æ‰§è¡Œå™¨

    èŒè´£ï¼š
    1. éªŒè¯è¾“å…¥
    2. è®¡ç®—æŒ‡æ ‡
    3. åˆ†æä¸Šä¸‹æ–‡
    4. é€‰æ‹©åˆé€‚çš„æç¤ºè¯å±‚æ¬¡
    5. åŠ è½½æç¤ºè¯å†…å®¹
    6. è¿”å›å®Œæ•´ç»“æœ
    """

    def __init__(self, skill_dir: Optional[Path] = None):
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨

        Args:
            skill_dir: æŠ€èƒ½ç›®å½•è·¯å¾„
        """
        if skill_dir is None:
            # é»˜è®¤ä¸ºå½“å‰æŠ€èƒ½ç›®å½•
            skill_dir = Path(__file__).parent.parent

        self.skill_dir = Path(skill_dir)
        self.prompts_dir = self.skill_dir / "prompts"

        # åˆå§‹åŒ–ç»„ä»¶
        self.validator = ContextFundamentalsValidator()
        self.calculator = ContextFundamentalsCalculator()
        self.analyzer = ContextFundamentalsAnalyzer()

    def execute(
        self,
        request: str,
        context: Optional[Dict[str, Any]] = None,
        force_level: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¸Šä¸‹æ–‡åŸºç¡€çŸ¥è¯†åˆ†æ

        Args:
            request: ç”¨æˆ·è¯·æ±‚
            context: é™„åŠ ä¸Šä¸‹æ–‡
            force_level: å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šæç¤ºè¯å±‚æ¬¡ï¼ˆ"00", "01", "02", "03"ï¼‰

        Returns:
            Dict: å®Œæ•´çš„æ‰§è¡Œç»“æœ
        """
        # 1. éªŒè¯è¾“å…¥
        validation = self.validator.validate(request, context)

        # 2. è®¡ç®—æŒ‡æ ‡
        metrics = self.calculator.calculate(request, context)

        # 3. åˆ†æä¸Šä¸‹æ–‡
        analysis = self.analyzer.analyze(request, context)

        # 4. é€‰æ‹©æç¤ºè¯å±‚æ¬¡
        if force_level:
            prompt_level = force_level
        else:
            prompt_level = self._select_prompt_level(metrics, analysis, validation)

        # 5. åŠ è½½æç¤ºè¯
        prompt_content = self._load_prompt(prompt_level)

        # 6. æ„å»ºæœ€ç»ˆç»“æœ
        result = {
            "validation": self._format_validation(validation),
            "metrics": self._format_metrics(metrics),
            "analysis": self._format_analysis(analysis),
            "prompt_level": prompt_level,
            "prompt_content": prompt_content,
            "summary": self._generate_summary(validation, metrics, analysis, prompt_level),
            "recommendations": self._generate_recommendations(metrics, analysis)
        }

        return result

    def _select_prompt_level(
        self,
        metrics: ContextMetrics,
        analysis: ContextAnalysis,
        validation: ValidationResult
    ) -> str:
        """
        æ™ºèƒ½é€‰æ‹©æç¤ºè¯å±‚æ¬¡

        é€‰æ‹©é€»è¾‘ï¼š
        1. å¦‚æœæœ‰ä¸¥é‡éªŒè¯é”™è¯¯ï¼Œä½¿ç”¨æœ€ç®€å•çš„Level 00
        2. åŸºäºå¤æ‚åº¦åˆ†æ•°ã€tokenæ•°é‡ã€å¤±æ•ˆæ¨¡å¼ç»¼åˆåˆ¤æ–­
        3. å€¾å‘äºä½¿ç”¨è¾ƒä½å±‚æ¬¡ï¼ˆæ¸è¿›å¼æŠ«éœ²ï¼‰
        """
        # æ£€æŸ¥éªŒè¯ç»“æœ
        if validation.has_errors():
            # æœ‰éªŒè¯é”™è¯¯ï¼Œä½¿ç”¨åŸºç¡€å±‚
            return "00"

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡å¤±æ•ˆæ¨¡å¼
        critical_failures = [f for f in analysis.detected_failures if f.severity == "high"]
        if critical_failures:
            # æœ‰ä¸¥é‡é—®é¢˜ï¼Œå…ˆè§£å†³åŸºç¡€
            return "00"

        # åŸºäºæŒ‡æ ‡ç»¼åˆåˆ¤æ–­
        complexity = metrics.complexity_score
        tokens = metrics.token_count

        # å†³ç­–æ ‘
        if complexity < 0.3 and tokens < 5000:
            # ç®€å•è¯·æ±‚ï¼Œæ ¸å¿ƒæ¦‚å¿µå±‚
            return "00"

        elif complexity < 0.5 and tokens < 10000:
            # ä¸­ç­‰å¤æ‚åº¦ï¼ŒåŸºç¡€åº”ç”¨å±‚
            return "01"

        elif complexity < 0.7 or tokens < 20000:
            # è¾ƒå¤æ‚ï¼Œä¸­çº§åœºæ™¯å±‚
            return "02"

        else:
            # é«˜åº¦å¤æ‚ï¼Œé«˜çº§åº”ç”¨å±‚
            return "03"

    def _load_prompt(self, level: str) -> str:
        """
        åŠ è½½æŒ‡å®šå±‚æ¬¡çš„æç¤ºè¯

        Args:
            level: æç¤ºè¯å±‚æ¬¡ï¼ˆ"00", "01", "02", "03"ï¼‰

        Returns:
            str: æç¤ºè¯å†…å®¹
        """
        # æ–‡ä»¶åæ˜ å°„
        filename_map = {
            "00": "00_context.md",
            "01": "01_basic.md",
            "02": "02_intermediate.md",
            "03": "03_advanced.md"
        }

        if level not in filename_map:
            raise ValueError(f"æ— æ•ˆçš„æç¤ºè¯å±‚æ¬¡: {level}")

        prompt_file = self.prompts_dir / filename_map[level]

        if not prompt_file.exists():
            raise FileNotFoundError(f"æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompt_file}")

        with open(prompt_file, 'r', encoding='utf-8') as f:
            content = f.read()

        return content

    def _format_validation(self, validation: ValidationResult) -> Dict[str, Any]:
        """æ ¼å¼åŒ–éªŒè¯ç»“æœ"""
        return {
            "is_valid": validation.is_valid,
            "has_errors": validation.has_errors(),
            "has_warnings": validation.has_warnings(),
            "summary": validation.get_summary(),
            "issues": [
                {
                    "severity": issue.severity.value,
                    "category": issue.category,
                    "message": issue.message,
                    "suggestion": issue.suggestion
                }
                for issue in validation.issues
            ]
        }

    def _format_metrics(self, metrics: ContextMetrics) -> Dict[str, Any]:
        """æ ¼å¼åŒ–æŒ‡æ ‡"""
        return {
            "token_count": metrics.token_count,
            "character_count": metrics.character_count,
            "word_count": metrics.word_count,
            "line_count": metrics.line_count,
            "complexity_score": metrics.complexity_score,
            "information_density": metrics.information_density,
            "structure_quality": metrics.structure_quality,
            "relevance_score": metrics.relevance_score,
            "keyword_overlap": metrics.keyword_overlap,
            "completeness_score": metrics.completeness_score,
            "consistency_score": metrics.consistency_score,
            "freshness_score": metrics.freshness_score,
            "recommended_level": metrics.recommended_prompt_level,
            "actions": metrics.recommended_actions,
            "warnings": metrics.warnings
        }

    def _format_analysis(self, analysis: ContextAnalysis) -> Dict[str, Any]:
        """æ ¼å¼åŒ–åˆ†æç»“æœ"""
        return {
            "detected_failures": [
                {
                    "mode": failure.mode.value,
                    "severity": failure.severity,
                    "evidence": failure.evidence,
                    "suggestions": failure.suggestions
                }
                for failure in analysis.detected_failures
            ],
            "recognized_patterns": [
                {
                    "type": pattern.pattern_type,
                    "confidence": pattern.confidence,
                    "description": pattern.description
                }
                for pattern in analysis.recognized_patterns
            ],
            "quality_scores": analysis.quality_scores,
            "optimization_suggestions": analysis.optimization_suggestions,
            "recommended_strategy": analysis.recommended_strategy
        }

    def _generate_summary(
        self,
        validation: ValidationResult,
        metrics: ContextMetrics,
        analysis: ContextAnalysis,
        prompt_level: str
    ) -> str:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        lines = []

        # éªŒè¯çŠ¶æ€
        if validation.is_valid:
            lines.append("âœ… è¯·æ±‚éªŒè¯é€šè¿‡")
        else:
            lines.append("âŒ è¯·æ±‚éªŒè¯å¤±è´¥")

        # Tokenæ•°é‡
        lines.append(f"ğŸ“Š Tokenæ•°é‡: {metrics.token_count:,}")

        # å¤æ‚åº¦
        complexity_desc = self._describe_complexity(metrics.complexity_score)
        lines.append(f"ğŸ¯ å¤æ‚åº¦: {complexity_desc}")

        # é€‰æ‹©çš„å±‚æ¬¡
        level_desc = {
            "00": "æ ¸å¿ƒæ¦‚å¿µå±‚ï¼ˆå¿«é€Ÿç†è§£ï¼‰",
            "01": "åŸºç¡€åº”ç”¨å±‚ï¼ˆå¸¸è§åœºæ™¯ï¼‰",
            "02": "ä¸­çº§åœºæ™¯å±‚ï¼ˆå¤æ‚ä»»åŠ¡ï¼‰",
            "03": "é«˜çº§åº”ç”¨å±‚ï¼ˆå¤§è§„æ¨¡ç³»ç»Ÿï¼‰"
        }.get(prompt_level, "æœªçŸ¥")
        lines.append(f"ğŸ“š æç¤ºè¯å±‚æ¬¡: Level {prompt_level} - {level_desc}")

        # å¤±æ•ˆæ¨¡å¼
        if analysis.detected_failures:
            lines.append(f"âš ï¸ æ£€æµ‹åˆ°{len(analysis.detected_failures)}ä¸ªæ½œåœ¨å¤±æ•ˆæ¨¡å¼")
        else:
            lines.append("âœ… æœªæ£€æµ‹åˆ°å¤±æ•ˆæ¨¡å¼")

        # è´¨é‡åˆ†æ•°
        avg_quality = sum(analysis.quality_scores.values()) / len(analysis.quality_scores) if analysis.quality_scores else 0
        quality_desc = self._describe_quality(avg_quality)
        lines.append(f"ğŸ“ˆ ä¸Šä¸‹æ–‡è´¨é‡: {quality_desc}")

        return "\n".join(lines)

    def _describe_complexity(self, score: float) -> str:
        """æè¿°å¤æ‚åº¦"""
        if score < 0.3:
            return "ç®€å•"
        elif score < 0.5:
            return "ä¸­ç­‰"
        elif score < 0.7:
            return "è¾ƒå¤æ‚"
        else:
            return "é«˜åº¦å¤æ‚"

    def _describe_quality(self, score: float) -> str:
        """æè¿°è´¨é‡"""
        if score > 0.8:
            return "ä¼˜ç§€"
        elif score > 0.6:
            return "è‰¯å¥½"
        elif score > 0.4:
            return "ä¸­ç­‰"
        else:
            return "éœ€è¦æ”¹è¿›"

    def _generate_recommendations(
        self,
        metrics: ContextMetrics,
        analysis: ContextAnalysis
    ) -> List[str]:
        """ç”Ÿæˆç»¼åˆæ¨è"""
        recommendations = []

        # æ¥è‡ªæŒ‡æ ‡çš„å»ºè®®
        recommendations.extend(metrics.recommended_actions)

        # æ¥è‡ªåˆ†æçš„å»ºè®®
        recommendations.extend(analysis.optimization_suggestions)

        # æ¥è‡ªæ¨èç­–ç•¥çš„å»ºè®®
        recommendations.append(f"ç­–ç•¥å»ºè®®: {analysis.recommended_strategy}")

        # å»é‡å¹¶æ’åº
        unique_recommendations = list(set(recommendations))

        return unique_recommendations


# ä¾¿æ·å‡½æ•°
def execute_context_fundamentals(
    request: str,
    context: Optional[Dict[str, Any]] = None,
    skill_dir: Optional[Path] = None,
    force_level: Optional[str] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œä¸Šä¸‹æ–‡åŸºç¡€çŸ¥è¯†åˆ†æçš„ä¾¿æ·å‡½æ•°

    Args:
        request: ç”¨æˆ·è¯·æ±‚
        context: é™„åŠ ä¸Šä¸‹æ–‡
        skill_dir: æŠ€èƒ½ç›®å½•
        force_level: å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šæç¤ºè¯å±‚æ¬¡

    Returns:
        Dict: å®Œæ•´çš„æ‰§è¡Œç»“æœ
    """
    executor = ContextFundamentalsExecutor(skill_dir)
    return executor.execute(request, context, force_level)


if __name__ == "__main__":
    import sys

    # æµ‹è¯•
    test_cases = [
        {
            "name": "ç®€å•é—®é¢˜",
            "request": "ä»€ä¹ˆæ˜¯ä¸Šä¸‹æ–‡ï¼Ÿ",
            "context": None
        },
        {
            "name": "ä¸­ç­‰å¤æ‚åº¦",
            "request": "å¦‚ä½•åœ¨AIç³»ç»Ÿä¸­ä¼˜åŒ–ä¸Šä¸‹æ–‡ç®¡ç†ï¼Ÿè¯·è¯´æ˜æœ€ä½³å®è·µå’Œå¸¸è§é™·é˜±",
            "context": {"domain": "AI", "scale": "medium"}
        },
        {
            "name": "é«˜åº¦å¤æ‚",
            "request": "è®¾è®¡ä¸€ä¸ªåŒ…å«50ä¸ªå¾®æœåŠ¡çš„å¤§å‹ç”µå•†ç³»ç»Ÿçš„ä¸Šä¸‹æ–‡ç®¡ç†æ¶æ„ï¼Œéœ€è¦è€ƒè™‘åˆ†å¸ƒå¼åä½œã€ç‰ˆæœ¬æ§åˆ¶ã€æ€§èƒ½ä¼˜åŒ–ç­‰å¤šä¸ªæ–¹é¢",
            "context": {"scale": "large", "services": 50, "architecture": "microservices"}
        }
    ]

    for test_case in test_cases:
        print(f"\n{'='*80}")
        print(f"æµ‹è¯•ç”¨ä¾‹: {test_case['name']}")
        print(f"{'='*80}\n")

        result = execute_context_fundamentals(
            request=test_case["request"],
            context=test_case["context"]
        )

        # æ‰“å°æ‘˜è¦
        print("ğŸ“‹ æ‰§è¡Œæ‘˜è¦:")
        print(result["summary"])
        print()

        # æ‰“å°éªŒè¯ç»“æœ
        if result["validation"]["issues"]:
            print("ğŸ” éªŒè¯é—®é¢˜:")
            for issue in result["validation"]["issues"]:
                print(f"  [{issue['severity'].upper()}] {issue['message']}")
                if issue.get("suggestion"):
                    print(f"    ğŸ’¡ {issue['suggestion']}")
            print()

        # æ‰“å°æ£€æµ‹åˆ°çš„å¤±æ•ˆæ¨¡å¼
        if result["analysis"]["detected_failures"]:
            print("âš ï¸ å¤±æ•ˆæ¨¡å¼:")
            for failure in result["analysis"]["detected_failures"]:
                print(f"\n  {failure['mode']} ({failure['severity']}):")
                for evidence in failure["evidence"]:
                    print(f"    - {evidence}")
            print()

        # æ‰“å°è¯†åˆ«çš„æ¨¡å¼
        if result["analysis"]["recognized_patterns"]:
            print("ğŸ” è¯†åˆ«çš„æ¨¡å¼:")
            for pattern in result["analysis"]["recognized_patterns"]:
                print(f"  - {pattern['type']}: {pattern['description']}")
            print()

        # æ‰“å°æ¨è
        if result["recommendations"]:
            print("ğŸ’¡ æ¨è:")
            for rec in result["recommendations"][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  - {rec}")
            print()

        # æ‰“å°æç¤ºè¯å±‚æ¬¡
        print(f"ğŸ“š é€‰æ‹©çš„æç¤ºè¯: Level {result['prompt_level']}")
        print(f"   æç¤ºè¯é•¿åº¦: {len(result['prompt_content'])} å­—ç¬¦")
        print()

        # è¯¢é—®æ˜¯å¦ç»§ç»­
        input("\næŒ‰Enteré”®ç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹...")
