"""
Context Fundamentals Calculator

è´Ÿè´£è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³æŒ‡æ ‡
æä¾›ç¡®å®šæ€§çš„å®šé‡è®¡ç®—
"""

from typing import Dict, Any, Optional, List
from dataclasses import dataclass
import re
import json


@dataclass
class ContextMetrics:
    """ä¸Šä¸‹æ–‡æŒ‡æ ‡"""
    # åŸºç¡€æŒ‡æ ‡
    token_count: int
    character_count: int
    word_count: int
    line_count: int

    # å¤æ‚åº¦æŒ‡æ ‡
    complexity_score: float  # 0.0-1.0
    information_density: float  # 0.0-1.0
    structure_quality: float  # 0.0-1.0

    # ç›¸å…³æ€§æŒ‡æ ‡
    relevance_score: float  # 0.0-1.0
    keyword_overlap: float  # 0.0-1.0

    # å¥åº·åº¦æŒ‡æ ‡
    completeness_score: float  # 0.0-1.0
    consistency_score: float  # 0.0-1.0
    freshness_score: float  # 0.0-1.0

    # æ¨èæŒ‡æ ‡
    recommended_prompt_level: str  # "00", "01", "02", "03"
    recommended_actions: List[str]
    warnings: List[str]


class ContextFundamentalsCalculator:
    """ä¸Šä¸‹æ–‡åŸºç¡€è®¡ç®—å™¨"""

    # Tokenä¼°ç®—å¸¸é‡
    CHARS_PER_TOKEN_EN = 4.0  # è‹±æ–‡
    CHARS_PER_TOKEN_CN = 1.5  # ä¸­æ–‡
    CHARS_PER_TOKEN_MIXED = 2.5  # ä¸­è‹±æ··åˆ

    # é˜ˆå€¼å®šä¹‰
    OPTIMAL_TOKEN_COUNT = 10000
    MAX_TOKEN_COUNT = 50000
    WARNING_TOKEN_COUNT = 30000

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–è®¡ç®—å™¨

        Args:
            config: é…ç½®å‚æ•°
        """
        self.config = config or {}

    def calculate(self, request: str, context: Optional[Dict[str, Any]] = None) -> ContextMetrics:
        """
        è®¡ç®—ä¸Šä¸‹æ–‡æŒ‡æ ‡

        Args:
            request: ç”¨æˆ·è¯·æ±‚
            context: é™„åŠ ä¸Šä¸‹æ–‡

        Returns:
            ContextMetrics: è®¡ç®—ç»“æœ
        """
        # 1. è®¡ç®—åŸºç¡€æŒ‡æ ‡
        character_count = len(request)
        word_count = self._count_words(request)
        line_count = self._count_lines(request)
        token_count = self._estimate_tokens(request)

        if context:
            context_str = str(context)
            character_count += len(context_str)
            token_count += self._estimate_tokens(context_str)

        # 2. è®¡ç®—å¤æ‚åº¦æŒ‡æ ‡
        complexity_score = self._calculate_complexity_score(request, context)
        information_density = self._calculate_information_density(request)
        structure_quality = self._calculate_structure_quality(request, context)

        # 3. è®¡ç®—ç›¸å…³æ€§æŒ‡æ ‡
        relevance_score = self._calculate_relevance_score(request, context)
        keyword_overlap = self._calculate_keyword_overlap(request, context)

        # 4. è®¡ç®—å¥åº·åº¦æŒ‡æ ‡
        completeness_score = self._calculate_completeness_score(request, context)
        consistency_score = self._calculate_consistency_score(context)
        freshness_score = self._calculate_freshness_score(context)

        # 5. æ¨èæç¤ºè¯å±‚æ¬¡
        recommended_level = self._recommend_prompt_level(
            complexity_score, token_count, information_density
        )

        # 6. ç”Ÿæˆæ¨èæ“ä½œå’Œè­¦å‘Š
        recommended_actions = self._generate_recommendations(
            token_count, complexity_score, relevance_score, completeness_score
        )
        warnings = self._generate_warnings(
            token_count, complexity_score, consistency_score
        )

        return ContextMetrics(
            token_count=token_count,
            character_count=character_count,
            word_count=word_count,
            line_count=line_count,
            complexity_score=complexity_score,
            information_density=information_density,
            structure_quality=structure_quality,
            relevance_score=relevance_score,
            keyword_overlap=keyword_overlap,
            completeness_score=completeness_score,
            consistency_score=consistency_score,
            freshness_score=freshness_score,
            recommended_prompt_level=recommended_level,
            recommended_actions=recommended_actions,
            warnings=warnings
        )

    def _count_words(self, text: str) -> int:
        """è®¡ç®—å­—æ•°ï¼ˆä¸­æ–‡æŒ‰å­—ç¬¦ï¼Œè‹±æ–‡æŒ‰å•è¯ï¼‰"""
        # ä¸­æ–‡
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        # è‹±æ–‡å•è¯
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
        return chinese_chars + english_words

    def _count_lines(self, text: str) -> int:
        """è®¡ç®—è¡Œæ•°"""
        return len(text.split('\n'))

    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡"""
        # æ£€æµ‹è¯­è¨€æ¯”ä¾‹
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        total_chars = len(text)
        chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0

        # æ ¹æ®è¯­è¨€æ¯”ä¾‹é€‰æ‹©ä¼°ç®—æ–¹æ³•
        if chinese_ratio > 0.7:
            # ä¸»è¦ä¸­æ–‡
            chars_per_token = self.CHARS_PER_TOKEN_CN
        elif chinese_ratio < 0.3:
            # ä¸»è¦è‹±æ–‡
            chars_per_token = self.CHARS_PER_TOKEN_EN
        else:
            # æ··åˆ
            chars_per_token = self.CHARS_PER_TOKEN_MIXED

        estimated_tokens = int(total_chars / chars_per_token)
        return estimated_tokens

    def _calculate_complexity_score(self, request: str, context: Optional[Dict[str, Any]]) -> float:
        """è®¡ç®—å¤æ‚åº¦åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰"""
        score = 0.0

        # å› ç´ 1ï¼šè¯·æ±‚é•¿åº¦ï¼ˆ0-0.25ï¼‰- å¢åŠ æƒé‡
        request_len = len(request)
        if request_len < 50:
            length_score = 0.0
        elif request_len < 100:
            length_score = 0.05
        elif request_len < 200:
            length_score = 0.10
        elif request_len < 500:
            length_score = 0.15
        elif request_len < 1000:
            length_score = 0.20
        else:
            length_score = 0.25
        score += length_score

        # å› ç´ 2ï¼šé—®é¢˜æ•°é‡ï¼ˆ0-0.15ï¼‰
        question_count = sum(request.count(marker) for marker in ['ï¼Ÿ', '?', 'å¦‚ä½•', 'æ€ä¹ˆ', 'what', 'how'])
        if question_count <= 1:
            question_score = 0.0
        elif question_count <= 2:
            question_score = 0.08
        elif question_count <= 4:
            question_score = 0.12
        else:
            question_score = 0.15
        score += question_score

        # å› ç´ 3ï¼šæŠ€æœ¯æœ¯è¯­å¯†åº¦ï¼ˆ0-0.2ï¼‰
        tech_terms = [
            'context', 'ä¸Šä¸‹æ–‡', 'token', 'AI', 'model', 'ç³»ç»Ÿ', 'æ¶æ„', 'algorithm',
            'åˆ†å¸ƒå¼', 'å¾®æœåŠ¡', 'å¤±æ•ˆ', 'æ¨¡å¼', 'åŠ¨æ€', 'ç¼“å­˜', 'ç‰ˆæœ¬æ§åˆ¶', 'æ€§èƒ½ä¼˜åŒ–',
            'å®‰å…¨æ€§', 'åä½œ', 'æ™ºèƒ½'
        ]
        tech_term_count = sum(1 for term in tech_terms if term.lower() in request.lower())
        tech_score = min(tech_term_count * 0.03, 0.2)
        score += tech_score

        # å› ç´ 4ï¼šä¸Šä¸‹æ–‡å¤æ‚åº¦ï¼ˆ0-0.2ï¼‰
        if context:
            context_str = str(context)
            context_size = len(context_str)

            # æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸­çš„æ•°å€¼ï¼ˆå¦‚services: 50ï¼‰
            import re
            numbers = re.findall(r'\b\d+\b', context_str)
            has_large_numbers = any(int(n) > 10 for n in numbers)

            if context_size < 100:
                context_score = 0.05
            elif context_size < 1000:
                context_score = 0.10
            elif has_large_numbers or context_size < 5000:
                context_score = 0.15
            else:
                context_score = 0.20
        else:
            context_score = 0.0
        score += context_score

        # å› ç´ 5ï¼šæŠ½è±¡ç¨‹åº¦å’Œå¤åˆåº¦ï¼ˆ0-0.2ï¼‰
        abstract_keywords = [
            'åŸç†', 'æœºåˆ¶', 'è®¾è®¡', 'æ¶æ„', 'ä¼˜åŒ–', 'æœ€ä½³å®è·µ', 'pattern', 'design',
            'åˆ†å¸ƒå¼', 'å¤šä¸ª', 'å¤šç§', 'åŒæ—¶', 'é›†æˆ', 'å®ç°', 'ç­–ç•¥', 'è€ƒè™‘'
        ]
        abstract_count = sum(request.count(keyword) for keyword in abstract_keywords)
        abstract_score = min(abstract_count * 0.03, 0.2)
        score += abstract_score

        return min(score, 1.0)

    def _calculate_information_density(self, request: str) -> float:
        """è®¡ç®—ä¿¡æ¯å¯†åº¦ï¼ˆ0.0-1.0ï¼‰"""
        if not request:
            return 0.0

        # è®¡ç®—æœ‰æ•ˆå­—ç¬¦æ¯”ä¾‹ï¼ˆå­—æ¯ã€æ•°å­—ã€ä¸­æ–‡ï¼‰
        valid_chars = sum(1 for c in request if c.isalnum() or '\u4e00' <= c <= '\u9fff')
        density = valid_chars / len(request)

        return min(density, 1.0)

    def _calculate_structure_quality(self, request: str, context: Optional[Dict[str, Any]]) -> float:
        """è®¡ç®—ç»“æ„è´¨é‡ï¼ˆ0.0-1.0ï¼‰"""
        score = 0.0

        # å› ç´ 1ï¼šæ˜¯å¦æœ‰ç»“æ„æ ‡è®°ï¼ˆ0-0.4ï¼‰
        structure_markers = ['\n', 'ï¼Œ', 'ã€‚', '.', ',', 'ã€', ';', 'ï¼›']
        marker_count = sum(request.count(marker) for marker in structure_markers)
        structure_score = min(marker_count / 20, 0.4)
        score += structure_score

        # å› ç´ 2ï¼šä¸Šä¸‹æ–‡ç»“æ„ï¼ˆ0-0.3ï¼‰
        if context and isinstance(context, dict):
            # æœ‰ç»“æ„çš„ä¸Šä¸‹æ–‡
            if len(context.keys()) > 0:
                score += 0.3

        # å› ç´ 3ï¼šé€»è¾‘è¿è´¯æ€§ï¼ˆ0-0.3ï¼‰
        # ç®€åŒ–ï¼šæ£€æŸ¥æ˜¯å¦æœ‰è¿æ¥è¯
        connectives = ['å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'because', 'therefore', 'however', 'then']
        connective_count = sum(1 for word in connectives if word in request)
        connective_score = min(connective_count * 0.1, 0.3)
        score += connective_score

        return min(score, 1.0)

    def _calculate_relevance_score(self, request: str, context: Optional[Dict[str, Any]]) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºå…³é”®è¯åŒ¹é…
        # å®é™…åº”è¯¥ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦

        # åŸºç¡€ç›¸å…³æ€§ï¼ˆ0.5ï¼‰
        score = 0.5

        if not context:
            return score

        # æå–è¯·æ±‚ä¸­çš„å…³é”®è¯
        request_words = set(re.findall(r'\w+', request.lower()))

        # è®¡ç®—ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®è¯
        context_str = str(context).lower()
        context_words = set(re.findall(r'\w+', context_str))

        # è®¡ç®—é‡å ç‡
        if request_words:
            overlap = len(request_words & context_words)
            relevance = overlap / len(request_words)
            score += relevance * 0.5

        return min(score, 1.0)

    def _calculate_keyword_overlap(self, request: str, context: Optional[Dict[str, Any]]) -> float:
        """è®¡ç®—å…³é”®è¯é‡å åº¦ï¼ˆ0.0-1.0ï¼‰"""
        if not context:
            return 0.0

        # æå–å…³é”®è¯
        request_keywords = set(re.findall(r'\w+', request.lower()))
        context_str = str(context).lower()
        context_keywords = set(re.findall(r'\w+', context_str))

        if not request_keywords:
            return 0.0

        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(request_keywords & context_keywords)
        union = len(request_keywords | context_keywords)

        if union == 0:
            return 0.0

        return intersection / union

    def _calculate_completeness_score(self, request: str, context: Optional[Dict[str, Any]]) -> float:
        """è®¡ç®—å®Œæ•´æ€§åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰"""
        score = 0.0

        # å› ç´ 1ï¼šè¯·æ±‚é•¿åº¦æ˜¯å¦åˆç†ï¼ˆ0-0.3ï¼‰
        request_len = len(request)
        if 20 <= request_len <= 1000:
            score += 0.3
        elif request_len >= 10:
            score += 0.15

        # å› ç´ 2ï¼šæ˜¯å¦æœ‰ä¸Šä¸‹æ–‡ï¼ˆ0-0.3ï¼‰
        if context:
            score += 0.3

        # å› ç´ 3ï¼šé—®é¢˜æ˜¯å¦æ˜ç¡®ï¼ˆ0-0.4ï¼‰
        # æ£€æŸ¥æ˜¯å¦æœ‰é—®å·æˆ–ç–‘é—®è¯
        has_question = any(marker in request for marker in ['ï¼Ÿ', '?', 'å¦‚ä½•', 'æ€ä¹ˆ', 'what', 'how'])
        if has_question:
            score += 0.4

        return min(score, 1.0)

    def _calculate_consistency_score(self, context: Optional[Dict[str, Any]]) -> float:
        """è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰"""
        if not context:
            return 1.0  # æ²¡æœ‰ä¸Šä¸‹æ–‡ï¼Œè§†ä¸ºä¸€è‡´

        # ç®€åŒ–å®ç°ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„çŸ›ç›¾
        # å®é™…åº”è¯¥æ›´å¤æ‚
        score = 1.0

        # æ£€æŸ¥æ˜¯å¦æœ‰ç‰ˆæœ¬å†²çª
        if 'version' in str(context).lower():
            # å¯èƒ½æœ‰å¤šä¸ªç‰ˆæœ¬ä¿¡æ¯
            score -= 0.2

        return max(score, 0.0)

    def _calculate_freshness_score(self, context: Optional[Dict[str, Any]]) -> float:
        """è®¡ç®—æ–°é²œåº¦åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰"""
        if not context:
            return 1.0

        # ç®€åŒ–å®ç°ï¼šå¦‚æœä¸Šä¸‹æ–‡åŒ…å«æ—¶é—´æˆ³ï¼Œæ£€æŸ¥æ˜¯å¦æ–°é²œ
        # å®é™…åº”è¯¥è§£ææ—¶é—´æˆ³
        score = 1.0

        context_str = str(context)
        if 'timestamp' in context_str or 'æ—¶é—´' in context_str:
            # æœ‰æ—¶é—´ä¿¡æ¯ï¼Œå‡è®¾æ˜¯æ–°é²œçš„
            score = 0.9

        return score

    def _recommend_prompt_level(self, complexity_score: float, token_count: int, information_density: float) -> str:
        """æ¨èæç¤ºè¯å±‚æ¬¡"""
        # æ ¹æ®å¤æ‚åº¦ã€tokenæ•°é‡ã€ä¿¡æ¯å¯†åº¦æ¨è

        # Level 00: æ ¸å¿ƒæ¦‚å¿µï¼ˆæœ€ç®€å•ï¼‰
        if complexity_score < 0.3 and token_count < 5000:
            return "00"

        # Level 01: åŸºç¡€åº”ç”¨ï¼ˆå¸¸è§åœºæ™¯ï¼‰
        elif complexity_score < 0.5 and token_count < 10000:
            return "01"

        # Level 02: ä¸­çº§åœºæ™¯ï¼ˆå¤æ‚ä»»åŠ¡ï¼‰
        elif complexity_score < 0.7 or token_count < 20000:
            return "02"

        # Level 03: é«˜çº§åº”ç”¨ï¼ˆå¤§è§„æ¨¡ç³»ç»Ÿï¼‰
        else:
            return "03"

    def _generate_recommendations(self, token_count: int, complexity: float, relevance: float, completeness: float) -> List[str]:
        """ç”Ÿæˆæ¨èæ“ä½œ"""
        recommendations = []

        # Tokenç›¸å…³å»ºè®®
        if token_count > self.WARNING_TOKEN_COUNT:
            recommendations.append("è€ƒè™‘ç²¾ç®€ä¸Šä¸‹æ–‡æˆ–åˆ†è§£ä»»åŠ¡")
        elif token_count < 1000:
            recommendations.append("å¯ä»¥æ·»åŠ æ›´å¤šç»†èŠ‚ä»¥è·å¾—æ›´å¥½çš„å›ç­”")

        # å¤æ‚åº¦ç›¸å…³å»ºè®®
        if complexity > 0.7:
            recommendations.append("å¤æ‚ä»»åŠ¡ï¼Œå»ºè®®ä½¿ç”¨task-decomposeræŠ€èƒ½åˆ†è§£")

        # ç›¸å…³æ€§ç›¸å…³å»ºè®®
        if relevance < 0.5:
            recommendations.append("ä¸Šä¸‹æ–‡ç›¸å…³æ€§è¾ƒä½ï¼Œå»ºè®®ç§»é™¤æ— å…³ä¿¡æ¯")

        # å®Œæ•´æ€§ç›¸å…³å»ºè®®
        if completeness < 0.6:
            recommendations.append("è¯·æ±‚ä¸å¤Ÿå®Œæ•´ï¼Œå»ºè®®æ·»åŠ æ›´å¤šèƒŒæ™¯ä¿¡æ¯")

        # å¦‚æœæ²¡æœ‰é—®é¢˜
        if not recommendations:
            recommendations.append("ä¸Šä¸‹æ–‡çŠ¶æ€è‰¯å¥½")

        return recommendations

    def _generate_warnings(self, token_count: int, complexity: float, consistency: float) -> List[str]:
        """ç”Ÿæˆè­¦å‘Š"""
        warnings = []

        if token_count > self.MAX_TOKEN_COUNT:
            warnings.append("âš ï¸ ä¸Šä¸‹æ–‡è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™")

        if complexity > 0.8:
            warnings.append("âš ï¸ ä»»åŠ¡å¤æ‚åº¦å¾ˆé«˜ï¼Œå»ºè®®åˆ†æ­¥è¿›è¡Œ")

        if consistency < 0.7:
            warnings.append("âš ï¸ ä¸Šä¸‹æ–‡å¯èƒ½å­˜åœ¨çŸ›ç›¾ä¿¡æ¯")

        return warnings


# ä¾¿æ·å‡½æ•°
def calculate_metrics(request: str, context: Optional[Dict[str, Any]] = None) -> ContextMetrics:
    """
    è®¡ç®—æŒ‡æ ‡çš„ä¾¿æ·å‡½æ•°

    Args:
        request: ç”¨æˆ·è¯·æ±‚
        context: é™„åŠ ä¸Šä¸‹æ–‡

    Returns:
        ContextMetrics: è®¡ç®—ç»“æœ
    """
    calculator = ContextFundamentalsCalculator()
    return calculator.calculate(request, context)


if __name__ == "__main__":
    # æµ‹è¯•
    test_cases = [
        ("ä»€ä¹ˆæ˜¯ä¸Šä¸‹æ–‡ï¼Ÿ", None),
        ("å¦‚ä½•ä¼˜åŒ–AIç³»ç»Ÿçš„ä¸Šä¸‹æ–‡ç®¡ç†ï¼Ÿè¯·è¯¦ç»†è¯´æ˜æœ€ä½³å®è·µ", {"context": "ç³»ç»Ÿæ¶æ„"}),
        ("æˆ‘éœ€è¦åœ¨ä¸€ä¸ªåŒ…å«100ä¸ªæ–‡ä»¶çš„é¡¹ç›®ä¸­è¿›è¡Œå¤§è§„æ¨¡é‡æ„ï¼Œéœ€è¦è€ƒè™‘ä¸Šä¸‹æ–‡ç®¡ç†ã€å›¢é˜Ÿåä½œã€æ€§èƒ½ä¼˜åŒ–ç­‰å¤šä¸ªæ–¹é¢", {"size": "large"}),
    ]

    for request, context in test_cases:
        print(f"\n{'='*60}")
        print(f"è¯·æ±‚: {request}")
        print(f"ä¸Šä¸‹æ–‡: {context}")
        print('='*60)

        metrics = calculate_metrics(request, context)

        print(f"\nğŸ“Š åŸºç¡€æŒ‡æ ‡:")
        print(f"  Tokenæ•°é‡: {metrics.token_count:,}")
        print(f"  å­—ç¬¦æ•°: {metrics.character_count:,}")
        print(f"  å­—æ•°: {metrics.word_count:,}")
        print(f"  è¡Œæ•°: {metrics.line_count}")

        print(f"\nğŸ¯ å¤æ‚åº¦æŒ‡æ ‡:")
        print(f"  å¤æ‚åº¦åˆ†æ•°: {metrics.complexity_score:.2f}")
        print(f"  ä¿¡æ¯å¯†åº¦: {metrics.information_density:.2f}")
        print(f"  ç»“æ„è´¨é‡: {metrics.structure_quality:.2f}")

        print(f"\nğŸ”— ç›¸å…³æ€§æŒ‡æ ‡:")
        print(f"  ç›¸å…³æ€§åˆ†æ•°: {metrics.relevance_score:.2f}")
        print(f"  å…³é”®è¯é‡å : {metrics.keyword_overlap:.2f}")

        print(f"\nâœ… å¥åº·åº¦æŒ‡æ ‡:")
        print(f"  å®Œæ•´æ€§åˆ†æ•°: {metrics.completeness_score:.2f}")
        print(f"  ä¸€è‡´æ€§åˆ†æ•°: {metrics.consistency_score:.2f}")
        print(f"  æ–°é²œåº¦åˆ†æ•°: {metrics.freshness_score:.2f}")

        print(f"\nğŸ’¡ æ¨è:")
        print(f"  æç¤ºè¯å±‚æ¬¡: Level {metrics.recommended_prompt_level}")
        print(f"  æ“ä½œå»ºè®®:")
        for action in metrics.recommended_actions:
            print(f"    - {action}")

        if metrics.warnings:
            print(f"  è­¦å‘Š:")
            for warning in metrics.warnings:
                print(f"    {warning}")
