"""
Context Fundamentals Analyzer

è´Ÿè´£åˆ†æä¸Šä¸‹æ–‡çš„æ·±åº¦ç‰¹å¾
æä¾›ç¡®å®šæ€§çš„åˆ†æé€»è¾‘
"""

from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass
from enum import Enum
import re


class ContextFailureMode(Enum):
    """ä¸Šä¸‹æ–‡å¤±æ•ˆæ¨¡å¼"""
    LOST_IN_THE_MIDDLE = "lost_in_the_middle"
    POISONING = "poisoning"
    DISTRACTION = "distraction"
    CLASH = "clash"
    OVERFLOW = "overflow"
    FRAGMENTATION = "fragmentation"


@dataclass
class FailureModeDetection:
    """å¤±æ•ˆæ¨¡å¼æ£€æµ‹ç»“æœ"""
    mode: ContextFailureMode
    severity: str  # "low", "medium", "high"
    evidence: List[str]
    suggestions: List[str]


@dataclass
class ContextPattern:
    """ä¸Šä¸‹æ–‡æ¨¡å¼"""
    pattern_type: str  # "incremental", "layered", "on_demand", "isolated"
    confidence: float  # 0.0-1.0
    description: str


@dataclass
class ContextAnalysis:
    """ä¸Šä¸‹æ–‡åˆ†æç»“æœ"""
    # å¤±æ•ˆæ¨¡å¼æ£€æµ‹
    detected_failures: List[FailureModeDetection]

    # æ¨¡å¼è¯†åˆ«
    recognized_patterns: List[ContextPattern]

    # è´¨é‡åˆ†æ
    quality_scores: Dict[str, float]

    # ä¼˜åŒ–å»ºè®®
    optimization_suggestions: List[str]

    # æ¨èç­–ç•¥
    recommended_strategy: str


class ContextFundamentalsAnalyzer:
    """ä¸Šä¸‹æ–‡åŸºç¡€åˆ†æå™¨"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            config: é…ç½®å‚æ•°
        """
        self.config = config or {}

    def analyze(self, request: str, context: Optional[Dict[str, Any]] = None) -> ContextAnalysis:
        """
        åˆ†æä¸Šä¸‹æ–‡

        Args:
            request: ç”¨æˆ·è¯·æ±‚
            context: é™„åŠ ä¸Šä¸‹æ–‡

        Returns:
            ContextAnalysis: åˆ†æç»“æœ
        """
        # 1. æ£€æµ‹å¤±æ•ˆæ¨¡å¼
        detected_failures = self._detect_failure_modes(request, context)

        # 2. è¯†åˆ«ä¸Šä¸‹æ–‡æ¨¡å¼
        recognized_patterns = self._recognize_patterns(context)

        # 3. è®¡ç®—è´¨é‡åˆ†æ•°
        quality_scores = self._calculate_quality_scores(request, context)

        # 4. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        optimization_suggestions = self._generate_optimization_suggestions(
            detected_failures, quality_scores
        )

        # 5. æ¨èç­–ç•¥
        recommended_strategy = self._recommend_strategy(
            detected_failures, recognized_patterns, quality_scores
        )

        return ContextAnalysis(
            detected_failures=detected_failures,
            recognized_patterns=recognized_patterns,
            quality_scores=quality_scores,
            optimization_suggestions=optimization_suggestions,
            recommended_strategy=recommended_strategy
        )

    def _detect_failure_modes(self, request: str, context: Optional[Dict[str, Any]]) -> List[FailureModeDetection]:
        """æ£€æµ‹å¤±æ•ˆæ¨¡å¼"""
        detections = []

        # 1. Lost-in-the-Middleæ£€æµ‹
        lost_middle = self._detect_lost_in_the_middle(context)
        if lost_middle:
            detections.append(lost_middle)

        # 2. ä¸Šä¸‹æ–‡æ¯’åŒ–æ£€æµ‹
        poisoning = self._detect_poisoning(context)
        if poisoning:
            detections.append(poisoning)

        # 3. ä¸Šä¸‹æ–‡åˆ†å¿ƒæ£€æµ‹
        distraction = self._detect_distraction(context)
        if distraction:
            detections.append(distraction)

        # 4. ä¸Šä¸‹æ–‡å†²çªæ£€æµ‹
        clash = self._detect_clash(context)
        if clash:
            detections.append(clash)

        # 5. ä¸Šä¸‹æ–‡æº¢å‡ºæ£€æµ‹
        overflow = self._detect_overflow(context)
        if overflow:
            detections.append(overflow)

        # 6. ä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–æ£€æµ‹
        fragmentation = self._detect_fragmentation(context)
        if fragmentation:
            detections.append(fragmentation)

        return detections

    def _detect_lost_in_the_middle(self, context: Optional[Dict[str, Any]]) -> Optional[FailureModeDetection]:
        """æ£€æµ‹Lost-in-the-Middleç°è±¡"""
        if not context:
            return None

        context_str = str(context)
        context_len = len(context_str)

        # æ£€æµ‹æ¡ä»¶ï¼šä¸Šä¸‹æ–‡å¾ˆé•¿ä¸”åŒ…å«å¤§é‡åˆ—è¡¨æˆ–æ•°ç»„
        evidence = []
        severity = "low"

        # æ£€æŸ¥é•¿åº¦
        if context_len > 50000:
            evidence.append(f"ä¸Šä¸‹æ–‡å¾ˆé•¿ï¼ˆ{context_len:,}å­—ç¬¦ï¼‰")
            severity = "medium"

        # æ£€æŸ¥æ˜¯å¦æœ‰é•¿åˆ—è¡¨
        list_matches = re.findall(r'\[.*?\]', context_str)
        long_lists = [m for m in list_matches if len(m) > 100]
        if len(long_lists) > 3:
            evidence.append(f"åŒ…å«{len(long_lists)}ä¸ªé•¿åˆ—è¡¨")
            severity = "high"

        # æ£€æŸ¥æ˜¯å¦æœ‰å¤§é‡é¡¹
        if 'items' in str(context) or 'list' in str(context).lower():
            # å¯èƒ½æœ‰å¾ˆå¤šé¡¹
            pass

        if evidence and severity != "low":
            return FailureModeDetection(
                mode=ContextFailureMode.LOST_IN_THE_MIDDLE,
                severity=severity,
                evidence=evidence,
                suggestions=[
                    "å°†å…³é”®ä¿¡æ¯æ”¾åœ¨å¼€å¤´æˆ–ç»“å°¾",
                    "ä½¿ç”¨åˆ†æ®µå¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½è¿‡å¤šä¿¡æ¯",
                    "ä½¿ç”¨æ¸è¿›å¼ä¿¡æ¯æŠ«éœ²",
                    "æ˜ç¡®æŒ‡å‡ºå…³é”®ä¿¡æ¯çš„ä½ç½®"
                ]
            )

        return None

    def _detect_poisoning(self, context: Optional[Dict[str, Any]]) -> Optional[FailureModeDetection]:
        """æ£€æµ‹ä¸Šä¸‹æ–‡æ¯’åŒ–"""
        if not context:
            return None

        evidence = []
        severity = "low"

        context_str = str(context).lower()

        # æ£€æŸ¥æ˜¯å¦æœ‰ç‰ˆæœ¬çŸ›ç›¾
        version_indicators = ['v1', 'v2', 'version', 'ç‰ˆæœ¬', 'old', 'new']
        version_count = sum(context_str.count(indicator) for indicator in version_indicators)
        if version_count > 2:
            evidence.append("æ£€æµ‹åˆ°å¤šä¸ªç‰ˆæœ¬ä¿¡æ¯")
            severity = "medium"

        # æ£€æŸ¥æ˜¯å¦æœ‰çŸ›ç›¾å…³é”®è¯
        contradiction_pairs = [
            ('true', 'false'),
            ('enabled', 'disabled'),
            ('allow', 'deny'),
            ('æˆåŠŸ', 'å¤±è´¥'),
            ('æ˜¯', 'å¦')
        ]
        for word1, word2 in contradiction_pairs:
            if word1 in context_str and word2 in context_str:
                evidence.append(f"æ£€æµ‹åˆ°çŸ›ç›¾ä¿¡æ¯: {word1} vs {word2}")
                severity = "high"
                break

        if evidence:
            return FailureModeDetection(
                mode=ContextFailureMode.POISONING,
                severity=severity,
                evidence=evidence,
                suggestions=[
                    "æ˜ç¡®ç‰ˆæœ¬æ§åˆ¶ï¼Œä½¿ç”¨æœ€æ–°ç‰ˆæœ¬",
                    "ç§»é™¤è¿‡æ—¶ä¿¡æ¯",
                    "ä½¿ç”¨æ˜ç¡®çš„ä¼˜å…ˆçº§æ ‡è®°",
                    "éªŒè¯ä¿¡æ¯ä¸€è‡´æ€§"
                ]
            )

        return None

    def _detect_distraction(self, context: Optional[Dict[str, Any]]) -> Optional[FailureModeDetection]:
        """æ£€æµ‹ä¸Šä¸‹æ–‡åˆ†å¿ƒ"""
        if not context:
            return None

        evidence = []
        severity = "low"

        context_str = str(context)
        context_len = len(context_str)

        # æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦è¿‡å¤§
        if context_len > 100000:  # 100Kå­—ç¬¦
            evidence.append(f"ä¸Šä¸‹æ–‡å¾ˆå¤§ï¼ˆ{context_len:,}å­—ç¬¦ï¼‰")
            severity = "medium"

        # æ£€æŸ¥æ˜¯å¦æœ‰å¤§é‡é‡å¤å†…å®¹
        # ç®€åŒ–ï¼šæ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„å¥å­
        sentences = context_str.split('ã€‚')
        unique_sentences = set(sentences)
        if len(sentences) > 10 and len(unique_sentences) / len(sentences) < 0.7:
            evidence.append("æ£€æµ‹åˆ°å¤§é‡é‡å¤å†…å®¹")
            severity = "medium"

        if evidence:
            return FailureModeDetection(
                mode=ContextFailureMode.DISTRACTION,
                severity=severity,
                evidence=evidence,
                suggestions=[
                    "è¿‡æ»¤æ— å…³ä¿¡æ¯",
                    "ä½¿ç”¨ç›¸å…³æ€§è¯„åˆ†ç­›é€‰",
                    "åªä¿ç•™æ ¸å¿ƒå†…å®¹",
                    "ä½¿ç”¨references/ç›®å½•å­˜å‚¨è¯¦ç»†ä¿¡æ¯"
                ]
            )

        return None

    def _detect_clash(self, context: Optional[Dict[str, Any]]) -> Optional[FailureModeDetection]:
        """æ£€æµ‹ä¸Šä¸‹æ–‡å†²çª"""
        if not context or not isinstance(context, dict):
            return None

        evidence = []
        severity = "low"

        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªæºæä¾›ä¸åŒä¿¡æ¯
        if 'sources' in context or 'æ¥æº' in str(context):
            evidence.append("æ£€æµ‹åˆ°å¤šä¸ªä¿¡æ¯æº")
            severity = "medium"

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸ä¸€è‡´çš„æ•°æ®
        # ç®€åŒ–ï¼šæ£€æŸ¥æ•°å€¼çŸ›ç›¾
        values = []
        def extract_values(obj):
            if isinstance(obj, dict):
                for v in obj.values():
                    extract_values(v)
            elif isinstance(obj, list):
                for item in obj:
                    extract_values(item)
            elif isinstance(obj, (int, float)):
                values.append(obj)

        extract_values(context)
        # å¦‚æœæœ‰å¤§é‡æ•°å€¼ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çŸ›ç›¾
        # è¿™é‡Œç®€åŒ–å¤„ç†

        if evidence:
            return FailureModeDetection(
                mode=ContextFailureMode.CLASH,
                severity=severity,
                evidence=evidence,
                suggestions=[
                    "æ˜ç¡®ä¿¡æ¯ä¼˜å…ˆçº§",
                    "ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®æº",
                    "è§£å†³å†²çªæ ‡è®°ä¸ä¸€è‡´",
                    "å»ºç«‹å†²çªè§£å†³åè®®"
                ]
            )

        return None

    def _detect_overflow(self, context: Optional[Dict[str, Any]]) -> Optional[FailureModeDetection]:
        """æ£€æµ‹ä¸Šä¸‹æ–‡æº¢å‡º"""
        if not context:
            return None

        evidence = []
        severity = "low"

        context_str = str(context)
        context_len = len(context_str)

        # ä¼°ç®—tokenæ•°é‡ï¼ˆä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/tokenï¼‰
        estimated_tokens = int(context_len / 2.5)

        if estimated_tokens > 100000:  # 100K tokens
            evidence.append(f"ä¼°ç®—tokenæ•°é‡: {estimated_tokens:,}")
            severity = "high"
        elif estimated_tokens > 50000:
            evidence.append(f"ä¼°ç®—tokenæ•°é‡: {estimated_tokens:,}")
            severity = "medium"

        if evidence:
            return FailureModeDetection(
                mode=ContextFailureMode.OVERFLOW,
                severity=severity,
                evidence=evidence,
                suggestions=[
                    "ç«‹å³åˆ†è§£ä»»åŠ¡",
                    "ä½¿ç”¨ç‹¬ç«‹å·¥ä½œåŒº",
                    "åº”ç”¨åŸå­åŒ–åŸåˆ™",
                    "å®æ–½æ¸è¿›å¼æŠ«éœ²"
                ]
            )

        return None

    def _detect_fragmentation(self, context: Optional[Dict[str, Any]]) -> Optional[FailureModeDetection]:
        """æ£€æµ‹ä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–"""
        if not context:
            return None

        evidence = []
        severity = "low"

        context_str = str(context)

        # æ£€æŸ¥æ˜¯å¦æœ‰å¤§é‡çŸ­ç‰‡æ®µ
        fragments = re.split(r'[,\n]', context_str)
        short_fragments = [f for f in fragments if 0 < len(f.strip()) < 20]
        if len(short_fragments) > 20:
            evidence.append(f"æ£€æµ‹åˆ°{len(short_fragments)}ä¸ªçŸ­ç‰‡æ®µ")
            severity = "medium"

        # æ£€æŸ¥ç»“æ„æ˜¯å¦æ¾æ•£
        # ç®€åŒ–ï¼šæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„è¿æ¥è¯
        connectives = ['å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'å› æ­¤', 'because', 'therefore', 'however']
        connective_count = sum(context_str.count(c) for c in connectives)
        if len(fragments) > 10 and connective_count < len(fragments) / 10:
            evidence.append("ç¼ºå°‘é€»è¾‘è¿æ¥")
            severity = "medium"

        if evidence:
            return FailureModeDetection(
                mode=ContextFailureMode.FRAGMENTATION,
                severity=severity,
                evidence=evidence,
                suggestions=[
                    "é‡ç»„ä¸Šä¸‹æ–‡ç»“æ„",
                    "æ·»åŠ é€»è¾‘è¿æ¥è¯",
                    "ä½¿ç”¨å±‚æ¬¡åŒ–ç»„ç»‡",
                    "åˆ›å»ºä¸»é¢˜åˆ†ç»„"
                ]
            )

        return None

    def _recognize_patterns(self, context: Optional[Dict[str, Any]]) -> List[ContextPattern]:
        """è¯†åˆ«ä¸Šä¸‹æ–‡æ¨¡å¼"""
        patterns = []

        if not context:
            return patterns

        context_str = str(context)

        # 1. å¢é‡æ¨¡å¼ï¼ˆIncrementalï¼‰
        if 'step' in context_str.lower() or 'phase' in context_str.lower() or 'é˜¶æ®µ' in context_str:
            patterns.append(ContextPattern(
                pattern_type="incremental",
                confidence=0.8,
                description="å¢é‡å¼ä¸Šä¸‹æ–‡ï¼šåˆ†é˜¶æ®µé€æ­¥å¢åŠ ä¿¡æ¯"
            ))

        # 2. å±‚æ¬¡åŒ–æ¨¡å¼ï¼ˆLayeredï¼‰
        if 'layer' in context_str.lower() or 'level' in context_str.lower() or 'å±‚æ¬¡' in context_str or 'çº§åˆ«' in context_str:
            patterns.append(ContextPattern(
                pattern_type="layered",
                confidence=0.9,
                description="å±‚æ¬¡åŒ–ä¸Šä¸‹æ–‡ï¼šæŒ‰æŠ½è±¡çº§åˆ«ç»„ç»‡"
            ))

        # 3. æŒ‰éœ€æ¨¡å¼ï¼ˆOn-demandï¼‰
        if 'lazy' in context_str.lower() or 'dynamic' in context_str.lower() or 'æŒ‰éœ€' in context_str:
            patterns.append(ContextPattern(
                pattern_type="on_demand",
                confidence=0.7,
                description="æŒ‰éœ€ä¸Šä¸‹æ–‡ï¼šåŠ¨æ€åŠ è½½å’Œå¸è½½"
            ))

        # 4. éš”ç¦»æ¨¡å¼ï¼ˆIsolatedï¼‰
        if 'workspace' in context_str.lower() or 'isolated' in context_str.lower() or 'éš”ç¦»' in context_str:
            patterns.append(ContextPattern(
                pattern_type="isolated",
                confidence=0.85,
                description="éš”ç¦»ä¸Šä¸‹æ–‡ï¼šç‹¬ç«‹å·¥ä½œåŒºï¼Œé¿å…å¹²æ‰°"
            ))

        return patterns

    def _calculate_quality_scores(self, request: str, context: Optional[Dict[str, Any]]) -> Dict[str, float]:
        """è®¡ç®—è´¨é‡åˆ†æ•°"""
        scores = {}

        # 1. å®Œæ•´æ€§ï¼ˆ0.0-1.0ï¼‰
        completeness = 0.5
        if context:
            completeness += 0.3
        if len(request) > 20:
            completeness += 0.2
        scores['completeness'] = min(completeness, 1.0)

        # 2. ç›¸å…³æ€§ï¼ˆ0.0-1.0ï¼‰
        relevance = 0.6  # é»˜è®¤ä¸­ç­‰ç›¸å…³
        if context:
            # ç®€åŒ–ï¼šåŸºäºå…³é”®è¯é‡å 
            request_words = set(re.findall(r'\w+', request.lower()))
            context_words = set(re.findall(r'\w+', str(context).lower()))
            if request_words:
                overlap = len(request_words & context_words) / len(request_words)
                relevance = 0.5 + overlap * 0.5
        scores['relevance'] = min(relevance, 1.0)

        # 3. ç»„ç»‡æ€§ï¼ˆ0.0-1.0ï¼‰
        organization = 0.5
        if context and isinstance(context, dict):
            organization += 0.3  # æœ‰ç»“æ„çš„ä¸Šä¸‹æ–‡
        # æ£€æŸ¥æ˜¯å¦æœ‰æ ¼å¼åŒ–
        if '\n' in request or 'ï¼Œ' in request:
            organization += 0.2
        scores['organization'] = min(organization, 1.0)

        # 4. æ¸…æ™°åº¦ï¼ˆ0.0-1.0ï¼‰
        clarity = 0.5
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„é—®é¢˜
        if any(marker in request for marker in ['ï¼Ÿ', '?', 'å¦‚ä½•', 'æ€ä¹ˆ', 'what', 'how']):
            clarity += 0.3
        # æ£€æŸ¥é•¿åº¦æ˜¯å¦åˆç†
        if 10 < len(request) < 1000:
            clarity += 0.2
        scores['clarity'] = min(clarity, 1.0)

        return scores

    def _generate_optimization_suggestions(
        self,
        failures: List[FailureModeDetection],
        scores: Dict[str, float]
    ) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []

        # åŸºäºå¤±æ•ˆæ¨¡å¼çš„å»ºè®®
        for failure in failures:
            suggestions.extend(failure.suggestions)

        # åŸºäºè´¨é‡åˆ†æ•°çš„å»ºè®®
        if scores.get('completeness', 0) < 0.7:
            suggestions.append("è¡¥å……æ›´å¤šèƒŒæ™¯ä¿¡æ¯å’Œéœ€æ±‚ç»†èŠ‚")

        if scores.get('relevance', 0) < 0.7:
            suggestions.append("ç§»é™¤æ— å…³ä¿¡æ¯ï¼Œæé«˜ä¸Šä¸‹æ–‡ç›¸å…³æ€§")

        if scores.get('organization', 0) < 0.7:
            suggestions.append("ä½¿ç”¨æ¸…æ™°çš„ç»“æ„ç»„ç»‡ä¸Šä¸‹æ–‡")

        if scores.get('clarity', 0) < 0.7:
            suggestions.append("ç”¨æ˜ç¡®çš„è¯­è¨€æè¿°é—®é¢˜å’Œéœ€æ±‚")

        # å»é‡
        suggestions = list(set(suggestions))

        return suggestions

    def _recommend_strategy(
        self,
        failures: List[FailureModeDetection],
        patterns: List[ContextPattern],
        scores: Dict[str, float]
    ) -> str:
        """æ¨èç­–ç•¥"""
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡å¤±æ•ˆ
        critical_failures = [f for f in failures if f.severity == "high"]

        if critical_failures:
            return "ç«‹å³ä¿®å¤ï¼šé¦–å…ˆè§£å†³ä¸¥é‡çš„ä¸Šä¸‹æ–‡å¤±æ•ˆæ¨¡å¼"

        # æ£€æŸ¥æ˜¯å¦æœ‰å·²è¯†åˆ«çš„æ¨¡å¼
        if patterns:
            best_pattern = max(patterns, key=lambda p: p.confidence)
            return f"ä¼˜åŒ–ç°æœ‰æ¨¡å¼ï¼š{best_pattern.description}"

        # æ£€æŸ¥æ•´ä½“è´¨é‡
        avg_quality = sum(scores.values()) / len(scores) if scores else 0.5

        if avg_quality > 0.8:
            return "ä¿æŒç°çŠ¶ï¼šä¸Šä¸‹æ–‡è´¨é‡è‰¯å¥½"

        elif avg_quality > 0.6:
            return "æ¸è¿›ä¼˜åŒ–ï¼šé€æ­¥æ”¹è¿›ä¸Šä¸‹æ–‡è´¨é‡"

        else:
            return "é‡æ„ä¸Šä¸‹æ–‡ï¼šä½¿ç”¨æ¨èçš„4å±‚æ¸è¿›å¼æ¶æ„"


# ä¾¿æ·å‡½æ•°
def analyze_context(request: str, context: Optional[Dict[str, Any]] = None) -> ContextAnalysis:
    """
    åˆ†æä¸Šä¸‹æ–‡çš„ä¾¿æ·å‡½æ•°

    Args:
        request: ç”¨æˆ·è¯·æ±‚
        context: é™„åŠ ä¸Šä¸‹æ–‡

    Returns:
        ContextAnalysis: åˆ†æç»“æœ
    """
    analyzer = ContextFundamentalsAnalyzer()
    return analyzer.analyze(request, context)


if __name__ == "__main__":
    # æµ‹è¯•
    test_cases = [
        ("ä»€ä¹ˆæ˜¯ä¸Šä¸‹æ–‡ï¼Ÿ", None),
        ("åœ¨ä¸€ä¸ªå¤§é¡¹ç›®ä¸­å¦‚ä½•ç®¡ç†ä¸Šä¸‹æ–‡ï¼Ÿ", {"project": "large", "files": ["file1", "file2", "file3"] * 100}),
        ("ç‰ˆæœ¬v1è¯´ä½¿ç”¨Aæ–¹æ³•ï¼Œä½†v2è¯´ä½¿ç”¨Bæ–¹æ³•ï¼Œåº”è¯¥å¬å“ªä¸ªï¼Ÿ", {"v1": "method A", "v2": "method B"}),
    ]

    for request, context in test_cases:
        print(f"\n{'='*60}")
        print(f"è¯·æ±‚: {request}")
        print('='*60)

        analysis = analyze_context(request, context)

        # å¤±æ•ˆæ¨¡å¼
        if analysis.detected_failures:
            print(f"\nâš ï¸ æ£€æµ‹åˆ°çš„å¤±æ•ˆæ¨¡å¼:")
            for failure in analysis.detected_failures:
                print(f"\n  {failure.mode.value} ({failure.severity}):")
                print(f"    è¯æ®:")
                for evidence in failure.evidence:
                    print(f"      - {evidence}")
                print(f"    å»ºè®®:")
                for suggestion in failure.suggestions:
                    print(f"      - {suggestion}")
        else:
            print(f"\nâœ… æœªæ£€æµ‹åˆ°å¤±æ•ˆæ¨¡å¼")

        # è¯†åˆ«çš„æ¨¡å¼
        if analysis.recognized_patterns:
            print(f"\nğŸ” è¯†åˆ«çš„ä¸Šä¸‹æ–‡æ¨¡å¼:")
            for pattern in analysis.recognized_patterns:
                print(f"  - {pattern.pattern_type} (ç½®ä¿¡åº¦: {pattern.confidence:.2f})")
                print(f"    {pattern.description}")

        # è´¨é‡åˆ†æ•°
        print(f"\nğŸ“Š è´¨é‡åˆ†æ•°:")
        for metric, score in analysis.quality_scores.items():
            print(f"  {metric}: {score:.2f}")

        # ä¼˜åŒ–å»ºè®®
        if analysis.optimization_suggestions:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for suggestion in analysis.optimization_suggestions:
                print(f"  - {suggestion}")

        # æ¨èç­–ç•¥
        print(f"\nğŸ¯ æ¨èç­–ç•¥:")
        print(f"  {analysis.recommended_strategy}")
